<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Developer docs · IntervalMDP.jl</title><meta name="title" content="Developer docs · IntervalMDP.jl"/><meta property="og:title" content="Developer docs · IntervalMDP.jl"/><meta property="twitter:title" content="Developer docs · IntervalMDP.jl"/><meta name="description" content="Documentation for IntervalMDP.jl."/><meta property="og:description" content="Documentation for IntervalMDP.jl."/><meta property="twitter:description" content="Documentation for IntervalMDP.jl."/><meta property="og:url" content="https://www.baymler.com/IntervalMDP.jl/developer/"/><meta property="twitter:url" content="https://www.baymler.com/IntervalMDP.jl/developer/"/><link rel="canonical" href="https://www.baymler.com/IntervalMDP.jl/developer/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/citations.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="IntervalMDP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">IntervalMDP.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../usage/">Usage</a></li><li><a class="tocitem" href="../models/">Models</a></li><li><a class="tocitem" href="../specifications/">Specifications</a></li><li><a class="tocitem" href="../algorithms/">Algorithms</a></li><li><span class="tocitem">API reference</span><ul><li><a class="tocitem" href="../reference/systems/">Systems</a></li><li><a class="tocitem" href="../reference/specifications/">Specifications</a></li><li><a class="tocitem" href="../reference/solve/">Solve Interface</a></li><li><a class="tocitem" href="../reference/data/">Data Storage</a></li><li><a class="tocitem" href="../api/">Index</a></li></ul></li><li><a class="tocitem" href="../data/">Data formats</a></li><li class="is-active"><a class="tocitem" href>Developer docs</a><ul class="internal"><li><a class="tocitem" href="#Dense-matrix-vs-sparse-matrix-vs-BDD/ADD"><span>Dense matrix vs sparse matrix vs BDD/ADD</span></a></li><li><a class="tocitem" href="#Bellman-algorithms"><span>Bellman algorithms</span></a></li><li><a class="tocitem" href="#CUDA-Programming-Model"><span>CUDA Programming Model</span></a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Developer docs</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Developer docs</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Zinoex/IntervalMDP.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/Zinoex/IntervalMDP.jl/blob/main/docs/src/developer.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Developer-documentation"><a class="docs-heading-anchor" href="#Developer-documentation">Developer documentation</a><a id="Developer-documentation-1"></a><a class="docs-heading-anchor-permalink" href="#Developer-documentation" title="Permalink"></a></h1><h2 id="Dense-matrix-vs-sparse-matrix-vs-BDD/ADD"><a class="docs-heading-anchor" href="#Dense-matrix-vs-sparse-matrix-vs-BDD/ADD">Dense matrix vs sparse matrix vs BDD/ADD</a><a id="Dense-matrix-vs-sparse-matrix-vs-BDD/ADD-1"></a><a class="docs-heading-anchor-permalink" href="#Dense-matrix-vs-sparse-matrix-vs-BDD/ADD" title="Permalink"></a></h2><div class="admonition is-todo" id="Todo-4360c8f5e4662691"><header class="admonition-header">Todo<a class="admonition-anchor" href="#Todo-4360c8f5e4662691" title="Permalink"></a></header><div class="admonition-body"><p>Describe the details and choice</p></div></div><h2 id="Bellman-algorithms"><a class="docs-heading-anchor" href="#Bellman-algorithms">Bellman algorithms</a><a id="Bellman-algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Bellman-algorithms" title="Permalink"></a></h2><h3 id="dev-docs-omax"><a class="docs-heading-anchor" href="#dev-docs-omax">O-maximization</a><a id="dev-docs-omax-1"></a><a class="docs-heading-anchor-permalink" href="#dev-docs-omax" title="Permalink"></a></h3><p>To optimize the procedure, we abstract the O-maximization algorithm into the sorting phase and the O-maximization phase: </p><pre><code class="language-julia hljs">function min_value(V, system, source, action)
    # Sort values of `V` in ascending order
    order = sortstates(V)
    v = o_maximize(system, source, action, order)
    return v
end</code></pre><p>Notice the the order is shared for all source-action pairs, and thus, we can pre-compute it once per Bellman update. We however only do so for dense transition ambiguity sets, as in the sparse case, it is often faster to sort repeatedly, but only for the support. I.e.,</p><pre><code class="language-julia hljs">function sortstates(V, system, source, action)  # I.e. sort per source-action pair
    supp = support(system, source, action)
    order = sortperm(@view(V[supp])) # Sort only for the support
    return supp[order]  # Return sorted indices in original indexing
end</code></pre><h4 id="GPU-acceleration"><a class="docs-heading-anchor" href="#GPU-acceleration">GPU acceleration</a><a id="GPU-acceleration-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-acceleration" title="Permalink"></a></h4><p>The sorting and O-maximization phases can be parallelized on the GPU to leverage the massive parallelism. The following assumes that the reader is familiar with the CUDA programming model; see <a href="#CUDA-Programming-Model">CUDA Programming Model</a> for a brief introduction. The specific execution plan depends on the storage type and size of model; please refer to the source code for specifics.</p><h5 id="Sorting"><a class="docs-heading-anchor" href="#Sorting">Sorting</a><a id="Sorting-1"></a><a class="docs-heading-anchor-permalink" href="#Sorting" title="Permalink"></a></h5><p>Sorting in parallel on the GPU is a well-studied problem, and there are many algorithms for doing so. We choose to use bitonic sorting, which is a sorting network that is easily parallelized and implementable on a GPU. The idea is to merge bitonic subsets, i.e. sets with first increasing then decreasing subsets of equal size, of increasingly larger sizes and perform minor rounds of swaps to maintain the bitonic property. The figure below shows 3 major rounds to sort a set of 8 elements (each line represents an element, each arrow is a comparison pointing towards the larger element). The latency<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1" class="footnote-ref">[1]</a><span class="footnote-preview" id="fn-1"></span></sup> of the sorting network is <span>$O((\lg n)^2)$</span>, and thus it scales well to larger number of elements. See <a href="https://en.wikipedia.org/wiki/Bitonic_sorter">Wikipedia</a> for more details.</p><p><img src="../assets/bitonic_sorting.svg" alt/></p><h5 id="O-maximization-phase"><a class="docs-heading-anchor" href="#O-maximization-phase">O-maximization phase</a><a id="O-maximization-phase-1"></a><a class="docs-heading-anchor-permalink" href="#O-maximization-phase" title="Permalink"></a></h5><p>In order to parallelize the O-maximization phase, observe that O-maximization implicity implements a cumulative sum according to the ordering over gaps and this is the only dependency between the states. Hence, if we can parallelize this cumulative sum, then we can parallelize the O-maximization phase. Luckily, there is a well-studied algorithm for computing the cumulative sum in parallel: tree reduction for prefix scan. The idea is best explained with figure below.</p><p><img src="../assets/tree_reduction_prefix_scan.svg" alt/></p><p>Here, we recursively compute the cumulative sum of larger and larger subsets of the array. The latency is <span>$O(\lg n)$</span>, and thus very efficient. See <a href="https://en.wikipedia.org/wiki/Prefix_sum">Wikipedia</a> for more details. Putting it all together, we get the following (pseudo-code) algorithm for O-maximization:</p><pre><code class="language-julia hljs">function o_maximize(system, source, action, order)
    p = lower_bounds(system, source, action)
    rem = 1 - sum(p)
    gap = upper_bounds(system, source, action) - p

    # Ordered cumulative sum of gaps via tree reduction
    cumgap = cumulative_sum(gap[order])

    @parallelize for (i, o) in enumerate(order)
        rem_state = max(rem - cumgap[i] + gap[o], 0)
        if gap[o] &lt; rem_state
            p[o] += gap[o]
        else
            p[o] += rem_state
            break
        end
    end

    return p
end</code></pre><p>When implementing the algorithm above in CUDA, it is possible to use warp shuffles to very efficiently perform tree reductions of up to 32 elements. For larger sets, shared memory to store the intermediate results, which is much faster than global memory. See <a href="#CUDA-Programming-Model">CUDA Programming Model</a> for more details on why these choices are important.</p><h3 id="dev-docs-vertex-enumeration"><a class="docs-heading-anchor" href="#dev-docs-vertex-enumeration">Vertex enumeration</a><a id="dev-docs-vertex-enumeration-1"></a><a class="docs-heading-anchor-permalink" href="#dev-docs-vertex-enumeration" title="Permalink"></a></h3><p>First, we concern ourselves with enumerating the vertices of a single marginal. The key observation for an efficient algorithm is that, while each vertex corresponds to a unique ordering of the states, many orderings yield the same vertex. Thus, we need an algorithm that generates each vertex exactly once without generating all orderings explicitly. To this end, we rely on a backtracking algorithm where state values are added to a list of a &quot;maximizing&quot; state values, and backtrack once a vertex is found, i.e. <code>sum(p) == 1</code> and the remaining state values are assigned a lower bound.</p><p>For the product of marginals, we simply apply <span>$Iterators.product$</span> to get an iterator over all combinations of vertices.</p><h3 id="dev-docs-mccormick"><a class="docs-heading-anchor" href="#dev-docs-mccormick">Recursive McCormick envelopes</a><a id="dev-docs-mccormick-1"></a><a class="docs-heading-anchor-permalink" href="#dev-docs-mccormick" title="Permalink"></a></h3><p>The recursive McCormick envelopes for polytoptic fRMDPs are described in [<a href="../references/#schnitzer2025efficient">1</a>], with the addition that we add the marginal constraints to the linear program and the constraint that each relaxation <code>q</code> in the recursion is a valid probability distribution, i.e. <code>sum(q) == 1</code>. </p><p>Another consideration is whether to recursively relax as a sequence of marginals or as a binary tree. In [<a href="../references/#schnitzer2025efficient">1</a>], the recursive relaxation is done as a sequence. However, the tree structure requires significantly fewer auxiliary variables and thus both memory and time. A formal argument of the resulting minimum value between the two relaxation structures is missing, but empirically, they yield the same results.</p><h2 id="CUDA-Programming-Model"><a class="docs-heading-anchor" href="#CUDA-Programming-Model">CUDA Programming Model</a><a id="CUDA-Programming-Model-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-Programming-Model" title="Permalink"></a></h2><p>We here give a brief introduction to the CUDA programming model to understand to algorithmic choices. For a more in-depth introduction, see the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a>. The CUDA framework is Single-Instruction Multiple-Thread (SIMT) parallel execution platform and Application Programming Interface. This is in contrast to Single-Instruction Multiple-Data where all data must be processed homogeneously without control flow. SIMT makes CUDA more flexible for heterogeneous processing and control flow. The smallest execution unit in CUDA is a thread, which is a sequential processing of instructions. A thread is uniquely identified by its thread index, which allows indexing into the global data for parallel processing. A group of 32 threads<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2" class="footnote-ref">[2]</a><span class="footnote-preview" id="fn-2"></span></sup> is called a warp, which will be executed <em>mostly</em> synchronously on a streaming multiprocessor. If control flow makes threads in a wrap diverge, instructions may need to be decoded twice and executed in two separate cycles. Due to this synchronous behavior, data can be shared in registers between threads in a warp for maximum performance. A collection of (up to) 1024 threads is called a block, and this is the largest aggregation that can be synchronized. Furthermore, threads in a block share the appropriately named shared memory. This is memory that is stored locally on the streaming multiprocessor for fast access. Note that shared memory is unintuitively faster than local memory (not to be confused with registers) due to local memory being allocated in device memory. Finally, a collection of (up to) 65536 blocks is called the grid of a kernel, which is the set of instructions to be executed. The grid is singular as only a single ever exists per launched kernel. Hence, if more blocks are necessary to process the amount of data, then a grid-strided loop or multiple kernels are necessary. </p><p><img src="../assets/cuda_programming_model.svg" alt/></p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Note that when assessing parallel algorithms, the asymptotic performance is measured by the latency, which is the delay in the number of parallel operations, before the result is available. This is in contrast to traditional algorithms, which are assessed by the total number of operations.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>with consecutive thread indices aligned to a multiple of 32.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../data/">« Data formats</a><a class="docs-footer-nextpage" href="../references/">References »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 5 December 2025 09:32">Friday 5 December 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
