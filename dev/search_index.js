var documenterSearchIndex = {"docs":
[{"location":"references/#Bibliography","page":"References","title":"Bibliography","text":"Y. Schnitzer, A. Abate and D. Parker. Efficient Solution and Learning of Robust Factored MDPs, arXiv preprint arXiv:2508.00707 (2025), arXiv:2508.00707.\n\n\n\nK. V. Delgado, S. Sanner and L. N. De Barros. Efficient solutions to factored MDPs with imprecise transition probabilities. Artificial Intelligence 175, 1498–1527 (2011).\n\n\n\nA. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain transition matrices. Operations Research 53, 780–798 (2005).\n\n\n\nW. Wiesemann, D. Kuhn and B. Rustem. Robust Markov decision processes. Mathematics of Operations Research 38, 153–183 (2013).\n\n\n\nM. Suilen, T. Badings, E. M. Bovy, D. Parker and N. Jansen. Robust markov decision processes: A place where AI and formal methods meet. In: Principles of Verification: Cycling the Probabilistic Landscape: Essays Dedicated to Joost-Pieter Katoen on the Occasion of His 60th Birthday, Part III (Springer, 2024); pp. 126–154, arXiv:2411.11451.\n\n\n\nB. Delahaye, K. G. Larsen, A. Legay, M. L. Pedersen and A. Wąsowski. Decision problems for interval Markov chains. In: International Conference on Language and Automata Theory and Applications (Springer, 2011); pp. 274–285.\n\n\n\nR. Givan, S. Leach and T. Dean. Bounded-parameter Markov decision processes. Artificial Intelligence 122, 71–109 (2000).\n\n\n\nM. Lahijanian, S. B. Andersson and C. Belta. Formal verification and synthesis for discrete-time stochastic systems. IEEE Transactions on Automatic Control 60, 2031–2045 (2015).\n\n\n\nF. B. Mathiesen, S. Haesaert and L. Laurenti. Scalable control synthesis for stochastic systems via structural IMDP abstractions. In: Proceedings of the 28th ACM International Conference on Hybrid Systems: Computation and Control (2025); pp. 1–12, arXiv:2411.11803.\n\n\n\nC. Baier and J.-P. Katoen. Principles of model checking (MIT press, 2008).\n\n\n\nG. De Giacomo, M. Y. Vardi and others. Linear Temporal Logic and Linear Dynamic Logic on Finite Traces. In: Ijcai, Vol. 13 (2013); pp. 854–860.\n\n\n\n","category":"section"},{"location":"reference/data/#Data-formats","page":"Data Storage","title":"Data formats","text":"","category":"section"},{"location":"reference/data/#PRISM","page":"Data Storage","title":"PRISM","text":"","category":"section"},{"location":"reference/data/#bmdp-tool","page":"Data Storage","title":"bmdp-tool","text":"","category":"section"},{"location":"reference/data/#IntervalMDP.jl","page":"Data Storage","title":"IntervalMDP.jl","text":"","category":"section"},{"location":"reference/data/#IntervalMDP.Data.write_prism_file","page":"Data Storage","title":"IntervalMDP.Data.write_prism_file","text":"write_prism_file(path_without_file_ending, problem)\n\nWrite the files required by PRISM explicit engine/format to \n\npath_without_file_ending.sta (states),\npath_without_file_ending.lab (labels),\npath_without_file_ending.tra (transitions), and\npath_without_file_ending.pctl (properties).\n\nIf the specification is a reward optimization problem, then a state rewards file .srew is also written.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_prism_file","page":"Data Storage","title":"IntervalMDP.Data.read_prism_file","text":"read_prism_file(path_without_file_ending)\n\nRead PRISM explicit file formats and pctl file, and return a ControlSynthesisProblem including system and specification.\n\nSee PRISM Explicit Model Files for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_bmdp_tool_file","page":"Data Storage","title":"IntervalMDP.Data.read_bmdp_tool_file","text":"read_bmdp_tool_file(path)\n\nRead a bmdp-tool transition probability file and return an IntervalMarkovDecisionProcess and a list of terminal states. From the file format, it is not clear if the desired reachability verification if the reachability specification is finite or infinite horizon, the satisfaction_mode is pessimistic or optimistic, or if the actions should minimize or maximize the probability of reachability.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.write_bmdp_tool_file","page":"Data Storage","title":"IntervalMDP.Data.write_bmdp_tool_file","text":"write_bmdp_tool_file(path, problem::IntervalMDP.AbstractIntervalMDPProblem)\n\nWrite a bmdp-tool transition probability file for the given an IMDP and a reachability specification. The file will not contain enough information to specify a reachability specification. The remaining parameters are rather command line arguments.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovProcess, spec::Specification)\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovProcess, prop::AbstractReachability)\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovProcess, terminal_states::Vector{T})\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IMDP, terminal_states::Vector{<:CartesianIndex})\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_intervalmdp_jl","page":"Data Storage","title":"IntervalMDP.Data.read_intervalmdp_jl","text":"read_intervalmdp_jl(model_path, spec_path; control_synthesis = false)\n\nRead an IntervalMDP.jl data file and return a ControlSynthesisProblem or  VerificationProblem depending on the control_synthesis flag.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_intervalmdp_jl_model","page":"Data Storage","title":"IntervalMDP.Data.read_intervalmdp_jl_model","text":"read_intervalmdp_jl_model(model_path)\n\nRead an IntervalMarkovDecisionProcess or IntervalMarkovChain from an IntervalMDP.jl system file (netCDF sparse format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_intervalmdp_jl_spec","page":"Data Storage","title":"IntervalMDP.Data.read_intervalmdp_jl_spec","text":"read_intervalmdp_jl_spec(spec_path)\n\nRead a Specification from an IntervalMDP.jl spec file (JSON-format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.write_intervalmdp_jl_model","page":"Data Storage","title":"IntervalMDP.Data.write_intervalmdp_jl_model","text":"write_intervalmdp_jl_model(model_path, mdp)\n\nWrite an IntervalMarkovDecisionProcess to an IntervalMDP.jl system file (netCDF sparse format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.write_intervalmdp_jl_spec","page":"Data Storage","title":"IntervalMDP.Data.write_intervalmdp_jl_spec","text":"write_intervalmdp_jl_spec(spec_path, spec::Specification)\n\nWrite a Specification to an IntervalMDP.jl spec file (JSON-format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"data/#Data-storage-formats","page":"Data formats","title":"Data storage formats","text":"IntervalMDP.jl supports reading and writing data in various formats, namely PRISM explicit format, bmdp-tool, and our own format (model and specification). To justify introducing another standard (see relevant XKCD), note that the PRISM explicit format and the bmdp-tool format are all written in ASCII, which is very inefficient in terms of storage space (especially for storing floating point numbers) and parsing time. We propose a binary format for the most storage-intensive part of the data, namely the transition probabilities, and use JSON for the specification, which is human- and machine-readable and widely used.","category":"section"},{"location":"data/#PRISM","page":"Data formats","title":"PRISM","text":"IntervalMDP.jl supports reading and writing PRISM [1] explicit data format.  The data format is split into 4 different files, one for the states, one for the labels, one for the transition probabilities, and one for the specification. Therefore, our interface for reading PRISM files takes the path without file ending and adds the appropriate ending to each of the four files.\n\n# Read\nproblem = read_prism_file(path_without_file_ending)\n\n# Write\nwrite_prism_file(path_without_file_ending, problem)\n\nThe problem structure contains both the \\gls{imdp} and the specification including whether to synthesize a maximizing or minimizing strategy and whether to use an optimistic or pessimistic adversary.\n\nPRISM uses 4 different ASCII-encoded files to store the explicit representation of the system: '.sta' (states), '.lab' (labels), '.tra' (transitions), and '.pctl' (property). In the tables below, we list the format for each file. The extended details of the PRISM explicit file format can be found in the appendix of the PRISM manual.","category":"section"},{"location":"data/#States-.sta","page":"Data formats","title":"States .sta","text":"Number of lines Description\nFirst line The first line containing symbolically (v1, v2, ..., vn) is a list of n variables in the model.\nm lines where m is the number of states Each line contains i:(v1, v2, ..., vn) where i is the index of the state and (v1, v2, ..., vn)` is an assignment of values to the variables in the model. Indices are zero-indexed.","category":"section"},{"location":"data/#Labels-.lab","page":"Data formats","title":"Labels .lab","text":"Number of lines Description\nFirst line Contains a space-separated list of labels with index i=\"label\". The first two must be 0=\"init\" 1=\"deadlock\".\nAll remaining lines Contains i: j1 j2 j3 ... where i is a state index and j1 j2 j3 ... are space-separated indices of labels associated with state i.","category":"section"},{"location":"data/#Transitions-.tra","page":"Data formats","title":"Transitions .tra","text":"Number of lines Description\nFirst line num_states num_choices num_transitions where num_state must match the number in the state file.\nFollowing num_transitions lines A list of transition probabilities with the format src_idx act_idx dest_idx [p_lower,p_upper] action.","category":"section"},{"location":"data/#Property-.pctl","page":"Data formats","title":"Property .pctl","text":"Number of lines Description\nFirst line PRISM property specification","category":"section"},{"location":"data/#bmdp-tool","page":"Data formats","title":"bmdp-tool","text":"bmdp-tool data format is similar to the PRISM explicit format transition probability files, where transition probabilities are stored line-by-line with source, action, destination, and probability bounds in ASCII. Key differences include no explicit listing of states, the fact that it only supports reachability properties, and that terminal states are listed directly in the transition probability file. As a result, bmdp-tool data format is a single file. This format lacks information about whether the reachability is finite or infinite time, and hence the reader only returns the set of terminal states.\n\n# Read\nimdp, terminal_states = read_bmdp_tool_file(path)\n\n# Write\nwrite_bmdp_tool_file(path, problem)\n\nbmdp-tool uses only one ASCII file with the following format:\n\nNumber of lines Description\nFirst line num_states.\nSecond line num_actions (not to be confused with num_choices of PRISM).\nThird line num_terminal.\nThe following num_terminal lines Indices (zero-indexed) of terminal states, one per line.\nThe following num_terminal lines Indices (zero-indexed) of terminal states, one per line.\nAll remaining lines A list of transition probabilities with the format src_idx act_idx dest_idx p_lower p_upper.\n\nterminology: Choices vs actions\nIn PRISM, the number of choices which is listed in the transition file is the sum of the number of feasible actions in each state. In bmdp-tool, the number of actions is the total number of different actions in the model, i.e. in each state up to num_actions may be feasible. This is a subtle difference, but it is important to be aware of as the parsing in either tool requires the right number to be specified.","category":"section"},{"location":"data/#IntervalMDP.jl","page":"Data formats","title":"IntervalMDP.jl","text":"IntervalMDP.jl also supports a different binary format based on NetCDF to store transition probabilities. We use JSON to store the specification, as storage space for the specification is much less a concern, and because JSON is a widely used, human-readable, file format.\n\n# Read\nimdp = read_intervalmdp_jl_model(model_path)\nspec = read_intervalmdp_jl_spec(spec_path)\nproblem = Problem(imdp, spec)\n\nproblem = read_intervalmdp_jl(model_path, spec_path)\n\n# Write\nwrite_intervalmdp_jl_model(model_path, imdp_or_problem)\nwrite_intervalmdp_jl_spec(spec_path, spec_or_problem)\n\nThe new format proposed uses netCDF, which is based on HDF5 underlying, to store transition probabilities, and a JSON file to store the specification. Transition probabilities are stored in CSC-format, which is unfortunately not natively stored in netCDF, nor any widely available format. Therefore, we store the following attributes and variables in the netCDF file:\n\nGlobal attributes:\n\nnum_states\nmodel (either imc or imdp)\nformat (assert sparse_csc)\nrows (assert to)\ncols (assert from if model is imc and from/action if model is imdp)\n\nVariables:\n\nlower_colptr (integer)\nlower_rowval (integer)\nlower_nzval (floating point)\nupper_colptr (integer)\nupper_rowval (integer)\nupper_nzval (floating point)\ninitial_states (integer)\nstateptr (integer, only for imdp)\naction_vals (any netCDF supported type, only for imdp)\n\nWe store the specification in a JSON format where the structure depends on the type of specification. For a reachability-like specification, the specification is the following format\n\n{\n    \"property\": {\n        \"type\": <\"reachability\"|\"reach-avoid\">,\n        \"infinite_time\": <true|false>,\n        \"time_horizon\": <positive int>,\n        \"eps\": <positive float>,\n        \"reach\": [<state_index:positive int>],\n        \"avoid\": [<state_index:positive int>]\n    },\n    \"satisfaction_mode\": <\"pessimistic\"|\"optimistic\">,\n    \"strategy_mode\": <\"minimize\"|\"maximize\">\n}\n\nFor a finite horizon property, eps is excluded, and similarly for an infinite horizon property, time\\_horizon is excluded.  For a proper reachability property, the avoid-field is excluded.\n\nIf we instead want to optimize a reward, the format is the following\n\n{\n    \"property\": {\n        \"type\": \"reward\",\n        \"infinite_time\": <true|false>,\n        \"time_horizon\": <positive int>,\n        \"eps\": <positive float>,\n        \"reward\": [<reward_per_state_index:float>]\n        \"discount\" <float:0-1>\n    },\n    \"satisfaction_mode\": <\"pessimistic\"|\"optimistic\">,\n    \"strategy_mode\": <\"minimize\"|\"maximize\">\n}\n\n[1] Kwiatkowska, Marta, Gethin Norman, and David Parker. \"PRISM 4.0: Verification of probabilistic real-time systems.\" Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23. Springer Berlin Heidelberg, 2011.","category":"section"},{"location":"reference/systems/#System-representation","page":"Systems","title":"System representation","text":"","category":"section"},{"location":"reference/systems/#api-frmdp","page":"Systems","title":"Factored RMDPs","text":"","category":"section"},{"location":"reference/systems/#Convenience-constructors-for-subclasses-of-fRMDPs","page":"Systems","title":"Convenience constructors for subclasses of fRMDPs","text":"","category":"section"},{"location":"reference/systems/#Probability-representation","page":"Systems","title":"Probability representation","text":"","category":"section"},{"location":"reference/systems/#Interval-ambiguity-sets","page":"Systems","title":"Interval ambiguity sets","text":"","category":"section"},{"location":"reference/systems/#Deterministic-Finite-Automaton-(DFA)","page":"Systems","title":"Deterministic Finite Automaton (DFA)","text":"","category":"section"},{"location":"reference/systems/#Transition-function-for-DFA","page":"Systems","title":"Transition function for DFA","text":"","category":"section"},{"location":"reference/systems/#Labelling-of-IMDP-states-to-Automaton-alphabet","page":"Systems","title":"Labelling of IMDP states to Automaton alphabet","text":"","category":"section"},{"location":"reference/systems/#IntervalMDP.num_states","page":"Systems","title":"IntervalMDP.num_states","text":"num_states(mp::IntervalMarkovProcess)\n\nReturn the number of states.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.num_actions","page":"Systems","title":"IntervalMDP.num_actions","text":"num_actions(mp::IntervalMarkovProcess)\n\nReturn the number of actions.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.initial_states","page":"Systems","title":"IntervalMDP.initial_states","text":"initial_states(mp::IntervalMarkovProcess)\n\nReturn the initial states. If the initial states are not specified, return nothing.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.AllStates","page":"Systems","title":"IntervalMDP.AllStates","text":"AllStates\n\nA type to represent all states in a Markov process. This type is used to specify all states as the initial states.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.FactoredRobustMarkovDecisionProcess","page":"Systems","title":"IntervalMDP.FactoredRobustMarkovDecisionProcess","text":"FactoredRobustMarkovDecisionProcess{N, M, P <: NTuple{N, Marginal}, VI <: InitialStates} <: IntervalMarkovProcess\n\nFactored Robust Markov Decision Processes (fRMDPs) [1, 2] are an extension of Robust Markov Decision Processes (RMDPs) [3–5] that incorporate a factored representation of the state and action spaces, i.e. with state and action variables.\n\nFormally, a fRMDP M is a tuple M = (S S_0 A mathcalG Gamma), where\n\nS = S_1 times cdots times S_n is a finite set of joint states with S_i    being a finite set of states for the i-th state variable,\nS_0 subseteq S is a set of initial states,\nA = A_1 times cdots times A_m is a finite set of joint actions with A_j    being a finite set of actions for the j-th action variable,\nmathcalG = (mathcalV mathcalE) is a directed bipartite graph with nodes    mathcalV = mathcalV_ind cup mathcalV_cond = S_1 ldots S_n A_1 ldots A_m cup S_1 ldots S_n   representing the state and action variables and their next-state counterparts, and edges    mathcalE subseteq mathcalV_ind times mathcalV_cond   representing dependencies of S_i on S_j and A_k,\nGamma = Gamma_s a_s in S a in A is a set of ambiguity sets for source-action pair (s a),    where each Gamma_s a = bigotimes_i=1^n Gamma^i_textPa_mathcalG(S_i) cap (s a) is   a product of ambiguity sets Gamma^i_textPa_mathcalG(S_i) cap (s a) along each marginal i conditional   on the values in (s a) of the parent variables textPa_mathcalG(S_i) of S_i in mathcalG, i.e.\n\n    Gamma_s a = left gamma in mathcalD(S)  gamma(t) = prod_i=1^n gamma^i(t_i  s_textPa_mathcalG_S(S_i) a_textPa_mathcalG_A(S_i))  gamma^i(cdot  s_textPa_mathcalG_S(S_i) a_textPa_mathcalG_A(S_i)) in Gamma^i_textPa_mathcalG(S_i) right\n\nFor a given source-action pair (s a) in S times A, any distribution gamma_s a in Gamma_s a is called a feasible distribution, and feasible transitions are triplets (s a t) in S times A times S where t in mathopsupp(gamma_s a) for any feasible distribution gamma_s a in Gamma_s a.\n\nType parameters\n\nN is the number of state variables.\nM is the number of action variables.\nP <: NTuple{N, Marginal} is a tuple type with a (potentially different) type for each marginal.\nVI <: InitialStates is the type of initial states.\n\nFields\n\nstate_vars::NTuple{N, Int32}: the number of values S_i for each state variable S_i as a tuple.\naction_vars::NTuple{M, Int32}: the number of values A_k for each action variable A_k as a tuple.\nsource_dims::NTuple{N, Int32}: for systems with terminal states along certain slices, it is possible to avoid   specifying them by using source_dims less than state_vars; this is useful e.g. in building abstractions.   The terminal states must be the last value for the slice dimension. If not supplied, it is assumed source_dims == state_vars.\ntransition::P is the marginal ambiguity sets. For a given source-action pair (s a) in S times A,   any Marginal element of transition subselects s and a corresponding to its state_variables   and action_variables, i.e. it encodes the operation \\text{Pa}_\\mathcal{G}(S'_i) \\cap (s, a).   The underlying ambiguity_sets object on Marginal encodes Gamma^i_textPa_mathcalG(S_i) cap (s a)   for all values of textPa_mathcalG(S_i). See Marginal for details about the layout of the underlying   AbstractAmbiguitySets object.\ninitial_states::VI: stores a representation of S_0. If no set of initial_states are given, then it is simply assigned   the zero-byte object AllStates(), which represents that all states are potential initial states. It is not used within   the value iteration.\n\nExample\n\nusing IntervalMDP\n\nstate_vars = (2, 3)\naction_vars = (1, 2)\n\nstate_indices = (1, 2)\naction_indices = (1,)\nstate_dims = (2, 3)\naction_dims = (1,)\nmarginal1 = Marginal(IntervalAmbiguitySets(;\n    # 6 ambiguity sets = 2 * 3 source states, 1 action\n    # Column layout: (a¹₁, s¹₁, s²₁), (a¹₁, s¹₂, s²₁), (a¹₁, s¹₁, s²₂), (a¹₁, s¹₂, s²₂), (a¹₁, s¹₁, s²₃), (a¹₁, s¹₂, s²₃)\n    # Equivalent to CartesianIndices(actions_dims..., state_dims...), i.e. actions first, then states in lexicographic order\n    lower = [\n        1/15  7/30  1/15  13/30  4/15  1/6\n        2/5   7/30  1/30  11/30  2/15  1/10\n    ],\n    upper = [\n        17/30  7/10   2/3   4/5  7/10  2/3\n        9/10   13/15  9/10  5/6  4/5   14/15\n    ]\n), state_indices, action_indices, state_dims, action_dims)\n\nstate_indices = (2,)\naction_indices = (2,)\nstate_dims = (3,)\naction_dims = (2,)\nmarginal2 = Marginal(IntervalAmbiguitySets(;\n    # 6 ambiguity sets = 3 source states, 2 actions\n    # Column layout: (a²₁, s²₁), (a²₂, s²₁), (a²₁, s²₂), (a²₂, s²₂), (a²₁, s²₃), (a²₂, s²₃)\n    # Equivalent to CartesianIndices(actions_dims..., state_dims...), i.e. actions first, then states in lexicographic order\n    lower = [\n        1/30  1/3   1/6   1/15  2/5   2/15\n        4/15  1/4   1/6   1/30  2/15  1/30\n        2/15  7/30  1/10  7/30  7/15  1/5\n    ],\n    upper = [\n        2/3    7/15  4/5    11/30  19/30  1/2\n        23/30  4/5   23/30  3/5    7/10   8/15\n        7/15   4/5   23/30  7/10   7/15   23/30\n    ]\n), state_indices, action_indices, state_dims, action_dims)\n\ninitial_states = [(1, 1)]  # Initial states are optional\nmdp = FactoredRobustMarkovDecisionProcess(state_vars, action_vars, (marginal1, marginal2), initial_states)\n\n# output\n\nFactoredRobustMarkovDecisionProcess\n├─ 2 state variables with cardinality: (2, 3)\n├─ 2 action variables with cardinality: (1, 2)\n├─ Initial states: [(1, 1)]\n├─ Transition marginals:\n│  ├─ Marginal 1:\n│  │  ├─ Conditional variables: states = (1, 2), actions = (1,)\n│  │  └─ Ambiguity set type: Interval (dense, Matrix{Float64})\n│  └─ Marginal 2:\n│     ├─ Conditional variables: states = (2,), actions = (2,)\n│     └─ Ambiguity set type: Interval (dense, Matrix{Float64})\n└─Inferred properties\n   ├─Model type: Factored Interval MDP\n   ├─Number of states: 6\n   ├─Number of actions: 2\n   ├─Default model checking algorithm: Robust Value Iteration\n   └─Default Bellman operator algorithm: Recursive O-Maximization\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.state_values-Tuple{FactoredRobustMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.state_values","text":"state_values(mdp::FactoredRMDP)\n\nReturn a tuple with the number of states for each state variable in the fRMDP.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.action_values-Tuple{FactoredRobustMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.action_values","text":"action_values(mdp::FactoredRMDP)\n\nReturn a tuple with the number of actions for each action variable in the fRMDP.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.marginals-Tuple{FactoredRobustMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.marginals","text":"marginals(mdp::FactoredRMDP)\n\nReturn the marginals of the fRMDP.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.IntervalMarkovChain","page":"Systems","title":"IntervalMDP.IntervalMarkovChain","text":"IntervalMarkovChain(ambiguity_set::IntervalAmbiguitySets, initial_states=AllStates())\n\nA convenience constructor for a FactoredRobustMarkovDecisionProcess representing an interval Markov chain, as IMCs are a subclass of fRMDPs, from a single IntervalAmbiguitySets object.\n\nFormally, an IMC M is a tuple M = (S S_0 Gamma), where\n\nS is a finite set of states,\nS_0 subseteq S is a set of initial states,\nGamma = Gamma_s_s in S is a set of ambiguity sets for source state s,   where each Gamma_s is an interval ambiguity set over S.\n\nNotice also that an IMC is an IntervalMarkovDecisionProcess with a single action.\n\nExample\n\nusing IntervalMDP\n\nprob = IntervalAmbiguitySets(;\n    lower = [\n        0     1/2   0\n        1/10  3/10  0\n        1/5   1/10  1\n    ],\n    upper = [\n        1/2   7/10  0\n        3/5   1/2   0\n        7/10  3/10  1\n    ],\n)\n\ninitial_states = [1]\nmc = IntervalMarkovChain(prob, initial_states)\n\n# output\n\nFactoredRobustMarkovDecisionProcess\n├─ 1 state variables with cardinality: (3,)\n├─ 1 action variables with cardinality: (1,)\n├─ Initial states: [1]\n├─ Transition marginals:\n│  └─ Marginal 1:\n│     ├─ Conditional variables: states = (1,), actions = (1,)\n│     └─ Ambiguity set type: Interval (dense, Matrix{Float64})\n└─Inferred properties\n   ├─Model type: Interval MDP\n   ├─Number of states: 3\n   ├─Number of actions: 1\n   ├─Default model checking algorithm: Robust Value Iteration\n   └─Default Bellman operator algorithm: O-Maximization\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.IntervalMarkovDecisionProcess","page":"Systems","title":"IntervalMDP.IntervalMarkovDecisionProcess","text":"IntervalMarkovDecisionProcess(ambiguity_set::IntervalAmbiguitySets, num_actions::Integer, initial_states::InitialStates = AllStates())\n\nA convenience constructor for a FactoredRobustMarkovDecisionProcess representing an interval Markov decision process, as IMDPs are a subclass of fRMDPs, from a single IntervalAmbiguitySets object and a specified number of actions.\n\nFormally, an IMDP M is a tuple M = (S S_0 A Gamma), where\n\nS is a finite set of states,\nS_0 subseteq S is a set of initial states,\nA is a finite set of actions,\n`Gamma = Gamma_sa_s in Sa in A is a set of ambiguity sets for source-action pair (s a), where each Gamma_sa is an interval ambiguity set over S.\n\nExample\n\nusing IntervalMDP\n\nprob1 = IntervalAmbiguitySets(;\n    lower = [\n        0    1/2\n        1/10 3/10\n        1/5  1/10\n    ],\n    upper = [\n        1/2  7/10\n        3/5  1/2\n        7/10 3/10\n    ],\n)\n\nprob2 = IntervalAmbiguitySets(;\n    lower = [\n        1/10 1/5\n        1/5  3/10\n        3/10 2/5\n    ],\n    upper = [\n        3/5 3/5\n        1/2 1/2\n        2/5 2/5\n    ],\n)\n\nprob3 = IntervalAmbiguitySets(;\n    lower = Float64[\n        0 0\n        0 0\n        1 1\n    ],\n    upper = Float64[\n        0 0\n        0 0\n        1 1\n    ]\n)\n\ninitial_states = [1]\nmdp = IntervalMarkovDecisionProcess([prob1, prob2, prob3], initial_states)\n\n# output\n\nFactoredRobustMarkovDecisionProcess\n├─ 1 state variables with cardinality: (3,)\n├─ 1 action variables with cardinality: (2,)\n├─ Initial states: [1]\n├─ Transition marginals:\n│  └─ Marginal 1:\n│     ├─ Conditional variables: states = (1,), actions = (1,)\n│     └─ Ambiguity set type: Interval (dense, Matrix{Float64})\n└─Inferred properties\n   ├─Model type: Interval MDP\n   ├─Number of states: 3\n   ├─Number of actions: 2\n   ├─Default model checking algorithm: Robust Value Iteration\n   └─Default Bellman operator algorithm: O-Maximization\n\n\n\n\n\nIntervalMarkovDecisionProcess(ps::Vector{<:IntervalAmbiguitySets}, initial_states::InitialStates = AllStates())\n\nA convenience constructor for a FactoredRobustMarkovDecisionProcess representing an interval Markov decision process from a vector of IntervalAmbiguitySets objects, one for each state and with the same number of actions in each. \n\nExample\n\nusing IntervalMDP\n\nprob = IntervalAmbiguitySets(;\n    lower = [\n        0    1/2  1/10 1/5  0 0\n        1/10 3/10 1/5  3/10 0 0\n        1/5  1/10 3/10 2/5  1 1\n    ],\n    upper = [\n        1/2  7/10 3/5 2/5 0 0\n        3/5  1/2  1/2 2/5 0 0\n        7/10 3/10 2/5 2/5 1 1\n    ],\n)\n\nnum_actions = 2\ninitial_states = [1]\nmdp = IntervalMarkovDecisionProcess(prob, num_actions, initial_states)\n\n# output\n\nFactoredRobustMarkovDecisionProcess\n├─ 1 state variables with cardinality: (3,)\n├─ 1 action variables with cardinality: (2,)\n├─ Initial states: [1]\n├─ Transition marginals:\n│  └─ Marginal 1:\n│     ├─ Conditional variables: states = (1,), actions = (1,)\n│     └─ Ambiguity set type: Interval (dense, Matrix{Float64})\n└─Inferred properties\n   ├─Model type: Interval MDP\n   ├─Number of states: 3\n   ├─Number of actions: 2\n   ├─Default model checking algorithm: Robust Value Iteration\n   └─Default Bellman operator algorithm: O-Maximization\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.Marginal","page":"Systems","title":"IntervalMDP.Marginal","text":"Marginal{A <: AbstractAmbiguitySets, N, M, I <: LinearIndices}\n\nA struct to represent the dependency graph of an fRMDP, namely by subselecting (in getindex) the (decomposed) state and action. Furthermore, the struct is responsible for converting the Cartesian index to a linear index for the underlying ambiguity sets.\n\ntodo: Todo\nDescribe source_dims\n\ntodo: Todo\nAdd example\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.ambiguity_sets-Tuple{Marginal}","page":"Systems","title":"IntervalMDP.ambiguity_sets","text":"ambiguity_sets(p::Marginal)\n\nReturn the underlying ambiguity sets of the marginal.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.state_variables-Tuple{Marginal}","page":"Systems","title":"IntervalMDP.state_variables","text":"state_variables(p::Marginal)\n\nReturn the state variable indices of the marginal.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.action_variables-Tuple{Marginal}","page":"Systems","title":"IntervalMDP.action_variables","text":"action_variables(p::Marginal)\n\nReturn the action variable indices of the marginal.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.source_shape-Tuple{Marginal}","page":"Systems","title":"IntervalMDP.source_shape","text":"source_shape(p::Marginal)\n\nReturn the shape of the source (state) variables of the marginal. The FactoredRobustMarkovDecisionProcess  checks if this is less than or equal to the corresponding state values.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.action_shape-Tuple{Marginal}","page":"Systems","title":"IntervalMDP.action_shape","text":"action_shape(p::Marginal)\n\nReturn the shape of the action variables of the marginal. The FactoredRobustMarkovDecisionProcess checks if this is equal to the corresponding action values.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#Base.getindex-Tuple{Marginal, Any, Any}","page":"Systems","title":"Base.getindex","text":"getindex(p::Marginal, action, source)\n\nGet the ambiguity set corresponding to the given source (state) and action, where  the relevant indices of source and action are selected by p.action_indices and p.state_indices respectively. The selected index is then converted to a linear index for the underlying ambiguity sets.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_sets","page":"Systems","title":"IntervalMDP.num_sets","text":"num_sets(ambiguity_sets::AbstractAmbiguitySets)\n\nReturn the number of ambiguity sets in the AbstractAmbiguitySets object.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.num_target","page":"Systems","title":"IntervalMDP.num_target","text":"num_target(p::Marginal)\n\nReturn the number of target states of the marginal.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.support","page":"Systems","title":"IntervalMDP.support","text":"support(ambiguity_set::AbstractAmbiguitySet)\n\nReturn the support (set of indices with non-zero probability) of the ambiguity set.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.IntervalAmbiguitySets","page":"Systems","title":"IntervalMDP.IntervalAmbiguitySets","text":"IntervalAmbiguitySets{R, MR <: AbstractMatrix{R}}\n\nA matrix pair to represent the lower and upper bound of num_sets(ambiguity_sets) interval ambiguity sets (on the columns) to num_target(ambiguity_sets) destinations (on the rows). Marginal adds interpretation to the column indices. The matrices can be Matrix{R} or SparseMatrixCSC{R}, or their CUDA equivalents.  Due to the space complexity, if modelling IntervalMarkovChains or IntervalMarkovDecisionProcesses, it is recommended to use sparse matrices.\n\nThe columns represent the different ambiguity sets and the rows represent the targets. Due to the column-major format of Julia,  this is a more efficient representation in terms of cache locality.\n\nThe lower bound is explicitly stored, while the upper bound is computed from the lower bound and the gap. This choice is  because it simplifies repeated probability assignment using O-maximization [7, 8].\n\nFields\n\nlower::MR: The lower bound probabilities for num_sets(ambiguity_sets) ambiguity sets to num_target(ambiguity_sets) target states.\ngap::MR: The gap between upper and lower bound transition probabilities for num_sets(ambiguity_sets) ambiguity sets to num_target(ambiguity_sets) target states.\n\nExamples\n\nusing IntervalMDP\n\ndense_prob = IntervalAmbiguitySets(;\n    lower = [0.0 0.5; 0.1 0.3; 0.2 0.1],\n    upper = [0.5 0.7; 0.6 0.5; 0.7 0.3],\n)\n\n# output\n\nIntervalAmbiguitySets\n├─ Storage type: Matrix{Float64}\n├─ Number of target states: 3\n└─ Number of ambiguity sets: 2\n\nusing IntervalMDP, SparseArrays\nsparse_prob = IntervalAmbiguitySets(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\n# output\n\nIntervalAmbiguitySets\n├─ Storage type: SparseArrays.FixedSparseCSC{Float64, Int64}\n├─ Number of target states: 15\n├─ Number of ambiguity sets: 2\n├─ Maximum support size: 3\n└─ Number of non-zeros: 6\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.lower","page":"Systems","title":"IntervalMDP.lower","text":"lower(p::IntervalAmbiguitySet)\n\nReturn the lower bound transition probabilities of the ambiguity set to all target states.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.upper","page":"Systems","title":"IntervalMDP.upper","text":"upper(p::IntervalAmbiguitySet)\n\nReturn the upper bound transition probabilities of the ambiguity set to all target states.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.gap","page":"Systems","title":"IntervalMDP.gap","text":"gap(p::IntervalAmbiguitySet)\n\nReturn the gap between upper and lower bound transition probabilities of the ambiguity set to all target states.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.DFA","page":"Systems","title":"IntervalMDP.DFA","text":"DFA{\n    T <: TransitionFunction,  \n    VT <: AbstractVector{Int32},\n    DA <: AbstractDict{String, Int32}\n}\n\nA type representing Deterministic Finite Automaton (DFA) which are finite automata with deterministic transitions.\n\nFormally, let (Q 2^AP delta q_0 Q_ac) be an DFA, where \n\nQ is the set of states,\nQ_ac subseteq Q is the set of accepting states,\nQ_0 is the initial state,\n2^AP is the power set of automic propositions, and\ndelta  Q times 2^AP = Q is the deterministic transition function, for each state-input pair.\n\nThen the DFA type is defined as follows: indices 1:num_states are the states in Q,  transition represents delta, the set 2^AP is , and initial_state is the initial state q_0.  See TransitionFunction for more information on the structure of the transition function.\n\nFields\n\ntransition::T: transition function.\ninitial_state::Int32: initial states.\naccepting_states::VT: vector of accepting states\nlabelmap::DA: mapping from label to index.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.num_states-Tuple{DFA}","page":"Systems","title":"IntervalMDP.num_states","text":"num_states(dfa::DFA)\n\nReturn the number of states Q of the Deterministic Finite Automaton.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_labels-Tuple{DFA}","page":"Systems","title":"IntervalMDP.num_labels","text":"num_labels(dfa::DFA)\n\nReturn the number of labels (DFA inputs) in the Deterministic Finite Automaton.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.transition-Tuple{DFA}","page":"Systems","title":"IntervalMDP.transition","text":"transition(dfa::DFA)\n\nReturn the transition object of the Deterministic Finite Automaton. \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.labelmap-Tuple{DFA}","page":"Systems","title":"IntervalMDP.labelmap","text":"labelmap(dfa::DFA)\n\nReturn the label index mapping 2^AP to mathbbN of the Deterministic Finite Automaton. \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.initial_state-Tuple{DFA}","page":"Systems","title":"IntervalMDP.initial_state","text":"initial_state(dfa::DFA)\n\nReturn the initial state of the Deterministic Finite Automaton. \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.ProductProcess","page":"Systems","title":"IntervalMDP.ProductProcess","text":"struct ProductProcess{\n    M <: IntervalMarkovProcess,\n    D <: DeterministicAutomaton,\n    L <: AbstractLabelling,\n}\n\nA type representing the product between interval Markov processes (e.g. FactoredRobustMarkovDecisionProcess) and an automaton (typically a deterministic finite automaton DFA). \n\nFormally, given an interval Markov process M = (S A Gamma S_0), a labelling function L  S to 2^AP, and a DFA D = (Q 2^AP delta q_0 Q_ac), then a product process is a tuple M_prod = (Z A Gamma^prod Z_ac Z_0) where \n\nZ = S times Q is the set of product states q = (s, z)``,\nQ_0 = S_0 times q_0 subset Z is the set of initial product states z_0 = (s_0 q_0),\nZ_ac = S times Q_ac subseteq Z is the set of accepting product states,\nA is the set of actions, and\nGamma^prod = Gamma^prod_za_z in Z a in A where Gamma^prod_za =  gamma_za  gamma_za((t z)) = gamma_sa(t)delta_qL(s)(z) \n\nis a set of ambiguity sets on the product transition probabilities, for each product source-action pair.\n\nSee FactoredRobustMarkovDecisionProcess and DFA for more information on the structure, definition, and usage of the DFA and IMDP.\n\nFields\n\nmdp::M: contains details for the interval Markov process.\ndfa::D: contains details for the DFA\nlabelling_func::L: the labelling function from IMDP states to DFA actions\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.markov_process-Tuple{ProductProcess}","page":"Systems","title":"IntervalMDP.markov_process","text":"markov_process(proc::ProductIntervalMarkovDecisionProcessDFA)\n\nReturn the interval markov decision process of the product \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.automaton-Tuple{ProductProcess}","page":"Systems","title":"IntervalMDP.automaton","text":"automaton(proc::ProductIntervalMarkovDecisionProcessDFA)\n\nReturn the deterministic finite automaton of the product \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.labelling_function-Tuple{ProductProcess}","page":"Systems","title":"IntervalMDP.labelling_function","text":"labelling_function(proc::ProductProcess)\n\nReturn the labelling function of the product \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.TransitionFunction","page":"Systems","title":"IntervalMDP.TransitionFunction","text":"struct TransitionFunction{\n    T <: Integer, \n    MT <: AbstractMatrix{T}\n}\n\nA type representing the determininistic transition function of a DFA.\n\nFormally, let T  Q times 2^AP = Q be a transition function, where \n\nQ is the set of DFA states, and\n2^AP is the power set of atomic propositions\n\nThen the TransitionFunction type is defined as matrix which stores the mapping. The row indices are the alphabet indices and the column indices represent the states.  \n\nFields\n\ntransition::MT: transition functions encoded as matrix with labels on the rows, source states on the columns, and integer values for the destination.\n\nThe choice to have labels on the rows is due to the column-major storage of matrices in Julia and the fact that we want the outer loop over DFA source states  in the Bellman operator bellman!.\n\nWe check that the transition matrix is valid, i.e. that all indices are positive and do not exceed the number of states.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.transition-Tuple{TransitionFunction}","page":"Systems","title":"IntervalMDP.transition","text":"transition(transition_func::TransitionFunction)\n\nReturn the transition matrix of the transition function. \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_states-Tuple{TransitionFunction}","page":"Systems","title":"IntervalMDP.num_states","text":"num_states(tf::TransitionFunction)\n\nReturn the number of states Q of the transition function.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_labels-Tuple{TransitionFunction}","page":"Systems","title":"IntervalMDP.num_labels","text":"num_labels(tf::TransitionFunction)\n\nReturn the number of labels (DFA inputs) in the transition function.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.DeterministicLabelling","page":"Systems","title":"IntervalMDP.DeterministicLabelling","text":"struct DeterministicLabelling{\n    T  <: Integer, \n    AT <: AbstractArray{T}\n}\n\nA type representing the labelling of IMDP states into DFA inputs.\n\nFormally, let L  S to 2^AP be a labelling function, where \n\nS is the set of IMDP states, and\n2^AP is the power set of atomic propositions\n\nThen the DeterministicLabelling type is defined as vector which stores the mapping. \n\nFields\n\nmap::AT: mapping function where indices are (factored) IMDP states and stored values are DFA inputs.\nnum_outputs::Int32: number of labels accounted for in mapping.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.mapping-Tuple{DeterministicLabelling}","page":"Systems","title":"IntervalMDP.mapping","text":"mapping(dl::DeterministicLabelling)\n\nReturn the mapping array of the labelling function. \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_labels-Tuple{DeterministicLabelling}","page":"Systems","title":"IntervalMDP.num_labels","text":"num_labels(dl::DeterministicLabelling)\n\nReturn the number of labels (DFA inputs) in the labelling function.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.state_values-Tuple{DeterministicLabelling}","page":"Systems","title":"IntervalMDP.state_values","text":"state_values(dl::DeterministicLabelling)\n\nReturn a tuple with the number of states for each state variable of the labeling function L  S to 2^AP, which can be multiple dimensions in case of factored IMDPs. \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.ProbabilisticLabelling","page":"Systems","title":"IntervalMDP.ProbabilisticLabelling","text":"struct ProbabilisticLabelling{\n    R <: Real, \n    MR <: AbstractArray{R}\n}\n\nA type representing the Probabilistic labelling of IMDP states into DFA inputs. Each labelling is assigned a probability.\n\nFormally, let L  S times 2^AP to 0 1 be a labelling function, where \n\nS is the set of IMDP states, and\n2^AP is the power set of atomic propositions\n\nThen the ProbabilisticLabelling type is defined as matrix which stores the mapping. \n\nFields\n\nmap::MT: mapping function encoded as matrix with labels on the rows, IMDP states on the columns, and valid probability values for the destination.\n\nThe choice to have labels on the rows is due to the column-major storage of matrices in Julia and the fact that we want the inner loop over DFA target states  in the Bellman operator bellman!.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.mapping-Tuple{ProbabilisticLabelling}","page":"Systems","title":"IntervalMDP.mapping","text":"mapping(pl::ProbabilisticLabelling)\n\nReturn the mapping matrix of the probabilistic labelling function. \n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_labels-Tuple{ProbabilisticLabelling}","page":"Systems","title":"IntervalMDP.num_labels","text":"num_labels(pl::ProbabilisticLabelling)\n\nReturn the number of labels (DFA inputs) in the probabilistic labelling function.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.state_values-Tuple{ProbabilisticLabelling}","page":"Systems","title":"IntervalMDP.state_values","text":"state_values(pl::ProbabilisticLabelling)\n\nReturn a tuple with the number of states for each state variable of the labeling function L  S to 2^AP, which can be multiple dimensions in case of factored IMDPs. \n\n\n\n\n\n","category":"method"},{"location":"reference/solve/#Solve-Interface","page":"Solve Interface","title":"Solve Interface","text":"","category":"section"},{"location":"reference/solve/#VI-like-Algorithms","page":"Solve Interface","title":"VI-like Algorithms","text":"","category":"section"},{"location":"reference/solve/#CommonSolve.solve","page":"Solve Interface","title":"CommonSolve.solve","text":"solve(problem::AbstractIntervalMDPProblem, alg::RobustValueIteration; callback=nothing)\n\nSolve minimizes/maximizes optimistic/pessimistic specification problems using value iteration for interval Markov processes. \n\nIt is possible to provide a callback function that will be called at each iteration with the current value function and iteration count. The callback function should have the signature callback(V::AbstractArray, k::Int).\n\nsolve can be called without specifying the algorithm, in which case it defaults to RobustValueIteration.\n\nExamples\n\nusing IntervalMDP\n\nprob1 = IntervalAmbiguitySets(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalAmbiguitySets(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalAmbiguitySets(;\n    lower = [\n        0.0 0.0\n        0.0 0.0\n        1.0 1.0\n    ],\n    upper = [\n        0.0 0.0\n        0.0 0.0\n        1.0 1.0\n    ]\n)\n\ntransition_probs = [prob1, prob2, prob3]\ninitial_state = [1]\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_state)\n\n# output\n\nFactoredRobustMarkovDecisionProcess\n├─ 1 state variables with cardinality: (3,)\n├─ 1 action variables with cardinality: (2,)\n├─ Initial states: [1]\n├─ Transition marginals:\n│  └─ Marginal 1:\n│     ├─ Conditional variables: states = (1,), actions = (1,)\n│     └─ Ambiguity set type: Interval (dense, Matrix{Float64})\n└─Inferred properties\n   ├─Model type: Interval MDP\n   ├─Number of states: 3\n   ├─Number of actions: 2\n   ├─Default model checking algorithm: Robust Value Iteration\n   └─Default Bellman operator algorithm: O-Maximization\n\nreach_states = [3]\ntime_horizon = 10\nprop = FiniteTimeReachability(reach_states, time_horizon)\nspec = Specification(prop, Pessimistic, Maximize)\n\n# output\n\nSpecification\n├─ Satisfaction mode: Pessimistic\n├─ Strategy mode: Maximize\n└─ Property: FiniteTimeReachability\n   ├─ Time horizon: 10\n   └─ Reach states: CartesianIndex{1}[CartesianIndex(3,)]\n\n# Verification\nproblem = VerificationProblem(mdp, spec)\nsol = solve(problem, RobustValueIteration(default_bellman_algorithm(mdp)); callback = (V, k) -> println(\"Iteration \", k))\nV, k, res = sol  # or `value_function(sol), num_iterations(sol), residual(sol)`\n\n# output\n\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nIteration 5\nIteration 6\nIteration 7\nIteration 8\nIteration 9\nIteration 10\nIntervalMDP.VerificationSolution{Float64, Vector{Float64}, Nothing}([0.9597716063999999, 0.9710050144, 1.0], [0.01593864639999998, 0.011487926399999848, -0.0], 10, nothing)\n\n\n# Control synthesis\nproblem = ControlSynthesisProblem(mdp, spec)\nsol = solve(problem, RobustValueIteration(default_bellman_algorithm(mdp)); callback = (V, k) -> println(\"Iteration \", k))\nσ, V, k, res = sol # or `strategy(sol), value_function(sol), num_iterations(sol), residual(sol)`\n\n# output\n\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nIteration 5\nIteration 6\nIteration 7\nIteration 8\nIteration 9\nIteration 10\nIntervalMDP.ControlSynthesisSolution{TimeVaryingStrategy{1, Vector{Tuple{Int32}}}, Float64, Vector{Float64}, Nothing}(TimeVaryingStrategy{1, Vector{Tuple{Int32}}}(Vector{Tuple{Int32}}[[(1,), (2,), (1,)], [(1,), (2,), (1,)], [(1,), (2,), (1,)], [(1,), (2,), (1,)], [(1,), (2,), (1,)], [(1,), (2,), (1,)], [(1,), (2,), (1,)], [(1,), (2,), (1,)], [(1,), (2,), (1,)], [(1,), (2,), (1,)]]), [0.9597716063999999, 0.9710050144, 1.0], [0.01593864639999998, 0.011487926399999848, -0.0], 10, nothing)\n\n\n\n\n\n","category":"function"},{"location":"reference/solve/#IntervalMDP.residual","page":"Solve Interface","title":"IntervalMDP.residual","text":"residual(s::VerificationSolution)\n\nReturn the residual of a verification solution.\n\n\n\n\n\nresidual(s::ControlSynthesisSolution)\n\nReturn the residual of a control synthesis solution.\n\n\n\n\n\n","category":"function"},{"location":"reference/solve/#IntervalMDP.num_iterations","page":"Solve Interface","title":"IntervalMDP.num_iterations","text":"num_iterations(s::VerificationSolution)\n\nReturn the number of iterations of a verification solution.\n\n\n\n\n\nnum_iterations(s::ControlSynthesisSolution)\n\nReturn the number of iterations of a control synthesis solution.\n\n\n\n\n\n","category":"function"},{"location":"reference/solve/#IntervalMDP.value_function","page":"Solve Interface","title":"IntervalMDP.value_function","text":"value_function(s::VerificationSolution)\n\nReturn the value function of a verification solution.\n\n\n\n\n\nvalue_function(s::ControlSynthesisSolution)\n\nReturn the value function of a control synthesis solution.\n\n\n\n\n\n","category":"function"},{"location":"reference/solve/#IntervalMDP.strategy-Tuple{IntervalMDP.ControlSynthesisSolution}","page":"Solve Interface","title":"IntervalMDP.strategy","text":"strategy(s::ControlSynthesisSolution)\n\nReturn the strategy of a control synthesis solution.\n\n\n\n\n\n","category":"method"},{"location":"reference/solve/#IntervalMDP.StationaryStrategy","page":"Solve Interface","title":"IntervalMDP.StationaryStrategy","text":"StationaryStrategy\n\nA stationary strategy is a strategy that is the same for all time steps.\n\n\n\n\n\n","category":"type"},{"location":"reference/solve/#IntervalMDP.TimeVaryingStrategy","page":"Solve Interface","title":"IntervalMDP.TimeVaryingStrategy","text":"TimeVaryingStrategy\n\nA time-varying strategy is a strategy that may vary over time. Since we need to store the strategy for each time step,  the strategy is finite, and thus only applies to finite time specifications, of the same length as the strategy.\n\n\n\n\n\n","category":"type"},{"location":"reference/solve/#IntervalMDP.RobustValueIteration","page":"Solve Interface","title":"IntervalMDP.RobustValueIteration","text":"RobustValueIteration\n\nA robust value iteration algorithm for solving interval Markov decision processes (IMDPs) with interval ambiguity sets. This algorithm is designed to handle both finite and infinite time specifications, optimizing for either the maximum or minimum expected value based on the given specification.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#Problem","page":"Specifications","title":"Problem","text":"","category":"section"},{"location":"reference/specifications/#Reachability","page":"Specifications","title":"Reachability","text":"","category":"section"},{"location":"reference/specifications/#Reach-avoid","page":"Specifications","title":"Reach-avoid","text":"","category":"section"},{"location":"reference/specifications/#Safety","page":"Specifications","title":"Safety","text":"","category":"section"},{"location":"reference/specifications/#Reward-specification","page":"Specifications","title":"Reward specification","text":"","category":"section"},{"location":"reference/specifications/#Hitting-time","page":"Specifications","title":"Hitting time","text":"","category":"section"},{"location":"reference/specifications/#DFA-Reachability","page":"Specifications","title":"DFA Reachability","text":"","category":"section"},{"location":"reference/specifications/#DFA-Safety","page":"Specifications","title":"DFA Safety","text":"","category":"section"},{"location":"reference/specifications/#IntervalMDP.VerificationProblem","page":"Specifications","title":"IntervalMDP.VerificationProblem","text":"VerificationProblem{S <: StochasticProcess, F <: Specification, C <: AbstractStrategy}\n\nA verification problem is a tuple of an interval Markov process and a specification.\n\nFields\n\nsystem::S: interval Markov process.\nspec::F: specification (either temporal logic or reachability-like).\nstrategy::C: strategy to be used for verification, which can be a given strategy or a no strategy,  i.e. select (but do not store! see [ControlSynthesisProblem]) optimal action for every state at every timestep.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.ControlSynthesisProblem","page":"Specifications","title":"IntervalMDP.ControlSynthesisProblem","text":"ControlSynthesisProblem{S <: StochasticProcess, F <: Specification}\n\nA verification problem is a tuple of an interval Markov process and a specification.\n\nFields\n\nsystem::S: interval Markov process.\nspec::F: specification (either temporal logic or reachability-like).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.system","page":"Specifications","title":"IntervalMDP.system","text":"system(prob::VerificationProblem)\n\nReturn the system of a problem.\n\n\n\n\n\nsystem(prob::ControlSynthesisProblem)\n\nReturn the system of a problem.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.specification","page":"Specifications","title":"IntervalMDP.specification","text":"specification(prob::VerificationProblem)\n\nReturn the specification of a problem.\n\n\n\n\n\nspecification(prob::ControlSynthesisProblem)\n\nReturn the specification of a problem.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.strategy-Tuple{VerificationProblem}","page":"Specifications","title":"IntervalMDP.strategy","text":"strategy(prob::VerificationProblem)\n\nReturn the strategy of a problem, if provided.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.Specification","page":"Specifications","title":"IntervalMDP.Specification","text":"Specification{F <: Property}\n\nA specfication is a property together with a satisfaction mode and a strategy mode.  The satisfaction mode is either Optimistic or Pessimistic. See SatisfactionMode for more details. The strategy  mode is either Maxmize or Minimize. See StrategyMode for more details.\n\nFields\n\nprop::F: verification property (either temporal logic or reachability-like).\nsatisfaction::SatisfactionMode: satisfaction mode (either optimistic or pessimistic). Default is pessimistic.\nstrategy::StrategyMode: strategy mode (either maximize or minimize). Default is maximize.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.system_property","page":"Specifications","title":"IntervalMDP.system_property","text":"system_property(spec::Specification)\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.satisfaction_mode","page":"Specifications","title":"IntervalMDP.satisfaction_mode","text":"satisfaction_mode(spec::Specification)\n\nReturn the satisfaction mode of a specification.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.SatisfactionMode","page":"Specifications","title":"IntervalMDP.SatisfactionMode","text":"SatisfactionMode\n\nWhen computing the satisfaction probability of a property over an interval Markov process, be it IMC or IMDP, the desired satisfaction probability to verify can either be Optimistic or Pessimistic. That is, upper and lower bounds on the satisfaction probability within the probability uncertainty.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.strategy_mode","page":"Specifications","title":"IntervalMDP.strategy_mode","text":"strategy_mode(spec::Specification)\n\nReturn the strategy mode of a specification.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.StrategyMode","page":"Specifications","title":"IntervalMDP.StrategyMode","text":"StrategyMode\n\nWhen computing the satisfaction probability of a property over an IMDP, the strategy can either maximize or minimize the satisfaction probability (wrt. the satisfaction mode).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeReachability","page":"Specifications","title":"IntervalMDP.FiniteTimeReachability","text":"FiniteTimeReachability{VT <: Vector{Union{<:Integer, <:Tuple, <:CartesianIndex}}, T <: Integer}\n\nFinite time reachability specified by a set of target/terminal states and a time horizon.  That is, denote a trace by omega = s_1 s_2 s_3 cdots, then if G is the set of target states and K is the time horizon, the property is \n\n    mathbbP^pi eta_mathrmreach(G K) = mathbbP^pi eta leftomega in Omega  exists k in 0 ldots K  omegak in G right\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::FiniteTimeReachability)\n\nReturn the set of states with respect to which to compute reachbility for a finite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReachability)\n\nReturn the time horizon of a finite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeReachability","page":"Specifications","title":"IntervalMDP.InfiniteTimeReachability","text":"InfiniteTimeReachability{VT <: Vector{Union{<:Integer, <:Tuple, <:CartesianIndex}}, R <: Real}\n\nInfiniteTimeReachability is similar to FiniteTimeReachability except that the time horizon is infinite, i.e., K = infty. In practice it means, performing the value iteration until the value function has converged, defined by some threshold convergence_eps. The convergence threshold is that the largest value of the most recent Bellman residual is less than convergence_eps.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::InfiniteTimeReachability)\n\nReturn the set of states with which to compute reachbility for a infinite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReachability)\n\nReturn the convergence threshold of an infinite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.ExactTimeReachability","page":"Specifications","title":"IntervalMDP.ExactTimeReachability","text":"ExactTimeReachability{VT <: Vector{Union{<:Integer, <:Tuple, <:CartesianIndex}}, T <: Integer}\n\nExact time reachability specified by a set of target/terminal states and a time horizon.  That is, denote a trace by omega = s_1 s_2 s_3 cdots, then if G is the set of target states and K is the time horizon, the property is \n\n    mathbbP^pi eta_mathrmexact-reach(G K) = mathbbP^pi eta leftomega in Omega  omegaK in G right\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{ExactTimeReachability}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::ExactTimeReachability)\n\nReturn the set of states with which to compute reachbility for an exact time reachability prop.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{ExactTimeReachability}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::ExactTimeReachability)\n\nReturn the time horizon of an exact time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeReachAvoid","page":"Specifications","title":"IntervalMDP.FiniteTimeReachAvoid","text":"FiniteTimeReachAvoid{VT <: Vector{Union{<:Integer, <:Tuple, <:CartesianIndex}}}, T <: Integer}\n\nFinite time reach-avoid specified by a set of target/terminal states, a set of avoid states, and a time horizon. That is, denote a trace by omega = s_1 s_2 s_3 cdots, then if G is the set of target states, O is the set of states to avoid, and K is the time horizon, the property is \n\n    mathbbP^pi eta_mathrmreach-avoid(G O K) = mathbbP^pi eta leftomega in Omega  exists k in 0 ldots K  omegak in G  forall k in 0 ldots k   omegak notin O right\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::FiniteTimeReachAvoid)\n\nReturn the set of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::FiniteTimeReachAvoid)\n\nReturn the set of states to avoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReachAvoid)\n\nReturn the time horizon of a finite time reach-avoid property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeReachAvoid","page":"Specifications","title":"IntervalMDP.InfiniteTimeReachAvoid","text":"InfiniteTimeReachAvoid{VT <: Vector{Union{<:Integer, <:Tuple, <:CartesianIndex}}, R <: Real}\n\nInfiniteTimeReachAvoid is similar to FiniteTimeReachAvoid except that the time horizon is infinite, i.e., K = infty.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::InfiniteTimeReachAvoid)\n\nReturn the set of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::InfiniteTimeReachAvoid)\n\nReturn the set of states to avoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReachAvoid)\n\nReturn the convergence threshold of an infinite time reach-avoid property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.ExactTimeReachAvoid","page":"Specifications","title":"IntervalMDP.ExactTimeReachAvoid","text":"ExactTimeReachAvoid{VT <: Vector{Union{<:Integer, <:Tuple, <:CartesianIndex}}}, T <: Integer}\n\nExact time reach-avoid specified by a set of target/terminal states, a set of avoid states, and a time horizon. That is, denote a trace by omega = s_1 s_2 s_3 cdots, then if G is the set of target states, O is the set of states to avoid, and K is the time horizon, the property is \n\n    mathbbP^pi eta_mathrmexact-reach-avoid(G O K) = mathbbP^pi eta leftomega in Omega  omegaK in G  forall k in 0 ldots K  omegak notin O right\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{ExactTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::ExactTimeReachAvoid)\n\nReturn the set of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{ExactTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::ExactTimeReachAvoid)\n\nReturn the set of states to avoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{ExactTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::ExactTimeReachAvoid)\n\nReturn the time horizon of an exact time reach-avoid property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeSafety","page":"Specifications","title":"IntervalMDP.FiniteTimeSafety","text":"FiniteTimeSafety{VT <: Vector{Union{<:Integer, <:Tuple, <:CartesianIndex}}, T <: Integer}\n\nFinite time safety specified by a set of avoid states and a time horizon.  That is, denote a trace by omega = s_1 s_2 s_3 cdots, then if O is the set of avoid states and K is the time horizon, the property is \n\n    mathbbP^pi eta_mathrmsafe(O K) = mathbbP^pi eta leftomega in Omega  forall k in 0 ldots K  omegak notin O right\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{FiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::FiniteTimeSafety)\n\nReturn the set of states with which to compute reachbility for a finite time reachability prop.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeSafety)\n\nReturn the time horizon of a finite time safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeSafety","page":"Specifications","title":"IntervalMDP.InfiniteTimeSafety","text":"InfiniteTimeSafety{VT <: Vector{Union{<:Integer, <:Tuple, <:CartesianIndex}}, R <: Real}\n\nInfiniteTimeSafety is similar to FiniteTimeSafety except that the time horizon is infinite, i.e., K = infty. In practice it means, performing the value iteration until the value function has converged, defined by some threshold convergence_eps. The convergence threshold is that the largest value of the most recent Bellman residual is less than convergence_eps.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{InfiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::InfiniteTimeSafety)\n\nReturn the set of states with which to compute safety for a infinite time safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeSafety)\n\nReturn the convergence threshold of an infinite time safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeReward","page":"Specifications","title":"IntervalMDP.FiniteTimeReward","text":"FiniteTimeReward{R <: Real, AR <: AbstractArray{R}, T <: Integer}\n\nFiniteTimeReward is a property of rewards r  S to mathbbR assigned to each state at each iteration and a discount factor nu. The time horizon K is finite, so the discount factor can be greater than or equal to one. The property is\n\n    mathbbE^pieta_mathrmreward(r nu K) = mathbbE^pietaleftsum_k=0^K nu^k r(omegak) right\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reward-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.reward","text":"reward(prop::FiniteTimeReward)\n\nReturn the reward vector of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.discount-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.discount","text":"discount(prop::FiniteTimeReward)\n\nReturn the discount factor of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReward)\n\nReturn the time horizon of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeReward","page":"Specifications","title":"IntervalMDP.InfiniteTimeReward","text":"InfiniteTimeReward{R <: Real, AR <: AbstractArray{R}}\n\nInfiniteTimeReward is a property of rewards assigned to each state at each iteration and a discount factor for guaranteed convergence. The time horizon is infinite, i.e. K = infty.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reward-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.reward","text":"reward(prop::FiniteTimeReward)\n\nReturn the reward vector of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.discount-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.discount","text":"discount(prop::FiniteTimeReward)\n\nReturn the discount factor of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReward)\n\nReturn the convergence threshold of an infinite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.ExpectedExitTime","page":"Specifications","title":"IntervalMDP.ExpectedExitTime","text":"ExpectedExitTime{VT <: Vector{Union{<:Integer, <:Tuple, <:CartesianIndex}}, R <: Real}\n\nExpectedExitTime is a property of hitting time with respect to an unsafe set. An equivalent characterization is that of the expected number of steps in the safe set until reaching the unsafe set. The time horizon is infinite, i.e., K = infty, thus the package performs value iteration until the value function has converged. The convergence threshold is that the largest value of the most recent Bellman residual is less than convergence_eps. Given an unsafe set O, the property is defined as\n\n    mathbbE^pieta_mathrmexit(O) = mathbbE^pietaleftk  omegak in O  forall k in 0 ldots k - 1  omegak notin O right\n\nwhere omega = s_0 s_1 ldots s_k is a trace of the system.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{ExpectedExitTime}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::ExpectedExitTime)\n\nReturn the set of unsafe states that we compute the expected hitting time with respect to.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{ExpectedExitTime}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::ExpectedExitTime)\n\nReturn the convergence threshold of an expected exit time.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeDFAReachability","page":"Specifications","title":"IntervalMDP.FiniteTimeDFAReachability","text":"FiniteTimeDFAReachability{VT <: Vector{<:Integer}, T <: Integer}\n\nFinite time reachability specified by a set of target/terminal states and a time horizon.  That is, denote a trace by z_1 z_2 z_3 cdots with z_k = (s_k q_k) then if T is the set of target states and H is the time horizon, the property is \n\n    mathbbP(exists k = 0 ldots H q_k in T)\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{FiniteTimeDFAReachability}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::FiniteTimeDFAReachability)\n\nReturn the set of DFA states with respect to which to compute reachbility for a finite time DFA reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeDFAReachability}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeDFAReachability)\n\nReturn the time horizon of a finite time DFA reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeDFAReachability","page":"Specifications","title":"IntervalMDP.InfiniteTimeDFAReachability","text":"InfiniteTimeDFAReachability{VT <: Vector{<:Integer}, R <: Real}\n\nInfiniteTimeDFAReachability is similar to FiniteTimeDFAReachability except that the time horizon is infinite, i.e., H = infty. In practice it means, performing the value iteration until the value function has converged, defined by some threshold convergence_eps. The convergence threshold is that the largest value of the most recent Bellman residual is less than convergence_eps.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{InfiniteTimeDFAReachability}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::InfiniteTimeDFAReachability)\n\nReturn the set of DFA states with respect to which to compute reachbility for a infinite time DFA reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeDFAReachability}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeDFAReachability)\n\nReturn the convergence threshold of an infinite time DFA reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeDFASafety","page":"Specifications","title":"IntervalMDP.FiniteTimeDFASafety","text":"FiniteTimeDFASafety{VT <: Vector{<:Integer}, T <: Integer}\n\nFinite time Safety specified by a set of target/terminal states and a time horizon.  That is, denote a trace by z_1 z_2 z_3 cdots with z_k = (s_k q_k) then if T is the set of target states and H is the time horizon, the property is \n\n    mathbbP(exists k = 0 ldots H q_k in T)\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{FiniteTimeDFASafety}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::FiniteTimeDFASafety)\n\nReturn the set of DFA states with respect to which to compute safety for a finite time DFA safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeDFASafety}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeDFASafety)\n\nReturn the time horizon of a finite time DFA safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeDFASafety","page":"Specifications","title":"IntervalMDP.InfiniteTimeDFASafety","text":"InfiniteTimeDFASafety{VT <: Vector{<:Integer}, R <: Real}\n\nInfiniteTimeDFASafety is similar to FiniteTimeDFASafety except that the time horizon is infinite, i.e., H = infty. In practice it means, performing the value iteration until the value function has converged, defined by some threshold convergence_eps. The convergence threshold is that the largest value of the most recent Bellman residual is less than convergence_eps.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{InfiniteTimeDFASafety}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::InfiniteTimeDFASafety)\n\nReturn the set of DFA states with respect to which to compute safety for a infinite time DFA safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeDFASafety}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeDFASafety)\n\nReturn the convergence threshold of an infinite time DFA safety property.\n\n\n\n\n\n","category":"method"},{"location":"models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"models/#Mathematical-Notation","page":"Models","title":"Mathematical Notation","text":"We denote the natural numbers by mathbbN and mathbbN_0 = mathbbN cup 0. A probability distribution gamma over a finite set S is a function gamma  S to 0 1 satisfying sum_s in S gamma(s) = 1. The support of the distribution mathopsupp(gamma) is defined as mathopsupp(gamma) =  s in S  gamma(s)  0. We denote by mathcalD(S) the set of all probability distributions over S. For underlinegamma overlinegamma  S to 0 1 such that underlinegamma(s) leq overlinegamma(s) for each s in S and sum_s in S underlinegamma(s) leq 1 leq sum_s in S overlinegamma(s), an interval ambiguity set Gamma subset mathcalD(S) is the set of distributions such that \n\n    Gamma =  gamma in mathcalD(S)  underlinegamma(s) leq gamma(s) leq overlinegamma(s) text for each  s in S \n\nunderlinegamma overlinegamma are referred to as the interval bounds of the interval ambiguity set. For n finite sets S_1 ldots S_n we denote by S_1 times cdots times S_n their Cartesian product. Given S = S_1 times cdots times S_n and n ambiguity sets Gamma_i in mathcalD(S_i), i = 1 ldots n, the product ambiguity set Gamma subseteq mathcalD(S) is defined as: \n\n    Gamma = left gamma in mathcalD(S)  gamma(s) = prod_i=1^n gamma^i(s^i)  gamma^i in Gamma_i right\n\nwhere s = (s_1 ldots s_n) in S. We will denote the product ambiguity set as Gamma = bigotimes_i=1^n Gamma_i. Each Gamma_i is called a marginal or component ambiguity set. A transition is a triplet (s a t) in S times A times S where s is the source state, a is the action, and t is the target state.","category":"section"},{"location":"models/#Factored-RMDPs","page":"Models","title":"Factored RMDPs","text":"Factored Robust Markov Decision Processes (fRMDPs) [1, 2] are an extension of Robust Markov Decision Processes (RMDPs) [3–5] that incorporate a factored representation of the state and action spaces, i.e. with state and action variables. This allows for a more compact representation of the transition model and flexibility in modeling complex systems. First, we define here fRMDPs, and then in the subsequent sections, we define various special subclasses of fRMDPs, including how they relate to each other and to fRMDPs.\n\nFormally, a fRMDP M is a tuple M = (S S_0 A mathcalG Gamma), where\n\nS = S_1 times cdots times S_n is a finite set of joint states with S_i being a finite set of states for the i-th state variable,\nS_0 subseteq S is a set of initial states,\nA = A_1 times cdots times A_m is a finite set of joint actions with A_j being a finite set of actions for the j-th action variable,\nmathcalG = (mathcalV mathcalE) is a directed bipartite graph with nodes mathcalV = mathcalV_ind cup mathcalV_cond = S_1 ldots S_n A_1 ldots A_m cup S_1 ldots S_n representing the state and action variables and their next-state counterparts, and edges mathcalE subseteq mathcalV_ind times mathcalV_cond representing dependencies of S_i on S_j and A_k,\nGamma = Gamma_s a_s in S a in A is a set of ambiguity sets for source-action pair (s a), where each Gamma_s a = bigotimes_i=1^n Gamma^i_textPa_mathcalG(S_i) cap (s a) is a product of ambiguity sets Gamma^i_textPa_mathcalG(S_i) cap (s a) along each marginal i conditional on the values in (s a) of the parent variables textPa_mathcalG(S_i) of S_i in mathcalG, i.e.\n\n    Gamma_s a = left gamma in mathcalD(S)  gamma(t) = prod_i=1^n gamma^i(t_i  s_textPa_mathcalG_S(S_i) a_textPa_mathcalG_A(S_i))  gamma^i(cdot  s_textPa_mathcalG_S(S_i) a_textPa_mathcalG_A(S_i)) in Gamma^i_textPa_mathcalG(S_i) right\n\nFor a given source-action pair (s a) in S times A, any distribution gamma_s a in Gamma_s a is called a feasible distribution, and feasible transitions are triplets (s a t) in S times A times S where t in mathopsupp(gamma_s a) for any feasible distribution gamma_s a in Gamma_s a. A path of an fRMDP is a sequence of states and actions omega = s0 a0 s1 a1 dots where sk in S and ak in A for all k in mathbbN_0, and (sk ak sk + 1) is a feasible transition  for all k in mathbbN_0. We denote by omegak = sk the state of the path at time k in mathbbN_0 and by Omega and Omega_fin the set of all infinite and finite paths, respectively.\n\nA strategy or policy for an fRMDP is a function pi  Omega_fin to A that assigns an action, given a (finite) path called the history. Time-dependent Markov strategies are functions from state and time step to an action, i.e. pi  S times mathbbN_0 to A. This can equivalently be described as a sequence of functions indexed by time mathbfpi = (pi0 pi1 ldots). If pi does not depend on time and solely depends on the current state, it is called a stationary strategy. Similar to a strategy, an adversary eta is a function that assigns a feasible distribution to a given state. The focus of this package is on dynamic uncertainties where the choice of the adversary is resolved at every time step, called dynamic uncertainty, and where the adversary has access to both the current state and action, called (s a)-rectangularity. We refer to [5] for further details on the distinction between static and dynamic uncertainties, types of rectangularity, and their implications. Given a strategy and an adversary, an fRMDP collapses to a finite (factored) Markov chain.\n\nBelow is an example of how to construct an fRMDP with 2 state variables (2 and 3 values respectively) and 2 action variables (1 and 2 values respectively), where each marginal ambiguity set is an interval ambiguity set. The first marginal depends on both state variables and the first action variable, while the second marginal only depends on the second state variable and the second action variable.\n\nusing IntervalMDP # hide\n\nstate_vars = (2, 3)\naction_vars = (1, 2)\n\nstate_indices = (1, 2)\naction_indices = (1,)\nstate_dims = (2, 3)\naction_dims = (1,)\nmarginal1 = Marginal(IntervalAmbiguitySets(;\n    # 6 ambiguity sets = 2 * 3 source states, 1 action\n    # Column layout: (a¹₁, s¹₁, s²₁), (a¹₁, s¹₂, s²₁), (a¹₁, s¹₁, s²₂), (a¹₁, s¹₂, s²₂), (a¹₁, s¹₁, s²₃), (a¹₁, s¹₂, s²₃)\n    # Equivalent to CartesianIndices(actions_dims..., state_dims...), i.e. actions first, then states in lexicographic order\n    lower = [\n        1/15  7/30  1/15  13/30  4/15  1/6\n        2/5   7/30  1/30  11/30  2/15  1/10\n    ],\n    upper = [\n        17/30  7/10   2/3   4/5  7/10  2/3\n        9/10   13/15  9/10  5/6  4/5   14/15\n    ]\n), state_indices, action_indices, state_dims, action_dims)\n\nstate_indices = (2,)\naction_indices = (2,)\nstate_dims = (3,)\naction_dims = (2,)\nmarginal2 = Marginal(IntervalAmbiguitySets(;\n    # 6 ambiguity sets = 3 source states, 2 actions\n    # Column layout: (a²₁, s²₁), (a²₂, s²₁), (a²₁, s²₂), (a²₂, s²₂), (a²₁, s²₃), (a²₂, s²₃)\n    # Equivalent to CartesianIndices(actions_dims..., state_dims...), i.e. actions first, then states in lexicographic order\n    lower = [\n        1/30  1/3   1/6   1/15  2/5   2/15\n        4/15  1/4   1/6   1/30  2/15  1/30\n        2/15  7/30  1/10  7/30  7/15  1/5\n    ],\n    upper = [\n        2/3    7/15  4/5    11/30  19/30  1/2\n        23/30  4/5   23/30  3/5    7/10   8/15\n        7/15   4/5   23/30  7/10   7/15   23/30\n    ]\n), state_indices, action_indices, state_dims, action_dims)\n\ninitial_states = [(1, 1)]  # Initial states are optional\nmdp = FactoredRobustMarkovDecisionProcess(state_vars, action_vars, (marginal1, marginal2), initial_states)\n\nwarn: Warn\nNotice that source-action pairs are on the columns of the matrices to defined the interval bounds. This is counter to most literature on transition matrices where transitions are from row to column. The choice of layout is to ensure that the memory access pattern is cache-friendly, as each column is stored contiguously in memory (column-major) and the Bellman updates iterate outer-most over source-action pairs. However, it also has a fundamental mathematical justification: the transition matrix can be viewed as a linear operator and the matrix form of a linear operator is defined such that the columns correspond to the input dimensions, i.e. from column to row. Furthermore, actions for the same source state are stored contiguously, which is also important for cache efficiency.\n\nA general and useful subclass of fRMDPs is when each marginal ambiguity set is an interval ambiguity set. This subclass is called factored IMDPs (fIMDPs) and is described in more detail below.","category":"section"},{"location":"models/#IMCs","page":"Models","title":"IMCs","text":"Interval Markov Chains (IMCs) [6] are a subclass of fRMDPs and a generalization of Markov Chains (MCs), where the transition probabilities are not known exactly, but they are constrained to be in some probability interval. Formally, an IMC M is a tuple M = (S S_0 Gamma), where\n\nS is a finite set of states,\nS_0 subseteq S is a set of initial states,\nGamma = Gamma_s_s in S is a set of ambiguity sets for source state s, where each Gamma_s is an interval ambiguity set over S.\n\nAn IMC is equivalent to an fRMDP where there is only one state variable, no action variables, and the ambiguity sets are interval ambiguity sets. The dependency graph is just two nodes S and S with a single edge from the former to the latter. Paths and adversaries are defined similarly to fRMDPs.\n\nExample:\n\nusing IntervalMDP # hide\n\nprob = IntervalAmbiguitySets(;\n    lower = [\n        0     1/2   0\n        1/10  3/10  0\n        1/5   1/10  1\n    ],\n    upper = [\n        1/2   7/10  0\n        3/5   1/2   0\n        7/10  3/10  1\n    ],\n)\n\ninitial_states = [1]  # Initial states are optional\nmc = IntervalMarkovChain(prob, initial_states)","category":"section"},{"location":"models/#IMDPs","page":"Models","title":"IMDPs","text":"Interval Markov Decision Processes (IMDPs) [7, 8], also called bounded-parameter MDPs, are a subclass of fRMDPs and a generalization of MDPs, where the transition probabilities, given source state and action, are not known exactly, but they are constrained to be in some probability interval. IMDPs generalized IMCs by adding actions. Formally, an IMDP M is a tuple M = (S S_0 A Gamma), where\n\nS is a finite set of states,\nS_0 subseteq S is a set of initial states,\nA is a finite set of actions,\n`Gamma = Gamma_s a_s in S a in A is a set of ambiguity sets for source-action pair (s a), where each Gamma_s a is an interval ambiguity set over S.\n\nAn IMDP is equivalent to an fRMDP where there is only one state variable, one action variable, and the ambiguity sets are interval ambiguity sets. The dependency graph is three nodes S, A, and S with two edges S rightarrow S and A rightarrow S. Paths and adversaries are defined similarly to fRMDPs.\n\nExample:\n\nusing IntervalMDP # hide\n\nprob1 = IntervalAmbiguitySets(;\n    lower = [\n        0    1/2\n        1/10 3/10\n        1/5  1/10\n    ],\n    upper = [\n        1/2  7/10\n        3/5  1/2\n        7/10 3/10\n    ],\n)\n\nprob2 = IntervalAmbiguitySets(;\n    lower = [\n        1/10 1/5\n        1/5  3/10\n        3/10 2/5\n    ],\n    upper = [\n        3/5 3/5\n        1/2 1/2\n        2/5 2/5\n    ],\n)\n\nprob3 = IntervalAmbiguitySets(;\n    lower = Float64[\n        0 0\n        0 0\n        1 1\n    ],\n    upper = Float64[\n        0 0\n        0 0\n        1 1\n    ]\n)\n\ninitial_states = [1]\nmdp = IntervalMarkovDecisionProcess([prob1, prob2, prob3], initial_states)\n\n# alternatively\nprob = IntervalAmbiguitySets(;\n    lower = [\n        0    1/2  1/10 1/5  0 0\n        1/10 3/10 1/5  3/10 0 0\n        1/5  1/10 3/10 2/5  1 1\n    ],\n    upper = [\n        1/2  7/10 3/5 2/5 0 0\n        3/5  1/2  1/2 2/5 0 0\n        7/10 3/10 2/5 2/5 1 1\n    ],\n)\n\nnum_actions = 2\nmdp = IntervalMarkovDecisionProcess(prob, num_actions, initial_states)\n\nIt is possible to skip defining actions when the transition is a guaranteed self-loop and is the last states in the ambiguity set.  This is useful for defining target states in reachability problems. The example below has 3 states (as shown by the 3 rows) and 2 actions (explictly defined by num_actions = 2). The last state is a target state with a guaranteed self-loop, i.e., the transition probabilities are P(3  3 a) = 1 for both actions a in 1 2.\n\nusing IntervalMDP # hide\n\nprob = IntervalAmbiguitySets(;\n    lower = [\n        0    1/2  1/10 1/5\n        1/10 3/10 1/5  3/10\n        1/5  1/10 3/10 2/5 \n    ],\n    upper = [\n        1/2  7/10 3/5 2/5\n        3/5  1/2  1/2 2/5\n        7/10 3/10 2/5 2/5\n    ],\n)\n\nnum_actions = 2\nmdp = IntervalMarkovDecisionProcess(prob, num_actions)","category":"section"},{"location":"models/#odIMDPs","page":"Models","title":"odIMDPs","text":"Orthogonally-decoupled IMDPs (odIMDPs) [9] are a subclass of fRMDPs designed to be more memory-efficient than IMDPs. The states are structured into an orthogonal, or grid-based, decomposition and the transition probability ambiguity sets, for each source-action pair, as a product of interval ambiguity sets along each marginal. \n\nFormally, an odIMDP M with n marginals is a tuple M = (S S_0 A Gamma), where\n\nS = S_1 times cdots times S_n is a finite set of joint states with S_i being a finite set of states for the i-th marginal,\nS_0 subseteq S is a set of initial states,\nA is a finite set of actions,\nGamma = Gamma_s a_s in S a in A is a set of ambiguity sets for source-action pair (s a), where each Gamma_s a = bigotimes_i=1^n Gamma^i_s a with Gamma^i_s a is an interval ambiguity set over the i-th marginal, i.e. over S_i.\n\nAn odIMDP is equivalent to an fRMDP where the dependency graph is mathcalG = (mathcalV mathcalE) with mathcalV = S_1 ldots S_n A cup S_1 ldots S_n and mathcalE = (S_i S_j)  i j = 1 ldots n cup (A_i S_j)  j = 1 ldots m i = 1 ldots n. In other words, each next-state variable S_i depends on all state and action variables and the dependency graph is a complete bipartite graph. Paths, strategies, and adversaries are defined similarly to fRMDPs.","category":"section"},{"location":"models/#fIMDPs","page":"Models","title":"fIMDPs","text":"Factored IMDPs (fIMDPs) are a subclass of fRMDPs where each marginal ambiguity set is an interval ambiguity set, but where the dependency graph can be arbitrary.  Formally, an fIMDP M with n marginals is a tuple M = (S S_0 A mathcalG Gamma), where\n\nS = S_1 times cdots times S_n is a finite set of joint states with S_i being a finite set of states for the i-th marginal,\nS_0 subseteq S is a set of initial states,\nA is a finite set of actions,\nmathcalG = (mathcalV mathcalE) is a directed bipartite graph with nodes mathcalV = mathcalV_ind cup mathcalV_cond = S_1 ldots S_n A_1 ldots A_m cup S_1 ldots S_n representing the state and action variables and their next-state counterparts, and edges mathcalE subseteq mathcalV_ind times mathcalV_cond representing dependencies of S_i on S_j and A_k,\nGamma = Gamma_s a_s in S a in A is a set of ambiguity sets for source-action pair (s a), where each Gamma_s a = bigotimes_i=1^n Gamma^i_textPa_mathcalG(S_i) cap (s a) with Gamma^i_textPa_mathcalG(S_i) cap (s a) is an interval ambiguity set over the i-th marginal, i.e. over S_i, conditional on the values in (s a) of the parent variables textPa_mathcalG(S_i) of S_i in mathcalG.\n\nThe example in Factored RMDPs is also an example of an fIMDP.","category":"section"},{"location":"models/#References","page":"Models","title":"References","text":"Y. Schnitzer, A. Abate and D. Parker. Efficient Solution and Learning of Robust Factored MDPs, arXiv preprint arXiv:2508.00707 (2025), arXiv:2508.00707.\n\n\n\nK. V. Delgado, S. Sanner and L. N. De Barros. Efficient solutions to factored MDPs with imprecise transition probabilities. Artificial Intelligence 175, 1498–1527 (2011).\n\n\n\nA. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain transition matrices. Operations Research 53, 780–798 (2005).\n\n\n\nW. Wiesemann, D. Kuhn and B. Rustem. Robust Markov decision processes. Mathematics of Operations Research 38, 153–183 (2013).\n\n\n\nM. Suilen, T. Badings, E. M. Bovy, D. Parker and N. Jansen. Robust markov decision processes: A place where AI and formal methods meet. In: Principles of Verification: Cycling the Probabilistic Landscape: Essays Dedicated to Joost-Pieter Katoen on the Occasion of His 60th Birthday, Part III (Springer, 2024); pp. 126–154, arXiv:2411.11451.\n\n\n\nB. Delahaye, K. G. Larsen, A. Legay, M. L. Pedersen and A. Wąsowski. Decision problems for interval Markov chains. In: International Conference on Language and Automata Theory and Applications (Springer, 2011); pp. 274–285.\n\n\n\nR. Givan, S. Leach and T. Dean. Bounded-parameter Markov decision processes. Artificial Intelligence 122, 71–109 (2000).\n\n\n\nM. Lahijanian, S. B. Andersson and C. Belta. Formal verification and synthesis for discrete-time stochastic systems. IEEE Transactions on Automatic Control 60, 2031–2045 (2015).\n\n\n\nF. B. Mathiesen, S. Haesaert and L. Laurenti. Scalable control synthesis for stochastic systems via structural IMDP abstractions. In: Proceedings of the 28th ACM International Conference on Hybrid Systems: Computation and Control (2025); pp. 1–12, arXiv:2411.11803.\n\n\n\n","category":"section"},{"location":"developer/#Developer-documentation","page":"Developer docs","title":"Developer documentation","text":"","category":"section"},{"location":"developer/#Dense-matrix-vs-sparse-matrix-vs-BDD/ADD","page":"Developer docs","title":"Dense matrix vs sparse matrix vs BDD/ADD","text":"todo: Todo\nDescribe the details and choice","category":"section"},{"location":"developer/#Bellman-algorithms","page":"Developer docs","title":"Bellman algorithms","text":"","category":"section"},{"location":"developer/#dev-docs-omax","page":"Developer docs","title":"O-maximization","text":"To optimize the procedure, we abstract the O-maximization algorithm into the sorting phase and the O-maximization phase: \n\nfunction min_value(V, system, source, action)\n    # Sort values of `V` in ascending order\n    order = sortstates(V)\n    v = o_maximize(system, source, action, order)\n    return v\nend\n\nNotice the the order is shared for all source-action pairs, and thus, we can pre-compute it once per Bellman update. We however only do so for dense transition ambiguity sets, as in the sparse case, it is often faster to sort repeatedly, but only for the support. I.e.,\n\nfunction sortstates(V, system, source, action)  # I.e. sort per source-action pair\n    supp = support(system, source, action)\n    order = sortperm(@view(V[supp])) # Sort only for the support\n    return supp[order]  # Return sorted indices in original indexing\nend","category":"section"},{"location":"developer/#GPU-acceleration","page":"Developer docs","title":"GPU acceleration","text":"The sorting and O-maximization phases can be parallelized on the GPU to leverage the massive parallelism. The following assumes that the reader is familiar with the CUDA programming model; see CUDA Programming Model for a brief introduction. The specific execution plan depends on the storage type and size of model; please refer to the source code for specifics.","category":"section"},{"location":"developer/#Sorting","page":"Developer docs","title":"Sorting","text":"Sorting in parallel on the GPU is a well-studied problem, and there are many algorithms for doing so. We choose to use bitonic sorting, which is a sorting network that is easily parallelized and implementable on a GPU. The idea is to merge bitonic subsets, i.e. sets with first increasing then decreasing subsets of equal size, of increasingly larger sizes and perform minor rounds of swaps to maintain the bitonic property. The figure below shows 3 major rounds to sort a set of 8 elements (each line represents an element, each arrow is a comparison pointing towards the larger element). The latency[1] of the sorting network is O((lg n)^2), and thus it scales well to larger number of elements. See Wikipedia for more details.\n\n(Image: )","category":"section"},{"location":"developer/#O-maximization-phase","page":"Developer docs","title":"O-maximization phase","text":"In order to parallelize the O-maximization phase, observe that O-maximization implicity implements a cumulative sum according to the ordering over gaps and this is the only dependency between the states. Hence, if we can parallelize this cumulative sum, then we can parallelize the O-maximization phase. Luckily, there is a well-studied algorithm for computing the cumulative sum in parallel: tree reduction for prefix scan. The idea is best explained with figure below.\n\n(Image: )\n\nHere, we recursively compute the cumulative sum of larger and larger subsets of the array. The latency is O(lg n), and thus very efficient. See Wikipedia for more details. Putting it all together, we get the following (pseudo-code) algorithm for O-maximization:\n\nfunction o_maximize(system, source, action, order)\n    p = lower_bounds(system, source, action)\n    rem = 1 - sum(p)\n    gap = upper_bounds(system, source, action) - p\n\n    # Ordered cumulative sum of gaps via tree reduction\n    cumgap = cumulative_sum(gap[order])\n\n    @parallelize for (i, o) in enumerate(order)\n        rem_state = max(rem - cumgap[i] + gap[o], 0)\n        if gap[o] < rem_state\n            p[o] += gap[o]\n        else\n            p[o] += rem_state\n            break\n        end\n    end\n\n    return p\nend\n\nWhen implementing the algorithm above in CUDA, it is possible to use warp shuffles to very efficiently perform tree reductions of up to 32 elements. For larger sets, shared memory to store the intermediate results, which is much faster than global memory. See CUDA Programming Model for more details on why these choices are important.","category":"section"},{"location":"developer/#dev-docs-vertex-enumeration","page":"Developer docs","title":"Vertex enumeration","text":"First, we concern ourselves with enumerating the vertices of a single marginal. The key observation for an efficient algorithm is that, while each vertex corresponds to a unique ordering of the states, many orderings yield the same vertex. Thus, we need an algorithm that generates each vertex exactly once without generating all orderings explicitly. To this end, we rely on a backtracking algorithm where state values are added to a list of a \"maximizing\" state values, and backtrack once a vertex is found, i.e. sum(p) == 1 and the remaining state values are assigned a lower bound.\n\nFor the product of marginals, we simply apply Iteratorsproduct to get an iterator over all combinations of vertices.","category":"section"},{"location":"developer/#dev-docs-mccormick","page":"Developer docs","title":"Recursive McCormick envelopes","text":"The recursive McCormick envelopes for polytoptic fRMDPs are described in [1], with the addition that we add the marginal constraints to the linear program and the constraint that each relaxation q in the recursion is a valid probability distribution, i.e. sum(q) == 1. \n\nAnother consideration is whether to recursively relax as a sequence of marginals or as a binary tree. In [1], the recursive relaxation is done as a sequence. However, the tree structure requires significantly fewer auxiliary variables and thus both memory and time. A formal argument of the resulting minimum value between the two relaxation structures is missing, but empirically, they yield the same results.","category":"section"},{"location":"developer/#CUDA-Programming-Model","page":"Developer docs","title":"CUDA Programming Model","text":"We here give a brief introduction to the CUDA programming model to understand to algorithmic choices. For a more in-depth introduction, see the CUDA C++ Programming Guide. The CUDA framework is Single-Instruction Multiple-Thread (SIMT) parallel execution platform and Application Programming Interface. This is in contrast to Single-Instruction Multiple-Data where all data must be processed homogeneously without control flow. SIMT makes CUDA more flexible for heterogeneous processing and control flow. The smallest execution unit in CUDA is a thread, which is a sequential processing of instructions. A thread is uniquely identified by its thread index, which allows indexing into the global data for parallel processing. A group of 32 threads[2] is called a warp, which will be executed mostly synchronously on a streaming multiprocessor. If control flow makes threads in a wrap diverge, instructions may need to be decoded twice and executed in two separate cycles. Due to this synchronous behavior, data can be shared in registers between threads in a warp for maximum performance. A collection of (up to) 1024 threads is called a block, and this is the largest aggregation that can be synchronized. Furthermore, threads in a block share the appropriately named shared memory. This is memory that is stored locally on the streaming multiprocessor for fast access. Note that shared memory is unintuitively faster than local memory (not to be confused with registers) due to local memory being allocated in device memory. Finally, a collection of (up to) 65536 blocks is called the grid of a kernel, which is the set of instructions to be executed. The grid is singular as only a single ever exists per launched kernel. Hence, if more blocks are necessary to process the amount of data, then a grid-strided loop or multiple kernels are necessary. \n\n(Image: )\n\n[1]: Note that when assessing parallel algorithms, the asymptotic performance is measured by the latency, which is the delay in the number of parallel operations, before the result is available. This is in contrast to traditional algorithms, which are assessed by the total number of operations.\n\n[2]: with consecutive thread indices aligned to a multiple of 32.","category":"section"},{"location":"api/","page":"Index","title":"Index","text":"","category":"section"},{"location":"specifications/#Specifications","page":"Specifications","title":"Specifications","text":"Specifications are compromised of a property and whether to minimize or maximize either the lower bound (pessimistic) or the upper bound (optimistic) ofthe value function. The property, or goal, e.g. reachability and reach-avoid, defines both how the value function is initialized and how it is updated after every Bellman iteration. The property also defines whether the horizon is finite or infinite, which impacts the stopping criteria and the resulting strategy type. In particular, for the infinite horizon, model checking algorithm continues until a convergence threshold is met and the strategy, if performing control synthesis, is stationary, while for a finite horizon, the strategy is time varying. \n\nnote: Note\nThe adversary is never synthesized directly and is always considered time-varying and dynamic. Over the infinite horizon, similar to the strategy, a time-varying adversary at convergence coincides with a stationary and static adversary [5]. Without loss of generality below, we assume that the adversary eta and strategy pi are given.\n\nAs an example of constructing the specification, we consider here a reachability specification for an IMDP.\n\nusing IntervalMDP # hide\n\ntime_horizon = 10\nprop = FiniteTimeReachability([3, 9, 10], time_horizon)\n\nspec = Specification(prop)  # Default: Pessimistic, Maximize\n\n# Explicit satisfaction mode (pessimistic/optimistic)\nspec = Specification(prop, Pessimistic) # Default: Maximize, useful for Markov chains\nspec = Specification(prop, Optimistic)\n\n# Explicit strategy mode (minimize/maxize)\nspec = Specification(prop, Pessimistic, Maximize)\nspec = Specification(prop, Pessimistic, Minimize)  # Unusual, but available\nspec = Specification(prop, Optimistic, Maximize)  # Unusual, but available\nspec = Specification(prop, Optimistic, Minimize)","category":"section"},{"location":"specifications/#Simple-properties","page":"Specifications","title":"Simple properties","text":"In the sections below, we will enumerate the possible simple properties (meaning no task automaton required), their equivalence to some value function, and how to construct them. For complex properties and how to construct task automata see Complex properties,","category":"section"},{"location":"specifications/#Reachability","page":"Specifications","title":"Reachability","text":"Given a target set G subset S and a horizon K in mathbbN cup infty, reachability is the following objective \n\nmathbbP^pi eta_mathrmreach(G K) = mathbbP^pi eta leftomega in Omega  exists k in 0 ldots K  omegak in G right\n\nThe property is equivalent to the following value function\n\n    beginaligned\n        V^pi eta_0(s) = mathbf1_G(s)\n        V^pi eta_k(s) = mathbf1_G(s) + mathbf1_S setminus G(s) mathbbE_t sim eta(s a K - k)V^pi eta_k - 1(t)\n    endaligned\n\nsuch that mathbbP^pi eta_mathrmreach(G K) = V_K(s), where for K = infty the adversary does not depend on time.\n\nExample:\n\nusing IntervalMDP # hide\n# Finite horizon\ntime_horizon = 10\n\n# Example with a single state variable\nprop = FiniteTimeReachability([3, 9, 10], time_horizon)          # Single state variable only\nprop = FiniteTimeReachability([(3,), (9,), (10,)], time_horizon) # Format available for multiple state variables\n\n# Example with 3 state variables\nprop = FiniteTimeReachability([(4, 3, 9)], time_horizon)\n\n# Infinite horizon\nconvergence_threshold = 1e-8\nprop = InfiniteTimeReachability([3, 9, 10], convergence_threshold)\n\nIn addition to finite and infinite horizon reachability, we also define exact time reachability, which is the following property\n\nmathbbP^pi eta_mathrmexact-reach(G K) = mathbbP^pi eta leftomega in Omega  omegaK in G right\n\nwhich is equivalent with the following value function\n\n    beginaligned\n        V^pi eta_0(s) = mathbf1_G(s)\n        V^pi eta_k(s) = mathbbE_t sim eta(s a K - k)V^pi eta_k - 1(t)\n    endaligned\n\nsuch that mathbbP^pi eta_mathrmexact-reach(G K) = V_K(s) for a horizon K in mathbbN.\n\nThis can be constructed similarly\n\nusing IntervalMDP # hide\ntime_horizon = 10\n\n# Example with a single state variable\nprop = ExactTimeReachability([3, 9, 10], time_horizon)          # Single state variable only\nprop = ExactTimeReachability([(3,), (9,), (10,)], time_horizon) # Format available for multiple state variables\n\n# Example with 3 state variables\nprop = ExactTimeReachability([(4, 3, 9)], time_horizon)","category":"section"},{"location":"specifications/#Reach-avoid","page":"Specifications","title":"Reach-avoid","text":"Given a target set G subset S, an avoid set O subset S (with G cap O = emptyset), and a horizon K in mathbbN cup infty, reach-avoid is the following objective \n\nmathbbP^pi eta_mathrmreach-avoid(G O K) = mathbbP^pi eta leftomega in Omega  exists k in 0 ldots K  omegak in G  forall k in 0 ldots k   omegak notin O right\n\nThe property is equivalent to the following value function\n\n    beginaligned\n        V^pi eta_0(s) = mathbf1_G(s)\n        V^pi eta_k(s) = mathbf1_G(s) + mathbf1_S setminus (G cup O)(s) mathbbE_t sim eta(s a K - k)V^pi eta_k - 1(t)\n    endaligned\n\nsuch that mathbbP^pi eta_mathrmreach-avoid(G O K) = V_K(s), where for K = infty the adversary does not depend on time.\n\nExample:\n\nusing IntervalMDP # hide\n# Finite horizon\ntime_horizon = 10\n\n# Example with a single state variable\nreach = [3, 9]\navoid = [10]\nprop = FiniteTimeReachAvoid(reach, avoid, time_horizon) # Single state variable only\n\nreach = [(3,), (9,)]\navoid = [(10,)]\nprop = FiniteTimeReachAvoid(reach, avoid, time_horizon) # Format available for multiple state variables\n\n# Example with 3 state variables\nreach = [(4, 3, 9)]\navoid = [(1, 1, 9)]\nprop = FiniteTimeReachAvoid(reach, avoid, time_horizon)\n\n# Infinite horizon\nconvergence_threshold = 1e-8\nprop = InfiniteTimeReachAvoid(reach, avoid, convergence_threshold)\n\nWe also define exact time reach-avoid, which is the following property\n\nmathbbP^pi eta_mathrmexact-reach-avoid(G O K) = mathbbP^pi eta leftomega in Omega  omegaK in G  forall k in 0 ldots K  omegak notin O right\n\nwhich is equivalent with the following value function\n\n    beginaligned\n        V^pi eta_0(s) = mathbf1_G(s)\n        V^pi eta_k(s) = mathbf1_S setminus O(s)mathbbE_t sim eta(s a K - k)V^pi eta_k - 1(t)\n    endaligned\n\nsuch that mathbbP^pi eta_mathrmexact-reach(G K) = V_K(s) for a horizon K in mathbbN.\n\nThis can be constructed similarly\n\nusing IntervalMDP # hide\ntime_horizon = 10\n\n# Example with a single state variable\nreach = [3, 9]\navoid = [10]\nprop = ExactTimeReachAvoid(reach, avoid, time_horizon) # Single state variable only\n\nreach = [(3,), (9,)]\navoid = [(10,)]\nprop = ExactTimeReachAvoid(reach, avoid, time_horizon) # Format available for multiple state variables\n\n# Example with 3 state variables\nreach = [(4, 3, 9)]\navoid = [(1, 1, 9)]\nprop = ExactTimeReachAvoid(reach, avoid, time_horizon)","category":"section"},{"location":"specifications/#Safety","page":"Specifications","title":"Safety","text":"Given an avoid set O subset S and a horizon K in mathbbN cup infty, safety is the following objective \n\nmathbbP^pi eta_mathrmsafe(O K) = mathbbP^pi eta leftomega in Omega  forall k in 0 ldots K  omegak notin O right\n\nThis property can by duality with reachability equivalently be states as mathbbP^pi eta_mathrmsafe(O K) = 1 - mathbbP^pi eta_mathrmreach(O K). Note that if the strategy and adversary are not given, their optimization direction must be flipped in the dual objective. Alternatively, the property can be stated via the following value function\n\n    beginaligned\n        V^pi eta_0(s) = -mathbf1_O(s)\n        V^pi eta_k(s) = -mathbf1_O(s) + mathbf1_S setminus O(s) mathbbE_t sim eta(s a K - k)V^pi eta_k - 1(t)\n    endaligned\n\nsuch that mathbbP^pi eta_mathrmsafe(G K) = 1 + V_K(s), where for K = infty the adversary does not depend on time. The benefit of this formulation is that the optimization directions need not be flipped.\n\nExample:\n\nusing IntervalMDP # hide\n# Finite horizon\ntime_horizon = 10\n\n# Example with a single state variable\nprop = FiniteTimeSafety([10], time_horizon)    # Single state variable only\nprop = FiniteTimeSafety([(10,)], time_horizon) # Format available for multiple state variables\n\n# Example with 3 state variables\nprop = FiniteTimeSafety([(4, 3, 9)], time_horizon)\n\n# Infinite horizon\nconvergence_threshold = 1e-8\nprop = InfiniteTimeSafety([3, 9, 10], convergence_threshold)","category":"section"},{"location":"specifications/#Discounted-reward","page":"Specifications","title":"Discounted reward","text":"Given a (state) reward function r  S to mathbbR, a discount factor nu in (0 1), and a horizon K in mathbbN cup infty, a (discounted) reward objective is then follow\n\nmathbbE^pieta_mathrmreward(r nu K) = mathbbE^pietaleftsum_k=0^K nu^k r(omegak) right\n\nFor a finite horizon, the discount factor is allowed to be nu = 1; for the infinite horizon, nu  1 is required for convergence.\n\nThe property is equivalent to the following value function\n\n    beginaligned\n        V^pi eta_0(s) = r(s)\n        V^pi eta_k(s) = r(s) + nu mathbbE_t sim eta(s a K - k)V^pi eta_k - 1(t)\n    endaligned\n\nsuch that mathbbE^pieta_mathrmreward(r nu K) = V_K(s), where for K = infty the adversary does not depend on time.\n\nExample:\n\nusing IntervalMDP # hide\n# Finite horizon\ntime_horizon = 10\ndiscount_factor = 0.9\n\n# Example with a single state variable\nrewards = [0.0, 2.0, 1.0, -1.0]  # For 4 states\nprop = FiniteTimeReward(rewards, discount_factor, time_horizon)\n\n# Example with 2 state variables of 2 and 4 values respectively\nrewards = [\n    0.0  2.0  1.0 -1.0;\n    1.0 -1.0  0.0  2.0\n]\nprop = FiniteTimeReward(rewards, discount_factor, time_horizon)\n\n# Infinite horizon\nconvergence_threshold = 1e-8\nprop = InfiniteTimeReward(rewards, discount_factor, convergence_threshold)","category":"section"},{"location":"specifications/#Expected-exit-time","page":"Specifications","title":"Expected exit time","text":"Given a avoid set O subset S, the expected exit time of the set S \\setminus O is the following objective \n\nmathbbE^pieta_mathrmexit(O) = mathbbE^pietaleftk  omegak in O  forall k in 0 ldots k - 1  omegak notin O right\n\nThe property is equivalent to the following value function\n\n    beginaligned\n        V^pi eta_0(s) = mathbf1_S setminus 0(s)\n        V^pi eta_k(s) = mathbf1_S setminus O(s) left(1 + mathbbE_t sim eta(s a)V^pi eta_k - 1(t)right)\n    endaligned\n\nsuch that mathbbE^pieta_mathrmexit(O) = V_infty(s). The adversary does not depend on time.\n\nExample:\n\nusing IntervalMDP # hide\n\nconvergence_threshold = 1e-8\n\n# Example with a single state variable\navoid_states = [10]\nprop = ExpectedExitTime(avoid_states, convergence_threshold)    # Single state variable only\n\navoid_states = [(10,)]\nprop = ExpectedExitTime(avoid_states, convergence_threshold) # Format available for multiple state variables\n\n# Example with 3 state variables\navoid_states = [(4, 3, 9)]\nprop = ExpectedExitTime(avoid_states, convergence_threshold)","category":"section"},{"location":"specifications/#Complex-properties","page":"Specifications","title":"Complex properties","text":"For complex, temporal properties, it is necessary to use some form of automaton to express the property. In this package, we support specifications via Deterministic Finite Automata (DFA), which via a lazy product construction with an fRMDP allows for efficient implementations of the Bellman operator. DFAs is an important class of task automata as it can express properties in syntactically co-safe Linear Temporal Logic (scLTL) [10] and Linear Temporal Logic over finite traces (LTLf) [11].\n\nFormally, a DFA is a tuple mathcalA = (Q q_0 2^mathrmAP delta F) where Q is a finite set of states, q_0 in Q is the initial state, 2^mathrmAP is a finite alphabet from atomic proposition mathrmAP, delta  Q times 2^mathrmAP to Q is a transition function, and F subseteq Q is a set of accepting states. The DFA accepts a word sigma = sigma_0 sigma_1 ldots sigma_n over the alphabet 2^mathrmAP if there exists a sequence of states q_0 q_1 ldots q_n such that q_i+1 = delta(q_i sigma_i) for all 0 geq i  n and q_n in F. We write mathcalA models sigma if the word sigma is accepted by the DFA mathcalA.\n\nA DFA can be constructed like in the following example[1]:\n\nusing IntervalMDP # hide\n\natomic_props = [\"a\", \"b\"]\n\ndelta = TransitionFunction([  # Columns: states, rows: input symbols\n    1 3 3  # symbol: \"\"\n    2 1 3  # symbol: \"a\"\n    3 3 3  # symbol: \"b\"\n    1 1 1  # symbol: \"ab\"\n])\n\ninitial_state = 1\n\ndfa = DFA(delta, initial_state, atomic_props)\n\nNotice that the DFA does not include the set of accepting states. This is because the accepting states do not impact the Bellman operator and therefore are defined in DFAReachability objects, which is shown below.\n\nusing IntervalMDP # hide\n\naccepting_states = [3]  # Accepting _DFA_ states\n\ntime_horizon = 10\nprop = FiniteTimeDFAReachability(accepting_states, time_horizon)\n\nconvergence_threshold = 1e-8\nprop = InfiniteTimeDFAReachability(accepting_states, convergence_threshold)\n\nGiven an fRMDP M = (S S_0 A mathcalG Gamma) and a labeling function L  S to Sigma that maps states of the fRMDP to symbols in the alphabet of the DFA, a path omega = s_0 s_1 ldots in the fRMDP produces a word L(s_0) L(s_1) ldots that is (possibly) accepted by the DFA. The probability of producing a path in the fRMDP that is accepted by the DFA can be expressed via the product construction M otimes mathcalA = (Z Z_0 A Gamma), where\n\nZ = S times Q is the set of product states, \nZ_0 = S_0 times q_0 is the set of initial product states,\nA is the set of actions, and\nGamma = Gamma_z a_z in Z a in A is the joint ambiguity set defined as\n\nGamma_z a = gamma_z a in mathcalD(Z)  exists gamma_s a in Gamma_s a text st  gamma_z a(z) = mathbf1_q(delta(q L(t))) gamma_s a(t)\n\nwhere z = (s q) and z = (t q). Then, the probability of generating a path, of length K in mathbbN, in the fRMDP that is accepted by the DFA is formally defined as\n\nmathbbP^pi eta_mathrmdfa-reach(F K) = mathbbP^pi eta_M otimes mathcalA leftomega in Omega  omegaK in S times F right\n\nNote that this is equivalent to reachability in the product fRMDP M otimes mathcalA. Therefore, the property can equivalently be stated via the value function for reachability in the product fRMDP.\n\n    beginaligned\n        V^pi eta_0(z) = mathbf1_S times F(z)\n        V^pi eta_k(z) = mathbf1_S times F(z) + mathbf1_Z setminus (S times F)(z) mathbbE_z sim eta(z a K - k)V^pi eta_k - 1(z)\n    endaligned\n\nsuch that mathbbP^pi eta_mathrmdfa-reach(F K) = V_K(z). \n\nNote that the product is never explicitly constructed, for three reasons: (i) the result is an RMDP and not an fRMDP, thus negating the computational benefits of using fRMDPs, (ii) the transition function will be sparse even if some marginals in the original fRMDP are dense, and (iii) the Bellman operator will not be able to leverage the structure of the product construction. Instead, we lazily construct the product as a ProductProcess, and sequentially update the value function first updating wrt. the DFA transition and then wrt. the fRMDP transition like\n\n    beginaligned\n        V^pi eta_0(s q) = mathbf1_F(q)\n        W^pi eta_k(t q) = mathbf1_F(q) + mathbf1_Q setminus F(q) V^pi eta_k - 1(t delta(q L(t)))\n        V^pi eta_k(s q) = mathbbE_t sim eta(s a K - k)W^pi eta_k(t q)\n    endaligned\n\nNotice that W^pi eta_k(t q) is shared for all s in S when updating V^pi eta_k(s q). This allows for efficient, cache-friendly implementations of the Bellman operator. The kernel for product processes merely forwards, for each DFA state q in Q setminus F, the Bellman update to the underlying Bellman operator algorithm, which is chosen based on the fRMDP model type, e.g. IMDP or odIMDP, storage type, e.g. dense or sparse, and hardware, e.g. CPU or CUDA, for efficicency.\n\nExample of constructing a product process:\n\nmap = [1, 2, 3]  # \"\", \"a\", \"b\"\nlf = DeterministicLabelling(map)\n\nproduct_process = ProductProcess(mdp, dfa, lf)\n\nThe product process can then be used in a VerificationProblem or ControlSynthesisProblem together with a specification with a DFA property.\n\n[1]: The automatic construction of a DFA from scLTL or LTLf formulae is not currently supported, but planned for future releases.","category":"section"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/#Model-checking","page":"Algorithms","title":"Model checking","text":"The core algorithmic component of this package is (robust) value iteration, which is used to solve verification and control synthesis problems for fRMDPs. Value iteration is an iterative algorithm that computes the value function for a given specification by repeatedly applying the Bellman operator until convergence.\n\nTo simplify the dicussion on the algorithmic choices, we will assume that the goal is to compute the maximizing pessimistic probability of reaching a set of states G, that is, \n\nmax_pi  min_eta   mathbbP^pi eta_mathrmreach(G K)\n\nSee Models for more details on the formal definition of fRMDPs, strategies, and adversaries; in this case the maximization and minimization operators respectively. The algorithms are easily adapted to other specifications.\n\nComputing the solution to the above problem can be framed in terms of value iteration. The value function V_k is the probability of reaching G in k steps or fewer. The value function is initialized to V_0(s) = 1 if s in G and V_0(s) = 0 otherwise. The value function is then iteratively updated according to the Bellman equation\n\nbeginaligned\n    V_0(s) = mathbf1_G(s) \n    V_k(s) = mathbf1_G(s) + mathbf1_S setminus G(s) max_a in A min_gamma_sa in Gamma_sa sum_t in S V_k-1(t) gamma_sa(t)\nendaligned\n\nwhere mathbf1_G(s) = 1  if s in G and 0 otherwise is the indicator function for set G. This Bellman update is repeated until k = K, or if K = infty, until the value function converges, i.e. V_k = V_k-1 for some k. The value function is then the solution to the problem.\n\nIn a more programmatic formulation, the algorithm can be summarized as follows:\n\nfunction value_iteration(system, spec)\n    V = initialize_value_function(spec)  # E.g. V[s] = 1 if s in G else 0 for reachability\n\n    while !converged(V)  # or for k in 1:K if K is finite\n        # Compute max_{a \\in A} \\min_{γ_{s,a} \\in Γ_{s,a}} \\sum_{t \\in S} V_{k-1}(t) γ_{s,a}(t) for all states s\n        V = bellman_update(V, system) # System contains information about S, A, and Γ\n        post_process!(V, spec) # E.g. set V[s] = 1 for s in G for reachability\n    end\nend\n\nWe slightly abuse terminology and call the max/min expectation the Bellman update, even though it is not a proper Bellman operator as it does not include the indicator function for G. The min/max expectation is however shared between all specifications, and thus it is natural to separate it from the specification-dependent post-processing step.\n\nNote that exact convergence is virtually, impossible, unless using (computationally slow) exact arithmetic, to achieve in a finite number of iterations due to the finite precision of floating point numbers. Hence, we instead use a residual tolerance epsilon and stop when Bellman residual V_k - V_k-1 is less than the threshold, V_k - V_k-1_infty  epsilon. See Bellman operator algorithms for algorithms that support exact arithmetic.","category":"section"},{"location":"algorithms/#Bellman-operator-algorithms","page":"Algorithms","title":"Bellman operator algorithms","text":"As the Bellman update is the most computationally intensive part of the algorithm, it is crucial to implement it efficiently including considerations about type stability, pre-allocation and in-place operations, memory access patterns, and parallelization.\n\nType stability: the Bellman update should be type stable, i.e. the correct kernel to dispatch to should be inferable at compile time, to avoid dynamic dispatch and heap allocations in the hot loop. This can be achieved by using parametric types and avoiding abstract types in the hot loop.\nPre-allocation and in-place operations: to avoid unnecessary allocations and reducing GC pressure, the value function (pre and post Bellman update) is be pre-allocated and updated in-place, and the Bellman update relies on pre-allocated workspace objects.\nMemory access patterns: to ensure cache efficiency, the memory access pattern should be as contiguous as possible. This is achieved by storing the transition matrices/ambiguity sets in column-major order, where each column corresponds to a source-action pair.\nParallelization: to leverage multi-core CPUs and CUDA hardware, the Bellman update should be parallelized across source-states and in the case of CUDA, also across actions and target states.\n\nA challenge with designing Bellman operator algorithms for fRMDPs is that min_gamma_sa in Gamma_sa sum_t in S V_k-1(t) gamma_sa(t) is not always computable exactly, and thus, we must resort to sound approximations. For IMDPs, the minimum can be computed exactly via O-maximization. Below, we will describe different algorithms for computing the Bellman update, their trade-offs, and algorithmic choices for an efficient implementation.","category":"section"},{"location":"algorithms/#O-maximization","page":"Algorithms","title":"O-maximization","text":"In case of an IMDP, the minimum over all feasible distributions can be computed as a solution to a Linear Programming (LP) problem, namely\n\n    beginaligned\n        min_gamma_s a quad  sum_t in S V_k-1(t) cdot gamma_s a(t) \n        quad  underlinegamma_s a(t) leq gamma_s a(t) leq overlinegamma_s a(t) quad forall t in S \n        quad  sum_t in S gamma_sa(t) = 1\n    endaligned\n\nHowever, due to the particular structure of the LP problem, we can use a more efficient algorithm: O-maximization, or ordering-maximization [7, 8]. In the case of pessimistic probability, we want to assign the most possible probability mass to the destinations with the smallest value of V_k-1, while obeying that the probability distribution is feasible, i.e. within the probability bounds and that it sums to 1. This is done by sorting the values of V_k-1 and then assigning state with the smallest value its upper bound, then the second smallest, and so on until the remaining mass must be assigned to the lower bound of the remaining states for probability distribution is feasible.\n\nfunction min_value(V, system, source, action)\n    # Sort values of `V` in ascending order\n    order = sortperm(V)\n\n    # Initialize distribution to lower bounds\n    p = lower_bounds(system, source, action)\n    budget = 1 - sum(p)\n\n    # Assign upper bounds to states with smallest values\n    # until remaining mass is zero\n    for idx in order\n        gap = upper_bounds(system, source, action)[idx] - p[idx]\n        if budget <= gap\n            p[idx] += budget\n            break\n        else\n            p[idx] += gap\n            budget -= gap\n        end\n    end\n\n    v = dot(V, p)\n    return v\nend\n\nFor fIMDPs, O-maximization can be applied recursively over the marginals as a sound under-approximation of the minimum [9]. Let S = S_1 times cdots times S_n be the state space factored into n state variables, and let Gamma_sa = Gamma^1_sa times cdots times Gamma^n_sa be the transition ambiguity sets factored into n marginals. Then, we can compute a bound on the minimum as\n\n    beginaligned\n        W_sa^kn(t^1 ldots t^n) = V_k - 1(t)\n        W_sa^ki-1(t^1 ldots t^i-1) = min_gamma^i_sa in Gamma^i_sa sum_t^i in S_i W_sa^ki(\n            t^1 ldots t^i) gamma^i_sa(t^i)\n            qquad qquad text for  i = 2 ldots n \n        W_sa^k =  W_sa^k0 = min_gamma^1_sa in Gamma^1_sa sum_t^1 in S_1 W_sa^k1(t^1) gamma^1_sa(t^1)\n    endaligned\n\nThen, V_k(s) = mathbf1_G(s) + mathbf1_S setminus G(s) max_a in A W_sa^k. Note that this is strictly better than building a joint ambiguity set by multiplying the marginal interval bounds [9].\n\nThe algorithm is the default Bellman algorithm for IMDPs, but not for fIMDPs. To explicitly select (recursive) O-maximization, do the following:\n\nalg = RobustValueIteration(OMaximization())\nresult = solve(problem, alg)\nnothing # hide\n\nO-maximization supports both floating point and exact arithmetic, and it is implemented for both CPU and CUDA hardware.","category":"section"},{"location":"algorithms/#Vertex-enumeration","page":"Algorithms","title":"Vertex enumeration","text":"A way to compute the minimum exactly for fIMDPs, and in general polytopic ambiguity sets, is via vertex enumeration [1]. The idea is to enumerate the Cartesian product of all vertices of each polytope and then compute the minimum over the vertices. This is however only feasible for few state values along each marginal, as the potential number of vertices for each marginal can grow with the factorial of the number of state values, and exponentially in the number of dimensions. Hence, this algorithm is only feasible for small problems, but it is included for completeness and as a reference implementation. To use vertex enumeration, do the following:\n\nalg = RobustValueIteration(VertexEnumeration())\nresult = solve(problem, alg)\nnothing # hide\n\nThe implementation iterates vertex combinations in a lazy manner, and thus, it does not store all vertices in memory. Furthermore, efficient generation of vertices for each marginal is done via backtracking to avoid enumerating all possible orderings.\n\nVertex enumeration supports both floating point and exact arithmetic.","category":"section"},{"location":"algorithms/#Recursive-McCormick-envelopes","page":"Algorithms","title":"Recursive McCormick envelopes","text":"Another method for computing a sound under-approximation of the minimum for fIMDPs is via recursive McCormick envelopes [1]. The idea is to relace each bilinear term gamma^1_s a(t^1) cdot gamma^2_s a(t^2) in sum_t in S V_k-1() gamma^1_s a(t^1) cdot gamma^2_s a(t^2) (for a system with two marginals) with a new variable q_s a(t^1 t^2) and add linear McCormick constraints to ensure that q_s a(t^1 t^2) is an over-approximation of the bilinear term. That is,\n\n    beginaligned\n        q_s a(t^1 t^2) geq underlinegamma^1_sa(t^1) cdot gamma^2_sa(t^2) + underlinegamma^2_sa(t^2) cdot gamma^1_sa(t^1) - underlinegamma^1_sa(t^1) cdot underlinegamma^2_sa(t^2) \n        q_s a(t^1 t^2) geq overlinegamma^1_sa(t^1) cdot gamma^2_sa(t^2) + overlinegamma^2_sa(t^2) cdot gamma^1_sa(t^1) - overlinegamma^1_sa(t^1) cdot overlinegamma^2_sa(t^2) \n        q_s a(t^1 t^2) leq underlinegamma^1_sa(t^1) cdot gamma^2_sa(t^2) + overlinegamma^2_sa(t^2) cdot gamma^1_sa(t^1) - underlinegamma^1_sa(t^1) cdot overlinegamma^2_sa(t^2) \n        q_s a(t^1 t^2) leq overlinegamma^1_sa(t^1) cdot gamma^2_sa(t^2) + underlinegamma^2_sa(t^2) cdot gamma^1_sa(t^1) - overlinegamma^1_sa(t^1) cdot underlinegamma^2_sa(t^2)\n    endaligned\n\nIn addition, we add the constraint that sum_t^1 in S_1 sum_t^2 in S_2 q_s a(t^1 t^2) = 1 such that q_s a is a valid probability distribution.\n\nThis results in a Linear Programming (LP) problem that can be solved efficiently. The McCormick envelopes can be applied recursively for more than two marginals. The algorithm is more efficient than vertex enumeration and is thus the default Bellman algorithm for fIMDPs.\n\nTo use recursive McCormick envelopes, do the following:\n\n# Use default LP solver (HiGHS)\nalg = RobustValueIteration(LPMcCormickRelaxation())\n\n# Choose a different LP solver\nusing Clarabel\nalg = RobustValueIteration(LPMcCormickRelaxation(; lp_solver=Clarabel.Optimizer))\n\nresult = solve(problem, alg)\nnothing # hide\n\nSee the JuMP documentation for a list of supported LP solvers. The recursive McCormick envelopes Bellman operator algorithm supports primarily floating point, but also exact arithmetic if the chosen LP solver does.","category":"section"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"The general procedure for using this package can be described in 5 steps\n\nConstruct a model, e.g. an Interval Markov Decision Process (IMDP) or some other subclass of FactoredRobustMarkovDecisionProcess.\nChoose property (reachability/reach-avoid/safety/reward + finite/infinite horizon).\nChoose specification (optimistic/pessimistic + maximize/minimize + property).\nCombine system and specification in a VerificationProblem or ControlSynthesisProblem, depending on whether you want to verify or synthesize a controller or not.\nCall solve with the constructed problem and optionally a chosen algorithm. If no algorithm is given, a default algorithm will be chosen.\n\nFirst, we construct a system; for the purpose of this example, we will construct either an IMDP. For more information about the different models, see Models. Note that all subclasses of FactoredRobustMarkovDecisionProcess are converted to an fRMDP internally for verification and control synthesis, and the default algorithm is inferred based on the structure of the fRMDP. An fRMDP consist of state variables (each can take on a finite number of values), action variables (similar to state variables), designated initial states, and a transition model; more specifically, the product of ambiguity sets for each marginal. See Factored RMDPs for more information about transition model.\n\nAn example of how to construct IMDP is the following:\n\nusing IntervalMDP\n\n# IMDP\nprob1 = IntervalAmbiguitySets(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalAmbiguitySets(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalAmbiguitySets(;\n    lower = [\n        0.0 0.0\n        0.0 0.0\n        1.0 1.0\n    ],\n    upper = [\n        0.0 0.0\n        0.0 0.0\n        1.0 1.0\n    ],\n)\n\ntransition_probs = [prob1, prob2, prob3]\ninitial_states = [1]  # Initial states are optional\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_states)\n\n# output\n\nFactoredRobustMarkovDecisionProcess\n├─ 1 state variables with cardinality: (3,)\n├─ 1 action variables with cardinality: (2,)\n├─ Initial states: [1]\n├─ Transition marginals:\n│  └─ Marginal 1:\n│     ├─ Conditional variables: states = (1,), actions = (1,)\n│     └─ Ambiguity set type: Interval (dense, Matrix{Float64})\n└─Inferred properties\n   ├─Model type: Interval MDP\n   ├─Number of states: 3\n   ├─Number of actions: 2\n   ├─Default model checking algorithm: Robust Value Iteration\n   └─Default Bellman operator algorithm: O-Maximization\n\nNote that for an IMDP, the transition probabilities are specified as a list of transition probabilities (with each column representing an action) for each state. The constructor will concatenate the transition probabilities into a single matrix, such that the columns represent source/action pairs and the rows represent target states.\n\ntip: Tip\nIMDPs can be very memory intensive if the ambiguity sets are stored as dense matrices. To reduce memory usage, consider using Sparse matrices and/or Factored RMDPs (recommended).\n\nNext, we choose a property. Currently supported are reachability, reach-avoid, safety, reward, expected exit time and DFA-based properties. For this example, we will use a reachability property, which requires specifying a set of target states target_set. Furthermore, this package distinguishes distinguish between finite and infinite horizon properties - for finite horizon, a time horizon must be given, while for infinite horizon, a convergence threshold must be provided. \n\nIn addition to the property, we need to specify whether we want to maximize or minimize the optimistic or pessimistic value (the value being satisfaction probability, discounted reward, etc.). We call this a specification.\n\n# Reachability\ntarget_set = [3]\nprop = FiniteTimeReachability(target_set, 10)  # Time steps\nprop = InfiniteTimeReachability(target_set, 1e-6)  # Residual tolerance\n\n## Specification\nspec = Specification(prop, Pessimistic, Maximize)\nspec = Specification(prop, Pessimistic, Minimize)\nspec = Specification(prop, Optimistic, Maximize)\nspec = Specification(prop, Optimistic, Minimize)\n\n## Combine system and specification in a Problem\nverification_problem = VerificationProblem(mdp, spec) # use `VerificationProblem(mdp, spec, strategy)` to verify under a given strategy\ncontrol_problem = ControlSynthesisProblem(mdp, spec)\n\n# output\n\nControlSynthesisProblem\n├─ FactoredRobustMarkovDecisionProcess\n│  ├─ 1 state variables with cardinality: (3,)\n│  ├─ 1 action variables with cardinality: (2,)\n│  ├─ Initial states: [1]\n│  ├─ Transition marginals:\n│  │  └─ Marginal 1:\n│  │     ├─ Conditional variables: states = (1,), actions = (1,)\n│  │     └─ Ambiguity set type: Interval (dense, Matrix{Float64})\n│  └─Inferred properties\n│     ├─Model type: Interval MDP\n│     ├─Number of states: 3\n│     ├─Number of actions: 2\n│     ├─Default model checking algorithm: Robust Value Iteration\n│     └─Default Bellman operator algorithm: O-Maximization\n└─ Specification\n   ├─ Satisfaction mode: Optimistic\n   ├─ Strategy mode: Minimize\n   └─ Property: InfiniteTimeReachability\n      ├─ Convergence threshold: 1.0e-6\n      └─ Reach states: CartesianIndex{1}[CartesianIndex(3,)]\n\ntip: Tip\nFor complex properties, e.g. LTLf, it is necessary to construct a Definite Finite Automaton (DFA) and (lazily) build the product with the fRMDP. See Complex properties for more details on the product construction and DFA properties. Note that constructing the DFA from an LTLf formula is currently not supported by this package. \n\nFinally, we call solve to solve the specification. solve returns the value function for all states in addition to the number of iterations performed and the last Bellman residual, wrapped in a solution object.\n\nsol = solve(verification_problem) # or solve(problem, alg) where e.g. alg = RobustValueIteration(LPMcCormickRelaxation()) to specify the algorithm\nV, k, res = sol\n\n# or alternatively\nV, k, res = value_function(sol), num_iterations(sol), residual(sol)\n\n# For control synthesis, we also get a strategy\nsol = solve(control_problem)\nV, k, res, strategy = sol\n\nFor now, only RobustValueIteration is supported, but more algorithms are planned.\n\nnote: Note\nTo use multi-threading for parallelization, you need to either start julia with julia --threads <n|auto> where n is a positive integer or to set the environment variable JULIA_NUM_THREADS to the number of threads you want to use. For more information, see Multi-threading.","category":"section"},{"location":"usage/#Sparse-matrices","page":"Usage","title":"Sparse matrices","text":"A disadvantage of IMDPs is that the size of the transition matrices grows O(n^2 m) where n is the number of states and m is the number of actions. Quickly, this becomes infeasible to store in memory. However, IMDPs frequently have lots of sparsity we may exploit. We choose in particular to store the transition matrices in the Compressed Sparse Column (CSC) format. This is a format that is widely used in Julia and other languages, and is supported by many linear algebra operations. The format consists of three arrays: colptr, rowval and nzval. The colptr array stores the indices of the first non-zero value in each column. The rowval array stores the row indices of the non-zero values, and the nzval array stores the non-zero values. We choose this format, since source states are stored as columns (see IntervalAmbiguitySets and Marginal for more information about the structure of the transition probability matrices). Thus the non-zero values for each source state is stored in sequentially in memory, enabling efficient memory access.\n\nTo use SparseMatrixCSC, we need to load SparseArrays. Below is an example of how to construct an IntervalMarkovChain with sparse transition matrices.\n\nusing SparseArrays\n\nlower = spzeros(3, 3)\nlower[2, 1] = 0.1\nlower[3, 1] = 0.2\nlower[1, 2] = 0.5\nlower[2, 2] = 0.3\nlower[3, 2] = 0.1\nlower[3, 3] = 1.0\n\nlower\n\nupper = spzeros(3, 3)\nupper[1, 1] = 0.5\nupper[2, 1] = 0.6\nupper[3, 1] = 0.7\nupper[1, 2] = 0.7\nupper[2, 2] = 0.5\nupper[3, 2] = 0.3\nupper[3, 3] = 1.0\n\nupper\n\nprob = IntervalAmbiguitySets(; lower = lower, upper = upper)\ninitial_state = 1\nimc = IntervalMarkovChain(prob, initial_state)\n\nIf you know that the matrix can be built sequentially, you can use the SparseMatrixCSC constructor directly with colptr, rowval and nzval. This is more efficient, since setindex! of SparseMatrixCSC needs to perform a binary search to find the correct index to insert the value, and possibly expand the size of the array.","category":"section"},{"location":"usage/#CUDA","page":"Usage","title":"CUDA","text":"This package is supports GPU-accelerated value iteration via CUDA (only for IMDPs and IMCs at the moment). This includes not only trivial parallelization across states but also parallel algorithms for O-maximization within each state for better computational efficiency and coalesced memory access for more speed.\n\nTo use CUDA, you need to first install CUDA.jl. For more information about this, see Installation. Next, you need to load the package with the following command:\n\nusing CUDA\n\nLoading CUDA will automatically load an extension that defines Bellman operators when the ambiguity sets are specified as CUDA arrays. It has been separated out into an extension to reduce precompilation time for users that do not need CUDA. Note that loading CUDA on a system without a CUDA-capable GPU, will not cause any errors, but only when running. You can check if CUDA is available using CUDA.functional().\n\nTo use CUDA, you need to transfer the model to the GPU. Once on the GPU, you can use the same functions as the CPU implementation. Using Julia's multiple dispatch, the package will automatically dispatch to the appropriate implementation of the Bellman operators.\n\nSimilar to CUDA.jl, we provide a cu function that transfers the model to the GPU[1]. You can either transfer the entire model or transfer the transition matrices separately. \n\n# Transfer entire model to GPU\nprob = IntervalAmbiguitySets(;\n    lower = sparse_hcat(\n        SparseVector(3, [2, 3], [0.1, 0.2]),\n        SparseVector(3, [1, 2, 3], [0.5, 0.3, 0.1]),\n        SparseVector(3, [3], [1.0]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(3, [1, 2, 3], [0.5, 0.6, 0.7]),\n        SparseVector(3, [1, 2, 3], [0.7, 0.5, 0.3]),\n        SparseVector(3, [3], [1.0]),\n    ),\n)\n\nmc = IntervalMDP.cu(IntervalMarkovChain(prob, 1))\n\n# Transfer ambiguity sets to GPU\nprob = IntervalMDP.cu(IntervalAmbiguitySets(;\n    lower = sparse_hcat(\n        SparseVector(3, [2, 3], [0.1, 0.2]),\n        SparseVector(3, [1, 2, 3], [0.5, 0.3, 0.1]),\n        SparseVector(3, [3], [1.0]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(3, [1, 2, 3], [0.5, 0.6, 0.7]),\n        SparseVector(3, [1, 2, 3], [0.7, 0.5, 0.3]),\n        SparseVector(3, [3], [1.0]),\n    ),\n))\n\nmc = IntervalMarkovChain(prob, [1])\n\n# Transfer transition matrices separately\nprob = IntervalAmbiguitySets(;\n    lower = IntervalMDP.cu(sparse_hcat(\n        SparseVector(3, [2, 3], [0.1, 0.2]),\n        SparseVector(3, [1, 2, 3], [0.5, 0.3, 0.1]),\n        SparseVector(3, [3], [1.0]),\n    )),\n    upper = IntervalMDP.cu(sparse_hcat(\n        SparseVector(3, [1, 2, 3], [0.5, 0.6, 0.7]),\n        SparseVector(3, [1, 2, 3], [0.7, 0.5, 0.3]),\n        SparseVector(3, [3], [1.0]),\n    )),\n)\n\nmc = IntervalMarkovChain(prob, [1])\n\n[1]: The difference to CUDA.jl's cu function is that IntervalMDPs.jl's cu is opinionated to preserve value types and use Int32 indices, to reduce register pressure but maintain accuracy","category":"section"},{"location":"#IntervalMDP","page":"Home","title":"IntervalMDP","text":"IntervalMDP.jl is a Julia package for modeling and verifying properties of various subclasses of factored Robust Markov Decision Processes (fRMDPs), in particular Interval Markov Decision Processes (IMDPs) and factored IMDPs (fIMDPs) via Value Iteration.\n\nRMDPs are an extension of Markov Decision Processes (MDPs) that account for uncertainty in the transition probabilities, and the factored variant introduces state and action variables such that the transition model is a product of the transition models of the individual variables, allowing for more compact representations and efficient algorithms. This package focuses on different subclasses of fRMDPs for which value iteration can be performed efficiently including Interval Markov Chains (IMCs), IMDPs, orthogonally-decoupled IMDPs (odIMDPs), and fIMDPs. See Models for more information on these models.\n\nThe aim of this package is to provide a user-friendly interface to solve verification and control synthesis problems for fRMDPs with great efficiency, which includes methods for accelerating the computation using CUDA hardware, pre-allocation, and other optimization techniques. See Algorithms for choices of the algorithmic implementation of the Bellman operator; the package aims to provide a sensible default choice of algorithms, but also allows the user to customize the algorithms to their needs.\n\ninfo: Info\nFor some subclasses of fRMDPs, the Bellman operator cannot be computed exactly, and thus, the provided Bellman operators are sound approximations. See Algorithms for more information.\n\nThe verification and control synthesis problems supported by this package include minimizing/maximizing pessimistic/optimistic specifications over properties such as reachability, reach-avoid, safety, (discounted) reward, and expected hitting times, and over finite and infinite horizons. For more complex properties, the package supports Deterministic Finite Automata (DFA), with lazy product construction and efficient, cache-friendly algorithms. See Specifications for more information on the supported specifications.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Value iteration over IMCs, IMDPs, odIMDPs, and fIMDPs.\nPlenty of built-in specifications including reachability, safety, reach-avoid, discounted reward, and expected hitting times.\nSupport for complex specifications via Deterministic Finite Automata (DFA) with lazy product construction.\nMultithreaded CPU and CUDA-accelerated value iteration.\nDense and sparse matrix support.\nParametric probability types (Float64, Float32, Rational{BigInt}) for customizable precision. Note that Rational{BigInt} is not supported for CUDA acceleration.\nData loading and writing in formats by various tools (PRISM, bmdp-tool, IntervalMDP.jl).\nExtensible and modular design to allow for custom models, distributed storage and computation, novel specifications, and additional Bellman operator and model checking algorithms, and integration with other tools and libraries[1].","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"This package requires Julia v1.9 or later. Refer to the official documentation on how to install it for your system.\n\nTo install IntervalMDP.jl, use the following command inside Julia's REPL:\n\njulia> import Pkg; Pkg.add(\"IntervalMDP\")\n\nIf you want to use the CUDA extension, you also need to install CUDA.jl:\n\njulia> import Pkg; Pkg.add(\"CUDA\")\n\n[1]: State-of-the-art tools for IMDPs are all standalone programs. We choose to develop this as a a package to enable better integration with other tools and improving the extensibility.","category":"section"}]
}
