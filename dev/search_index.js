var documenterSearchIndex = {"docs":
[{"location":"reference/specifications/#Problem","page":"Specifications","title":"Problem","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"Problem\nsystem\nspecification\nstrategy\nSpecification\nsystem_property\nProperty\nsatisfaction_mode\nSatisfactionMode\nstrategy_mode\nStrategyMode","category":"page"},{"location":"reference/specifications/#IntervalMDP.Problem","page":"Specifications","title":"IntervalMDP.Problem","text":"Problem{S <: IntervalMarkovProcess, F <: Specification}\n\nA problem is a tuple of an interval Markov process and a specification.\n\nFields\n\nsystem::S: interval Markov process.\nspec::F: specification (either temporal logic or reachability-like).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.system","page":"Specifications","title":"IntervalMDP.system","text":"system(prob::Problem)\n\nReturn the system of a problem.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.specification","page":"Specifications","title":"IntervalMDP.specification","text":"specification(prob::Problem)\n\nReturn the specification of a problem.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.strategy","page":"Specifications","title":"IntervalMDP.strategy","text":"strategy(prob::Problem)\n\nReturn the strategy of a problem, if provided.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.Specification","page":"Specifications","title":"IntervalMDP.Specification","text":"Specification{F <: Property}\n\nA specfication is a property together with a satisfaction mode and a strategy mode.  The satisfaction mode is either Optimistic or Pessimistic. See SatisfactionMode for more details. The strategy  mode is either Maxmize or Minimize. See StrategyMode for more details.\n\nFields\n\nprop::F: verification property (either temporal logic or reachability-like).\nsatisfaction::SatisfactionMode: satisfaction mode (either optimistic or pessimistic). Default is pessimistic.\nstrategy::StrategyMode: strategy mode (either maximize or minimize). Default is maximize.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.system_property","page":"Specifications","title":"IntervalMDP.system_property","text":"system_property(spec::Specification)\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.Property","page":"Specifications","title":"IntervalMDP.Property","text":"Property\n\nSuper type for all system Property\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.satisfaction_mode","page":"Specifications","title":"IntervalMDP.satisfaction_mode","text":"satisfaction_mode(spec::Specification)\n\nReturn the satisfaction mode of a specification.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.SatisfactionMode","page":"Specifications","title":"IntervalMDP.SatisfactionMode","text":"SatisfactionMode\n\nWhen computing the satisfaction probability of a property over an interval Markov process, be it IMC or IMDP, the desired satisfaction probability to verify can either be Optimistic or Pessimistic. That is, upper and lower bounds on the satisfaction probability within the probability uncertainty.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.strategy_mode","page":"Specifications","title":"IntervalMDP.strategy_mode","text":"strategy_mode(spec::Specification)\n\nReturn the strategy mode of a specification.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.StrategyMode","page":"Specifications","title":"IntervalMDP.StrategyMode","text":"StrategyMode\n\nWhen computing the satisfaction probability of a property over an IMDP, the strategy can either maximize or minimize the satisfaction probability (wrt. the satisfaction mode).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#Temporal-logic","page":"Specifications","title":"Temporal logic","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"LTLFormula\nisfinitetime(spec::LTLFormula)\nLTLfFormula\nisfinitetime(spec::LTLfFormula)\ntime_horizon(spec::LTLfFormula)\nPCTLFormula","category":"page"},{"location":"reference/specifications/#IntervalMDP.LTLFormula","page":"Specifications","title":"IntervalMDP.LTLFormula","text":"LTLFormula\n\nLinear Temporal Logic (LTL) property (first-order logic + next and until operators) [1]. Let ϕ denote the formula and M denote an interval Markov process. Then compute M  ϕ.\n\n[1] Vardi, M.Y. (1996). An automata-theoretic approach to linear temporal logic. In: Moller, F., Birtwistle, G. (eds) Logics for Concurrency. Lecture Notes in Computer Science, vol 1043. Springer, Berlin, Heidelberg.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{LTLFormula}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::LTLFormula)\n\nReturn false for an LTL formula. LTL formulas are not finite time property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.LTLfFormula","page":"Specifications","title":"IntervalMDP.LTLfFormula","text":"LTLfFormula\n\nAn LTL formula over finite traces [1]. See LTLFormula for the structure of LTL formulas. Let ϕ denote the formula, M denote an interval Markov process, and H the time horizon. Then compute M  ϕ within traces of length H.\n\nFields\n\nformula::String: LTL formula\ntime_horizon::T: Time horizon of the finite traces \n\n[1] Giuseppe De Giacomo and Moshe Y. Vardi. 2013. Linear temporal logic and linear dynamic logic on finite traces. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (IJCAI '13). AAAI Press, 854–860.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{LTLfFormula}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(spec::LTLfFormula)\n\nReturn true for an LTLf formula. LTLf formulas are specifically over finite traces.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{LTLfFormula}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(spec::LTLfFormula)\n\nReturn the time horizon of an LTLf formula.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.PCTLFormula","page":"Specifications","title":"IntervalMDP.PCTLFormula","text":"PCTLFormula\n\nA Probabilistic Computation Tree Logic (PCTL) formula [1]. Let ϕ denote the formula and M denote an interval Markov process. Then compute M  ϕ.\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#Reachability","page":"Specifications","title":"Reachability","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractReachability\n\nFiniteTimeReachability\nisfinitetime(spec::FiniteTimeReachability)\nterminal_states(spec::FiniteTimeReachability)\nreach(spec::FiniteTimeReachability)\ntime_horizon(spec::FiniteTimeReachability)\n\nInfiniteTimeReachability\nisfinitetime(spec::InfiniteTimeReachability)\nterminal_states(spec::InfiniteTimeReachability)\nreach(spec::InfiniteTimeReachability)\nconvergence_eps(spec::InfiniteTimeReachability)","category":"page"},{"location":"reference/specifications/#IntervalMDP.AbstractReachability","page":"Specifications","title":"IntervalMDP.AbstractReachability","text":"AbstractReachability\n\nSuper type for all reachability-like properties.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeReachability","page":"Specifications","title":"IntervalMDP.FiniteTimeReachability","text":"FiniteTimeReachability{VT <: Vector{<:CartesianIndex}, T <: Integer}\n\nFinite time reachability specified by a set of target/terminal states and a time horizon.  That is, denote a trace by s_1 s_2 s_3 cdots, then if T is the set of target states and H is the time horizon, the property is \n\n    mathbbP(exists k = 0 ldots H s_k in T)\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeReachability)\n\nReturn true for FiniteTimeReachability.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(spec::FiniteTimeReachability)\n\nReturn the set of terminal states of a finite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::FiniteTimeReachability)\n\nReturn the set of states with which to compute reachbility for a finite time reachability prop. This is equivalent for terminal_states(prop::FiniteTimeReachability) for a regular reachability property. See FiniteTimeReachAvoid for a more complex property where the reachability and terminal states differ.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReachability)\n\nReturn the time horizon of a finite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeReachability","page":"Specifications","title":"IntervalMDP.InfiniteTimeReachability","text":"InfiniteTimeReachability{R <: Real, VT <: Vector{<:CartesianIndex}}\n\nInfiniteTimeReachability is similar to FiniteTimeReachability except that the time horizon is infinite, i.e., H = infty. In practice it means, performing the value iteration until the value function has converged, defined by some threshold convergence_eps. The convergence threshold is that the largest value of the most recent Bellman residual is less than convergence_eps.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeReachability)\n\nReturn false for InfiniteTimeReachability.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(prop::InfiniteTimeReachability)\n\nReturn the set of terminal states of an infinite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::InfiniteTimeReachability)\n\nReturn the set of states with which to compute reachbility for a infinite time reachability property. This is equivalent for terminal_states(prop::InfiniteTimeReachability) for a regular reachability property. See InfiniteTimeReachAvoid for a more complex property where the reachability and terminal states differ.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReachability)\n\nReturn the convergence threshold of an infinite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#Reach-avoid","page":"Specifications","title":"Reach-avoid","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractReachAvoid\n\nFiniteTimeReachAvoid\nisfinitetime(spec::FiniteTimeReachAvoid)\nterminal_states(spec::FiniteTimeReachAvoid)\nreach(spec::FiniteTimeReachAvoid)\navoid(spec::FiniteTimeReachAvoid)\ntime_horizon(spec::FiniteTimeReachAvoid)\n\nInfiniteTimeReachAvoid\nisfinitetime(spec::InfiniteTimeReachAvoid)\nterminal_states(spec::InfiniteTimeReachAvoid)\nreach(spec::InfiniteTimeReachAvoid)\navoid(spec::InfiniteTimeReachAvoid)\nconvergence_eps(spec::InfiniteTimeReachAvoid)","category":"page"},{"location":"reference/specifications/#IntervalMDP.AbstractReachAvoid","page":"Specifications","title":"IntervalMDP.AbstractReachAvoid","text":"AbstractReachAvoid\n\nA property of reachability that includes a set of states to avoid.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeReachAvoid","page":"Specifications","title":"IntervalMDP.FiniteTimeReachAvoid","text":"FiniteTimeReachAvoid{VT <: AbstractVector{<:CartesianIndex}}, T <: Integer}\n\nFinite time reach-avoid specified by a set of target/terminal states, a set of avoid states, and a time horizon. That is, denote a trace by s_1 s_2 s_3 cdots, then if T is the set of target states, A is the set of states to avoid, and H is the time horizon, the property is \n\n    mathbbP(exists k = 0 ldots H s_k in T text and  forall k = 0 ldots k s_k notin A)\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeReachAvoid)\n\nReturn true for FiniteTimeReachAvoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(prop::FiniteTimeReachAvoid)\n\nReturn the set of terminal states of a finite time reach-avoid property. That is, the union of the reach and avoid sets.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::FiniteTimeReachAvoid)\n\nReturn the set of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::FiniteTimeReachAvoid)\n\nReturn the set of states to avoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReachAvoid)\n\nReturn the time horizon of a finite time reach-avoid property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeReachAvoid","page":"Specifications","title":"IntervalMDP.InfiniteTimeReachAvoid","text":"InfiniteTimeReachAvoid{R <: Real, VT <: AbstractVector{<:CartesianIndex}}\n\nInfiniteTimeReachAvoid is similar to FiniteTimeReachAvoid except that the time horizon is infinite, i.e., H = infty.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeReachAvoid)\n\nReturn false for InfiniteTimeReachAvoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(prop::InfiniteTimeReachAvoid)\n\nReturn the set of terminal states of an infinite time reach-avoid property. That is, the union of the reach and avoid sets.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::InfiniteTimeReachAvoid)\n\nReturn the set of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::InfiniteTimeReachAvoid)\n\nReturn the set of states to avoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReachAvoid)\n\nReturn the convergence threshold of an infinite time reach-avoid property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#Safety","page":"Specifications","title":"Safety","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractSafety\n\nFiniteTimeSafety\nisfinitetime(spec::FiniteTimeSafety)\nterminal_states(spec::FiniteTimeSafety)\navoid(spec::FiniteTimeSafety)\ntime_horizon(spec::FiniteTimeSafety)\n\nInfiniteTimeSafety\nisfinitetime(spec::InfiniteTimeSafety)\nterminal_states(spec::InfiniteTimeSafety)\navoid(spec::InfiniteTimeSafety)\nconvergence_eps(spec::InfiniteTimeSafety)","category":"page"},{"location":"reference/specifications/#IntervalMDP.AbstractSafety","page":"Specifications","title":"IntervalMDP.AbstractSafety","text":"AbstractSafety\n\nSuper type for all safety properties.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeSafety","page":"Specifications","title":"IntervalMDP.FiniteTimeSafety","text":"FiniteTimeSafety{VT <: Vector{<:CartesianIndex}, T <: Integer}\n\nFinite time safety specified by a set of avoid states and a time horizon.  That is, denote a trace by s_1 s_2 s_3 cdots, then if A is the set of avoid states and H is the time horizon, the property is \n\n    mathbbP(forall k = 0 ldots H s_k notin A)\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{FiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeSafety)\n\nReturn true for FiniteTimeSafety.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{FiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(spec::FiniteTimeSafety)\n\nReturn the set of terminal states of a finite time safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{FiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::FiniteTimeSafety)\n\nReturn the set of states with which to compute reachbility for a finite time reachability prop. This is equivalent for terminal_states(prop::FiniteTimeSafety).\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeSafety)\n\nReturn the time horizon of a finite time safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeSafety","page":"Specifications","title":"IntervalMDP.InfiniteTimeSafety","text":"InfiniteTimeSafety{R <: Real, VT <: Vector{<:CartesianIndex}}\n\nInfiniteTimeSafety is similar to FiniteTimeSafety except that the time horizon is infinite, i.e., H = infty. In practice it means, performing the value iteration until the value function has converged, defined by some threshold convergence_eps. The convergence threshold is that the largest value of the most recent Bellman residual is less than convergence_eps.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{InfiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeSafety)\n\nReturn false for InfiniteTimeSafety.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{InfiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(prop::InfiniteTimeSafety)\n\nReturn the set of terminal states of an infinite time safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{InfiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::InfiniteTimeSafety)\n\nReturn the set of states with which to compute safety for a infinite time safety property. This is equivalent for terminal_states(prop::InfiniteTimeSafety).\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeSafety}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeSafety)\n\nReturn the convergence threshold of an infinite time safety property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#Reward-specification","page":"Specifications","title":"Reward specification","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractReward\n\nFiniteTimeReward\nisfinitetime(spec::FiniteTimeReward)\nreward(spec::FiniteTimeReward)\ndiscount(spec::FiniteTimeReward)\ntime_horizon(spec::FiniteTimeReward)\n\nInfiniteTimeReward\nisfinitetime(spec::InfiniteTimeReward)\nreward(spec::InfiniteTimeReward)\ndiscount(spec::InfiniteTimeReward)\nconvergence_eps(spec::InfiniteTimeReward)","category":"page"},{"location":"reference/specifications/#IntervalMDP.AbstractReward","page":"Specifications","title":"IntervalMDP.AbstractReward","text":"AbstractReward{R <: Real}\n\nSuper type for all reward properties.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeReward","page":"Specifications","title":"IntervalMDP.FiniteTimeReward","text":"FiniteTimeReward{R <: Real, AR <: AbstractArray{R}, T <: Integer}\n\nFiniteTimeReward is a property of rewards R  S to mathbbR assigned to each state at each iteration and a discount factor gamma. The time horizon H is finite, so the discount factor is optional and  the optimal policy will be time-varying. Given a strategy pi  S to A, the property is\n\n    V(s_0) = mathbbEleftsum_k=0^H gamma^k R(s_k) mid s_0 piright\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeReward)\n\nReturn true for FiniteTimeReward.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reward-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.reward","text":"reward(prop::FiniteTimeReward)\n\nReturn the reward vector of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.discount-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.discount","text":"discount(prop::FiniteTimeReward)\n\nReturn the discount factor of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReward)\n\nReturn the time horizon of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeReward","page":"Specifications","title":"IntervalMDP.InfiniteTimeReward","text":"InfiniteTimeReward{R <: Real, AR <: AbstractArray{R}}\n\nInfiniteTimeReward is a property of rewards assigned to each state at each iteration and a discount factor for guaranteed convergence. The time horizon is infinite, i.e. H = infty, so the optimal policy will be stationary.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeReward)\n\nReturn false for InfiniteTimeReward.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reward-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.reward","text":"reward(prop::FiniteTimeReward)\n\nReturn the reward vector of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.discount-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.discount","text":"discount(prop::FiniteTimeReward)\n\nReturn the discount factor of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReward)\n\nReturn the convergence threshold of an infinite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"Index","title":"Index","text":"","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"The general procedure for using this package can be described in 3 steps","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Construct interval Markov process (IMC or IMDP)\nChoose specification (reachability or reach-avoid)\nCall value_iteration or satisfaction_prob.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"First, we construct a system. We can either construct an interval Markov chain (IMC) or an interval Markov decision process. (IMDP) Both systems consist of states, a designated initial state, and a transition matrix. In addition, an IMDP has actions.  An example of how to construct either is the following:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using IntervalMDP\n\n# IMC\nprob = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.0\n        0.1 0.3 0.0\n        0.2 0.1 1.0\n    ],\n    upper = [\n        0.5 0.7 0.0\n        0.6 0.5 0.0\n        0.7 0.3 1.0\n    ],\n)\n\ninitial_states = [1]  # Initial states are optional\nmc = IntervalMarkovChain(prob, initial_states)\n\n# IMDP\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [prob1, prob2, prob3]\ninitial_states = [1]  # Initial states are optional\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_states)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Note that for an IMDP, the transition probabilities are specified as a list of transition probabilities (with each column representing an action) for each state. The constructor will concatenate the transition probabilities into a single matrix, such that the columns represent source/action pairs and the rows represent target states. It will in addition construct a state pointer stateptr pointing to the first column of each state and concatenate a list of actions. See IntervalMarkovDecisionProcess for more details on how to construct an IMDP.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"For IMC, the transition probability structure is significantly simpler with source states on the columns and target states on the rows of the transition matrices. Internally, they are both represented by an IntervalMarkovDecisionProcess.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Next, we choose a specification. Currently supported are reachability, reach-avoid, and reward properties. For reachability, we specify a target set of states and for reach-avoid we specify a target set of states and an avoid set of states. Furthermore, this package distinguishes distinguish between finite and infinite horizon properties - for finite horizon, a time horizon must be given while for infinite horizon, a convergence threshold must be given. In addition to the property, we need to specify whether we want to maximize or minimize the optimistic or pessimistic satisfaction probability or discounted reward.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"## Properties\n# Reachability\ntarget_set = [3]\n\nprop = FiniteTimeReachability(target_set, 10)  # Time steps\nprop = InfiniteTimeReachability(target_set, 1e-6)  # Residual tolerance\n\n# Reach-avoid\ntarget_set = [3]\navoid_set = [2]\n\nprop = FiniteTimeReachAvoid(target_set, avoid_set, 10)  # Time steps\nprop = InfiniteTimeReachAvoid(target_set, avoid_set, 1e-6)  # Residual tolerance\n\n# Reward\nreward = [1.0, 2.0, 3.0]\ndiscount = 0.9  # Has to be between 0 and 1\n\nprop = FiniteTimeReward(reward, discount, 10)  # Time steps\nprop = InfiniteTimeReward(reward, discount, 1e-6)  # Residual tolerance\n\n## Specification\nspec = Specification(prop, Pessimistic, Maximize)\nspec = Specification(prop, Pessimistic, Minimize)\nspec = Specification(prop, Optimistic, Maximize)\nspec = Specification(prop, Optimistic, Minimize)\n\n## Combine system and specification in a Problem\nproblem = Problem(imdp_or_imc, spec)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Finally, we call value_iteration to solve the specification. value_iteration returns the value function for all states in addition to the number of iterations performed and the last Bellman residual.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"V, k, residual = value_iteration(problem)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"note: Note\nTo use multi-threading for parallelization, you need to either start julia with julia --threads <n|auto> where n is a positive integer or to set the environment variable JULIA_NUM_THREADS to the number of threads you want to use. For more information, see Multi-threading.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"tip: Tip\nFor less memory usage, it is recommended to use Sparse matrices and Int32 indices. ","category":"page"},{"location":"usage/#Sparse-matrices","page":"Usage","title":"Sparse matrices","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"A disadvantage of IMDPs is that the size of the transition matrices grows O(n^2 m) where n is the number of states and m is the number of actions. Quickly, this becomes infeasible to store in memory. However, IMDPs frequently have lots of sparsity we may exploit. We choose in particular to  store the transition matrices in the compressed sparse column (CSC) format. This is a format that is widely used in Julia and other languages, and is supported by many linear algebra operations. It consists of three arrays: colptr, rowval and nzval. The colptr array stores the indices of the first non-zero value in each column. The rowval array stores the row indices of the non-zero values, and the nzval array stores the non-zero values. We choose this format, since source states are on the columns (see IntervalProbabilities for more information about the structure of the transition probability matrices). Thus the non-zero values for each source state is stored in sequentially in memory, enabling efficient memory access.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To use SparseMatrixCSC, we need to load SparseArrays. Below is an example of how to construct an IntervalMarkovChain with sparse transition matrices.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using SparseArrays\n\nlower = spzeros(3, 3)\nlower[2, 1] = 0.1\nlower[3, 1] = 0.2\nlower[1, 2] = 0.5\nlower[2, 2] = 0.3\nlower[3, 2] = 0.1\nlower[3, 3] = 1.0\n\nlower","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using SparseArrays","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"upper = spzeros(3, 3)\nupper[1, 1] = 0.5\nupper[2, 1] = 0.6\nupper[3, 1] = 0.7\nupper[1, 2] = 0.7\nupper[2, 2] = 0.5\nupper[3, 2] = 0.3\nupper[3, 3] = 1.0\n\nupper","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"prob = IntervalProbabilities(; lower = lower, upper = upper)\ninitial_state = 1\nmc = IntervalMarkovChain(prob, initial_state)\n","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"If you know that the matrix can be built sequentially, you can use the SparseMatrixCSC constructor directly with colptr, rowval and nzval. This is more efficient, since setindex! of SparseMatrixCSC needs to perform a binary search to find the correct index to insert the value, and possibly expand the size of the array.","category":"page"},{"location":"usage/#CUDA","page":"Usage","title":"CUDA","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Part of the innovation of this package is GPU-accelerated value iteration via CUDA. This includes not only trivial parallelization across states but also parallel algorithms for O-maximization within each state for better computational efficiency and coalesced memory access for more speed. ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To use CUDA, you need to first install CUDA.jl. For more information about this, see Installation. Next, you need to load the package with the following command:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using CUDA","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Loading CUDA will automatically load an extension that defines value iteration with CUDA arrays. It has been separated out into an extension to reduce precompilation time for users that do not need CUDA. Note that loading CUDA on a system without a CUDA-capable GPU, will not cause any errors, but will simply not load the extension. You can check if CUDA is correctly loaded using CUDA.is_functional().","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To use CUDA, you need to transfer the model to the GPU. Once on the GPU, you can use the same functions as the CPU implementation. Using Julia's multiple dispatch, the package will automatically call the appropriate functions for the given variable types.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Similar to CUDA.jl, we provide a cu function that transfers the model to the GPU[1]. You can either transfer the entire model or transfer the transition matrices separately. ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"# Transfer entire model to GPU\nprob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(3, [2, 3], [0.1, 0.2]),\n        SparseVector(3, [1, 2, 3], [0.5, 0.3, 0.1]),\n        SparseVector(3, [3], [1.0]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(3, [1, 2, 3], [0.5, 0.6, 0.7]),\n        SparseVector(3, [1, 2, 3], [0.7, 0.5, 0.3]),\n        SparseVector(3, [3], [1.0]),\n    ),\n)\n\nmc = IntervalMDP.cu(IntervalMarkovChain(prob, 1))\n\n# Transfer transition matrices separately\nprob = IntervalProbabilities(;\n    lower = IntervalMDP.cu(sparse_hcat(\n        SparseVector(3, [2, 3], [0.1, 0.2]),\n        SparseVector(3, [1, 2, 3], [0.5, 0.3, 0.1]),\n        SparseVector(3, [3], [1.0]),\n    )),\n    upper = IntervalMDP.cu(sparse_hcat(\n        SparseVector(3, [1, 2, 3], [0.5, 0.6, 0.7]),\n        SparseVector(3, [1, 2, 3], [0.7, 0.5, 0.3]),\n        SparseVector(3, [3], [1.0]),\n    )),\n)\n\nmc = IntervalMarkovChain(prob,[1])","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"[1]: The difference to CUDA.jl's cu function is that we allow the specification of both the value and index type, which is important due to register pressure. To reduce register pressure but maintain accuracy, we are opinoinated to Float64 values and Int32 indices.","category":"page"},{"location":"theory/#Theory","page":"Theory","title":"Theory","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Interval Markov Decision Processes (IMDPs), also called bounded-parameter MDPs [1], are a generalization of MDPs, where the transition probabilities, given source state and action, are not known exactly, but they are constrained to be in some probability interval.  Formally, an IMDP M is a tuple M = (S S_0 A overlineP underlineP), where","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"S is a finite set of states,\nS_0 subseteq S is a set of initial states,\nA is a finite set of actions,\nunderlineP S times A times S  to 01 is a function, where underlineP(sas) defines the lower bound of the transition probability from state sin S (source) to state sin S (destination) under action a in A,\noverlineP S times A times S to 01 is a function, where overlineP(sas) defines the upper bound of the transition probability from state sin S to state sin S under action a in A.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"For each state-action pair (sa) in S times A, it holds that sum_sin S underlineP(sas) leq 1 leq sum_sin S overlineP(sas) and a transition probability distribution p_saSto01 is called feasible if underlineP(sas) leq p_sa(s) leq overlineP(sas) for all destinations sin S. The set of all feasible distributions for the state-action pair (sa) is denoted by Gamma_sa.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A path of an IMDP is a sequence of states and actions omega = (s_0a_0)(s_1a_1)dots, where (s_ia_i)in S times A. We denote by omega(k) = s_k the state of the path at time k in mathbbN^0 and by Omega the set of all paths.   A strategy or policy for an IMDP is a function pi that assigns an action to a given state of an IMDP. Time-dependent strategies are functions from state and time step to an action, i.e. pi Stimes mathbbN^0 to A. If pi does not depend on time and solely depends on the current state, it is called a stationary strategy. Similar to a strategy, an adversary eta is a function that assigns a feasible distribution to a given state. Given a strategy and an adversary, an IMDP collapses to a finite Markov chain.","category":"page"},{"location":"theory/#Reachability","page":"Theory","title":"Reachability","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"In this formal framework, we can describe computing reachability given a target set G and a horizon K in mathbbN cup infty as the following objective ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"mathopoptlimits_pi^pi  mathopoptlimits_eta^eta  mathbbP_pieta leftomega in Omega  exists k in 0K  omega(k)in G  right","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"where mathopopt^pimathopopt^eta in min max and mathbbP_pieta  is the probability of the Markov chain induced by strategy pi and adversary eta. When mathopopt^eta = min, the solution is called optimal pessimistic probability (or reward), and conversely is called optimal optimistic probability (or reward) when mathopopt^eta = max. The choice of the min/max for the action and pessimistic/optimistic probability depends on the application. ","category":"page"},{"location":"theory/#Discounted-reward","page":"Theory","title":"Discounted reward","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Discounted reward is similar to reachability but instead of a target set, we have a reward function r S to mathbbR and a discount factor gamma in (0 1). The objective is then","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"mathopoptlimits_pi^pi  mathopoptlimits_eta^eta  mathbbE_pieta leftsum_k=0^K gamma^k r(omega(k)) right","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"[1] Givan, Robert, Sonia Leach, and Thomas Dean. \"Bounded-parameter Markov decision processes.\" Artificial Intelligence 122.1-2 (2000): 71-109.","category":"page"},{"location":"reference/data/#Data-formats","page":"Data Storage","title":"Data formats","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"CurrentModule = IntervalMDP.Data","category":"page"},{"location":"reference/data/#PRISM","page":"Data Storage","title":"PRISM","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"write_prism_file\nread_prism_file","category":"page"},{"location":"reference/data/#IntervalMDP.Data.write_prism_file","page":"Data Storage","title":"IntervalMDP.Data.write_prism_file","text":"write_prism_file(path_without_file_ending, problem)\n\nWrite the files required by PRISM explicit engine/format to \n\npath_without_file_ending.sta (states),\npath_without_file_ending.lab (labels),\npath_without_file_ending.tra (transitions), and\npath_without_file_ending.pctl (properties).\n\nIf the specification is a reward optimization problem, then a state rewards file .srew is also written.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_prism_file","page":"Data Storage","title":"IntervalMDP.Data.read_prism_file","text":"read_prism_file(path_without_file_ending)\n\nRead PRISM explicit file formats and pctl file, and return a Problem including system and specification.\n\nSee PRISM Explicit Model Files for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#bmdp-tool","page":"Data Storage","title":"bmdp-tool","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"read_bmdp_tool_file\nwrite_bmdp_tool_file","category":"page"},{"location":"reference/data/#IntervalMDP.Data.read_bmdp_tool_file","page":"Data Storage","title":"IntervalMDP.Data.read_bmdp_tool_file","text":"read_bmdp_tool_file(path)\n\nRead a bmdp-tool transition probability file and return an IntervalMarkovDecisionProcess and a list of terminal states. From the file format, it is not clear if the desired reachability verification if the reachability specification is finite or infinite horizon, the satisfaction_mode is pessimistic or optimistic, or if the actions should minimize or maximize the probability of reachability.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.write_bmdp_tool_file","page":"Data Storage","title":"IntervalMDP.Data.write_bmdp_tool_file","text":"write_bmdp_tool_file(path, problem::Problem)\n\nWrite a bmdp-tool transition probability file for the given an IMDP and a reachability specification. The file will not contain enough information to specify a reachability specification. The remaining parameters are rather command line arguments.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovProcess, spec::Specification)\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovProcess, prop::AbstractReachability)\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovProcess, terminal_states::Vector{T})\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovDecisionProcess, terminal_states::Vector{<:CartesianIndex})\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.jl","page":"Data Storage","title":"IntervalMDP.jl","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"read_intervalmdp_jl\nread_intervalmdp_jl_model\nread_intervalmdp_jl_spec\nwrite_intervalmdp_jl_model\nwrite_intervalmdp_jl_spec","category":"page"},{"location":"reference/data/#IntervalMDP.Data.read_intervalmdp_jl","page":"Data Storage","title":"IntervalMDP.Data.read_intervalmdp_jl","text":"read_intervalmdp_jl(model_path, spec_path)\n\nRead an IntervalMDP.jl data file and return an IntervalMarkovDecisionProcess or IntervalMarkovChain and a list of terminal states. \n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_intervalmdp_jl_model","page":"Data Storage","title":"IntervalMDP.Data.read_intervalmdp_jl_model","text":"read_intervalmdp_jl_model(model_path)\n\nRead an IntervalMarkovDecisionProcess or IntervalMarkovChain from an IntervalMDP.jl system file (netCDF sparse format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_intervalmdp_jl_spec","page":"Data Storage","title":"IntervalMDP.Data.read_intervalmdp_jl_spec","text":"read_intervalmdp_jl_spec(spec_path)\n\nRead a Specification from an IntervalMDP.jl spec file (JSON-format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.write_intervalmdp_jl_model","page":"Data Storage","title":"IntervalMDP.Data.write_intervalmdp_jl_model","text":"write_intervalmdp_jl_model(model_path, mdp)\n\nWrite an IntervalMarkovDecisionProcess to an IntervalMDP.jl system file (netCDF sparse format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.write_intervalmdp_jl_spec","page":"Data Storage","title":"IntervalMDP.Data.write_intervalmdp_jl_spec","text":"write_intervalmdp_jl_spec(spec_path, spec::Specification)\n\nWrite a Specification to an IntervalMDP.jl spec file (JSON-format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#Value-iteration","page":"Value Iteration","title":"Value iteration","text":"","category":"section"},{"location":"reference/value_iteration/","page":"Value Iteration","title":"Value Iteration","text":"value_iteration\ncontrol_synthesis\nStationaryStrategy\nTimeVaryingStrategy","category":"page"},{"location":"reference/value_iteration/#IntervalMDP.value_iteration","page":"Value Iteration","title":"IntervalMDP.value_iteration","text":"value_iteration(problem::Problem)\n\nSolve minimizes/mazimizes optimistic/pessimistic specification problems using value iteration for interval Markov processes. \n\nExamples\n\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [prob1, prob2, prob3]\ninitial_state = 1\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_state)\n\nterminal_states = [3]\ntime_horizon = 10\nprop = FiniteTimeReachability(terminal_states, time_horizon)\nspec = Specification(prop, Pessimistic, Maximize)\nproblem = Problem(mdp, spec)\nV, k, residual = value_iteration(problem)\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IntervalMDP.control_synthesis","page":"Value Iteration","title":"IntervalMDP.control_synthesis","text":"control_synthesis(problem::Problem)\n\nCompute the optimal control strategy for the given problem (system + specification). If the specification is finite time, then the strategy is time-varying, with the returned strategy being in step order (i.e., the first element of the returned vector is the strategy for the first time step). If the specification is infinite time, then the strategy is stationary and only a single vector of length num_states(system) is returned.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IntervalMDP.StationaryStrategy","page":"Value Iteration","title":"IntervalMDP.StationaryStrategy","text":"StationaryStrategy\n\nA stationary strategy is a strategy that is the same for all time steps.\n\n\n\n\n\n","category":"type"},{"location":"reference/value_iteration/#IntervalMDP.TimeVaryingStrategy","page":"Value Iteration","title":"IntervalMDP.TimeVaryingStrategy","text":"TimeVaryingStrategy\n\nA time-varying strategy is a strategy that may vary over time. Since we need to store the strategy for each time step,  the strategy is finite, and thus only applies to finite time specifications, of the same length as the strategy.\n\n\n\n\n\n","category":"type"},{"location":"reference/value_iteration/#Bellman-update","page":"Value Iteration","title":"Bellman update","text":"","category":"section"},{"location":"reference/value_iteration/","page":"Value Iteration","title":"Value Iteration","text":"bellman\nbellman!\nconstruct_workspace\nconstruct_strategy_cache\nGivenStrategyConfig\nNoStrategyConfig\nStationaryStrategyConfig\nTimeVaryingStrategyConfig","category":"page"},{"location":"reference/value_iteration/#IntervalMDP.bellman","page":"Value Iteration","title":"IntervalMDP.bellman","text":"bellman(V, prob; upper_bound = false)\n\nCompute robust Bellman update with the value function V and the interval probabilities prob  that upper or lower bounds the expectation of the value function V via O-maximization [1]. Whether the expectation is maximized or minimized is determined by the upper_bound keyword argument. That is, if upper_bound == true then an upper bound is computed and if upper_bound == false then a lower bound is computed.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\nVprev = collect(1:15)\nVcur = bellman(Vprev, prob; upper_bound = false)\n\nnote: Note\nThis function will construct a workspace object and an output vector. For a hot-loop, it is more efficient to use bellman! and pass in pre-allocated objects.\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IntervalMDP.bellman!","page":"Value Iteration","title":"IntervalMDP.bellman!","text":"bellman!(workspace, strategy_cache, Vres, V, prob, stateptr; upper_bound = false, maximize = true)\n\nCompute in-place robust Bellman update with the value function V and the interval probabilities prob that upper or lower bounds the expectation of the value function V via O-maximization [1]. Whether the expectation is maximized or minimized is determined by the upper_bound keyword argument. That is, if upper_bound == true then an upper bound is computed and if upper_bound == false then a lower bound is computed. \n\nThe output is constructed in the input Vres and returned. The workspace object is also modified, and depending on the type, the strategy cache may be modified as well. See construct_workspace and construct_strategy_cache for more details on how to pre-allocate the workspace and strategy cache.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\nV = collect(1:15)\nworkspace = construct_workspace(prob)\nstrategy_cache = construct_strategy_cache(NoStrategyConfig())\nVres = similar(V)\n\nVres = bellman!(workspace, strategy_cache, Vres, V, prob; upper_bound = false, maximize = true)\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IntervalMDP.construct_workspace","page":"Value Iteration","title":"IntervalMDP.construct_workspace","text":"construct_workspace(mp::IntervalMarkovProcess)\n\nConstruct a workspace for computing the Bellman update, given a value function. If the Bellman update is used in a hot-loop, it is more efficient to use this function to preallocate the workspace and reuse across iterations.\n\nThe workspace type is determined by the type and size of the transition probability matrix, as well as the number of threads available.\n\n\n\n\n\nconstruct_workspace(prob::IntervalProbabilities)\n\nConstruct a workspace for computing the Bellman update, given a value function. If the Bellman update is used in a hot-loop, it is more efficient to use this function to preallocate the workspace and reuse across iterations.\n\nThe workspace type is determined by the type and size of the transition probability matrix, as well as the number of threads available.\n\n\n\n\n\nconstruct_workspace(prob::OrthogonalIntervalProbabilities)\n\nConstruct a workspace for computing the Bellman update, given a value function. If the Bellman update is used in a hot-loop, it is more efficient to use this function to preallocate the workspace and reuse across iterations.\n\nThe workspace type is determined by the type and size of the transition probability matrix, as well as the number of threads available.\n\n\n\n\n\nconstruct_workspace(prob::OrthogonalIntervalProbabilities)\n\nConstruct a workspace for computing the Bellman update, given a value function. If the Bellman update is used in a hot-loop, it is more efficient to use this function to preallocate the workspace and reuse across iterations.\n\nThe workspace type is determined by the type and size of the transition probability matrix, as well as the number of threads available.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IntervalMDP.construct_strategy_cache","page":"Value Iteration","title":"IntervalMDP.construct_strategy_cache","text":"construct_strategy_cache(mp::Union{IntervalProbabilities, IntervalMarkovProcess}, config::AbstractStrategyConfig)\n\nConstruct a strategy cache from a configuration for a given interval Markov process. The resuling cache type depends on the configuration and the device to store the strategy depends on the device of the Markov process.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IntervalMDP.GivenStrategyConfig","page":"Value Iteration","title":"IntervalMDP.GivenStrategyConfig","text":"GivenStrategyConfig\n\nA configuration for a strategy cache where a given strategy is applied.\n\n\n\n\n\n","category":"type"},{"location":"reference/value_iteration/#IntervalMDP.NoStrategyConfig","page":"Value Iteration","title":"IntervalMDP.NoStrategyConfig","text":"NoStrategyConfig\n\nA configuration for a strategy cache that does not store policies. See construct_strategy_cache for more details on how to construct the cache from the configuration.\n\n\n\n\n\n","category":"type"},{"location":"reference/value_iteration/#IntervalMDP.StationaryStrategyConfig","page":"Value Iteration","title":"IntervalMDP.StationaryStrategyConfig","text":"StationaryStrategyConfig\n\nA configuration for a strategy cache that stores stationary policies. Note that the strategy is updated at each iteration of the value iteration algorithm, if a new choice is strictly better than the previous one. See [1, Section 4.3] for more details why this is necessary. See construct_strategy_cache for more details on how to construct the cache from the configuration.\n\n[1] Forejt, Vojtěch, et al. \"Automated verification techniques for probabilistic systems.\" Formal Methods for Eternal Networked Software Systems: 11th International School on Formal Methods for the Design of Computer, Communication and Software Systems, SFM 2011, Bertinoro, Italy, June 13-18, 2011. Advanced Lectures 11 (2011): 53-113.\n\n\n\n\n\n","category":"type"},{"location":"reference/value_iteration/#IntervalMDP.TimeVaryingStrategyConfig","page":"Value Iteration","title":"IntervalMDP.TimeVaryingStrategyConfig","text":"TimeVaryingStrategyConfig\n\nA configuration for a strategy cache that stores time-varying policies. See construct_strategy_cache for more details on how to construct the cache from the configuration.\n\n\n\n\n\n","category":"type"},{"location":"data/#Data-storage-formats","page":"Data formats","title":"Data storage formats","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"IntervalMDP.jl supports reading and writing data in various formats, namely PRISM explicit format, bmdp-tool, and our own format (model and specification). To justify introducing another standard (see relevant XKCD), note that the PRISM explicit format and the bmdp-tool format are all written in ASCII, which is very inefficient in terms of storage space (especially for storing floating point numbers) and parsing time. We propose a binary format for the most storage-intensive part of the data, namely the transition probabilities, and use JSON for the specification, which is human- and machine-readable and widely used.","category":"page"},{"location":"data/#PRISM","page":"Data formats","title":"PRISM","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"IntervalMDP.jl supports reading and writing PRISM [1] explicit data format.  The data format is split into 4 different files, one for the states, one for the labels, one for the transition probabilities, and one for the specification. Therefore, our interface for reading PRISM files takes the path without file ending and adds the appropriate ending to each of the four files.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"# Read\nproblem = read_prism_file(path_without_file_ending)\n\n# Write\nwrite_prism_file(path_without_file_ending, problem)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"The problem structure contains both the \\gls{imdp} and the specification including whether to synthesize a maximizing or minimizing strategy and whether to use an optimistic or pessimistic adversary.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"PRISM uses 4 different ASCII-encoded files to store the explicit representation of the system: '.sta' (states), '.lab' (labels), '.tra' (transitions), and '.pctl' (property). In the tables below, we list the format for each file. The extended details of the PRISM explicit file format can be found in the appendix of the PRISM manual.","category":"page"},{"location":"data/#States-.sta","page":"Data formats","title":"States .sta","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line The first line containing symbolically (v1, v2, ..., vn) is a list of n variables in the model.\nm lines where m is the number of states Each line contains i:(v1, v2, ..., vn) where i is the index of the state and (v1, v2, ..., vn)` is an assignment of values to the variables in the model. Indices are zero-indexed.","category":"page"},{"location":"data/#Labels-.lab","page":"Data formats","title":"Labels .lab","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line Contains a space-separated list of labels with index i=\"label\". The first two must be 0=\"init\" 1=\"deadlock\".\nAll remaining lines Contains i: j1 j2 j3 ... where i is a state index and j1 j2 j3 ... are space-separated indices of labels associated with state i.","category":"page"},{"location":"data/#Transitions-.tra","page":"Data formats","title":"Transitions .tra","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line num_states num_choices num_transitions where num_state must match the number in the state file.\nFollowing num_transitions lines A list of transition probabilities with the format src_idx act_idx dest_idx [p_lower,p_upper] action.","category":"page"},{"location":"data/#Property-.pctl","page":"Data formats","title":"Property .pctl","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line PRISM property specification","category":"page"},{"location":"data/#bmdp-tool","page":"Data formats","title":"bmdp-tool","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"bmdp-tool data format is similar to the PRISM explicit format transition probability files, where transition probabilities are stored line-by-line with source, action, destination, and probability bounds in ASCII. Key differences include no explicit listing of states, the fact that it only supports reachability properties, and that terminal states are listed directly in the transition probability file. As a result, bmdp-tool data format is a single file. This format lacks information about whether the reachability is finite or infinite time, and hence the reader only returns the set of terminal states.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"# Read\nimdp, terminal_states = read_bmdp_tool_file(path)\n\n# Write\nwrite_bmdp_tool_file(path, problem)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"bmdp-tool uses only one ASCII file with the following format:","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line num_states.\nSecond line num_actions (not to be confused with num_choices of PRISM).\nThird line num_terminal.\nThe following num_terminal lines Indices (zero-indexed) of terminal states, one per line.\nThe following num_terminal lines Indices (zero-indexed) of terminal states, one per line.\nAll remaining lines A list of transition probabilities with the format src_idx act_idx dest_idx p_lower p_upper.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"terminology: Choices vs actions\nIn PRISM, the number of choices which is listed in the transition file is the sum of the number of feasible actions in each state. In bmdp-tool, the number of actions is the total number of different actions in the model, i.e. in each state up to num_actions may be feasible. This is a subtle difference, but it is important to be aware of as the parsing in either tool requires the right number to be specified.","category":"page"},{"location":"data/#IntervalMDP.jl","page":"Data formats","title":"IntervalMDP.jl","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"IntervalMDP.jl also supports a different binary format based on NetCDF to store transition probabilities. We use JSON to store the specification, as storage space for the specification is much less a concern, and because JSON is a widely used, human-readable, file format.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"# Read\nimdp = read_intervalmdp_jl_model(model_path)\nspec = read_intervalmdp_jl_spec(spec_path)\nproblem = Problem(imdp, spec)\n\nproblem = read_intervalmdp_jl(model_path, spec_path)\n\n# Write\nwrite_intervalmdp_jl_model(model_path, imdp_or_problem)\nwrite_intervalmdp_jl_spec(spec_path, spec_or_problem)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"The new format proposed uses netCDF, which is based on HDF5 underlying, to store transition probabilities, and a JSON file to store the specification. Transition probabilities are stored in CSC-format, which is unfortunately not natively stored in netCDF, nor any widely available format. Therefore, we store the following attributes and variables in the netCDF file:","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Global attributes:","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"num_states\nmodel (either imc or imdp)\nformat (assert sparse_csc)\nrows (assert to)\ncols (assert from if model is imc and from/action if model is imdp)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Variables:","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"lower_colptr (integer)\nlower_rowval (integer)\nlower_nzval (floating point)\nupper_colptr (integer)\nupper_rowval (integer)\nupper_nzval (floating point)\ninitial_states (integer)\nstateptr (integer, only for imdp)\naction_vals (any netCDF supported type, only for imdp)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"We store the specification in a JSON format where the structure depends on the type of specification. For a reachability-like specification, the specification is the following format","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"{\n    \"property\": {\n        \"type\": <\"reachability\"|\"reach-avoid\">,\n        \"infinite_time\": <true|false>,\n        \"time_horizon\": <positive int>,\n        \"eps\": <positive float>,\n        \"reach\": [<state_index:positive int>],\n        \"avoid\": [<state_index:positive int>]\n    },\n    \"satisfaction_mode\": <\"pessimistic\"|\"optimistic\">,\n    \"strategy_mode\": <\"minimize\"|\"maximize\">\n}","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"For a finite horizon property, eps is excluded, and similarly for an infinite horizon property, time\\_horizon is excluded.  For a proper reachability property, the avoid-field is excluded.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"If we instead want to optimize a reward, the format is the following","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"{\n    \"property\": {\n        \"type\": \"reward\",\n        \"infinite_time\": <true|false>,\n        \"time_horizon\": <positive int>,\n        \"eps\": <positive float>,\n        \"reward\": [<reward_per_state_index:float>]\n        \"discount\" <float:0-1>\n    },\n    \"satisfaction_mode\": <\"pessimistic\"|\"optimistic\">,\n    \"strategy_mode\": <\"minimize\"|\"maximize\">\n}","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"[1] Kwiatkowska, Marta, Gethin Norman, and David Parker. \"PRISM 4.0: Verification of probabilistic real-time systems.\" Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23. Springer Berlin Heidelberg, 2011.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = IntervalMDP","category":"page"},{"location":"#IntervalMDP","page":"Home","title":"IntervalMDP","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"IntervalMDP.jl is a Julia package for modeling and certifying Interval Markov Decision Processes (IMDPs) via Value Iteration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"IMDPs are a generalization of Markov Decision Processes (MDPs) where the transition probabilities are represented by intervals instead of point values, to model uncertainty. IMDPs are also frequently chosen as the model for abstracting the dynamics of a stochastic system, as one may compute upper and lower bounds on transitioning from one region to another.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The aim of this package is to provide a user-friendly interface to solve value iteration for IMDPs with great efficiency. Furthermore, it provides methods for accelerating the computation of the certificate using CUDA hardware. See Algorithms for algorithmic advances that this package introduces for enabling better use of the available hardware and higher performance.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"O-maximization and value iteration\nDense and sparse matrix support\nParametric probability types for customizable precision\nMultithreaded CPU and CUDA-accelerated value iteration\nData loading and writing in formats by various tools (PRISM, bmdp-tool, IntervalMDP.jl)","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: Info\nUntil now, all state-of-the-art tools for IMDPs have been standalone programs.  This is explicitly a package, enabling better integration with other tools and libraries.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package requires Julia v1.9 or later. Refer to the official documentation on how to install it for your system.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install IntervalMDP.jl, use the following command inside Julia's REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> import Pkg; Pkg.add(\"IntervalMDP\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to use the CUDA extension, you also need to install CUDA.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> import Pkg; Pkg.add(\"CUDA\")","category":"page"},{"location":"reference/systems/#System-representation","page":"Systems","title":"System representation","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalMarkovProcess\nnum_states(s::IntervalMarkovProcess)\ninitial_states(s::IntervalMarkovProcess)\nAllStates\ntransition_prob(mp::IntervalMarkovProcess)\nIntervalMarkovChain\nIntervalMarkovDecisionProcess\nstateptr(mdp::IntervalMarkovDecisionProcess)\nOrthogonalIntervalMarkovChain\nOrthogonalIntervalMarkovDecisionProcess\nstateptr(mdp::OrthogonalIntervalMarkovDecisionProcess)","category":"page"},{"location":"reference/systems/#IntervalMDP.IntervalMarkovProcess","page":"Systems","title":"IntervalMDP.IntervalMarkovProcess","text":"IntervalMarkovProcess\n\nAn abstract type for interval Markov processes including IntervalMarkovChain and IntervalMarkovDecisionProcess.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.num_states-Tuple{IntervalMarkovProcess}","page":"Systems","title":"IntervalMDP.num_states","text":"num_states(mp::IntervalMarkovProcess)\n\nReturn the number of states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.initial_states-Tuple{IntervalMarkovProcess}","page":"Systems","title":"IntervalMDP.initial_states","text":"initial_states(mp::IntervalMarkovProcess)\n\nReturn the initial states. If the initial states are not specified, return nothing.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.AllStates","page":"Systems","title":"IntervalMDP.AllStates","text":"AllStates\n\nA type to represent all states in a Markov process. This type is used to specify all states as the initial states.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.transition_prob-Tuple{IntervalMarkovProcess}","page":"Systems","title":"IntervalMDP.transition_prob","text":"transition_prob(mp::IntervalMarkovProcess)\n\nReturn the interval on transition probabilities.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.IntervalMarkovChain","page":"Systems","title":"IntervalMDP.IntervalMarkovChain","text":"IntervalMarkovChain(transition_prob::IntervalProbabilities, initial_states::InitialStates = AllStates())\n\nConstruct an Interval Markov Chain from a square matrix pair of interval transition probabilities. The initial states are optional and if not specified, all states are assumed to be initial states. The number of states is inferred from the size of the transition probability matrix.\n\nThe returned type is an IntervalMarkovDecisionProcess with only one action per state (i.e. stateptr[j + 1] - stateptr[j] == 1 for all j). This is done to unify the interface for value iteration.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.IntervalMarkovDecisionProcess","page":"Systems","title":"IntervalMDP.IntervalMarkovDecisionProcess","text":"IntervalMarkovDecisionProcess{\n    P <: IntervalProbabilities,\n    VT <: AbstractVector{Int32},\n    VI <: Union{AllStates, AbstractVector}\n}\n\nA type representing (stationary) Interval Markov Decision Processes (IMDP), which are Markov Decision Processes with uncertainty in the form of intervals on the transition probabilities.\n\nFormally, let (S S_0 A barP underbarP) be an interval Markov decision processes, where S is the set of states, S_0 subset S is the set of initial states, A is the set of actions, and barP  A to mathbbR^S times S and underbarP  A to mathbbR^S times S are functions representing the upper and lower bound transition probability matrices prespectively for each action. Then the IntervalMarkovDecisionProcess type is defined as follows: indices 1:num_states are the states in S, transition_prob represents barP and underbarP, actions are  implicitly defined by stateptr (e.g. if stateptr[3] == 4 and stateptr[4] == 7 then the actions available to state 3 are [4, 5, 6]),  and initial_states is the set of initial states S_0. If no initial states are specified, then the initial states are assumed to be all states in S.\n\nFields\n\ntransition_prob::P: interval on transition probabilities where columns represent source/action pairs and rows represent target states.\nstateptr::VT: pointer to the start of each source state in transition_prob (i.e. transition_prob[:, stateptr[j]:stateptr[j + 1] - 1] is the transition   probability matrix for source state j) in the style of colptr for sparse matrices in CSC format.\ninitial_states::VI: initial states.\nnum_states::Int32: number of states.\n\nExamples\n\ntransition_probs = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.1 0.2 0.0\n        0.1 0.3 0.2 0.3 0.0\n        0.2 0.1 0.3 0.4 1.0\n    ],\n    upper = [\n        0.5 0.7 0.6 0.6 0.0\n        0.6 0.5 0.5 0.5 0.0\n        0.7 0.3 0.4 0.4 1.0\n    ],\n)\n\nstateptr = [1, 3, 5, 6]\ninitial_states = [1]\n\nmdp = IntervalMarkovDecisionProcess(transition_probs, stateptr, initial_states)\n\nThere is also a constructor for IntervalMarkovDecisionProcess where the transition probabilities are given as a list of  transition probabilities for each source state.\n\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [prob1, prob2, prob3]\ninitial_states = [1]\n\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_states)\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.stateptr-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.stateptr","text":"stateptr(mdp::IntervalMarkovDecisionProcess)\n\nReturn the state pointer of the Interval Markov Decision Process. The state pointer is a vector of integers where the i-th element is the index of the first element of the i-th state in the transition probability matrix.  I.e. transition_prob[:, stateptr[j]:stateptr[j + 1] - 1] is the transition probability matrix for source state j.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.OrthogonalIntervalMarkovChain","page":"Systems","title":"IntervalMDP.OrthogonalIntervalMarkovChain","text":"OrthogonalIntervalMarkovChain(transition_prob::OrthogonalIntervalProbabilities, initial_states::InitialStates = AllStates())\n\nConstruct a Orthogonal Interval Markov Chain from orthogonal interval transition probabilities. The initial states are optional and if not specified, all states are assumed to be initial states. The number of states is inferred from the size of the transition probability matrix.\n\nThe returned type is an OrthogonalIntervalMarkovDecisionProcess with only one action per state (i.e. stateptr[j + 1] - stateptr[j] == 1 for all j). This is done to unify the interface for value iteration.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.OrthogonalIntervalMarkovDecisionProcess","page":"Systems","title":"IntervalMDP.OrthogonalIntervalMarkovDecisionProcess","text":"OrthogonaIntervalMarkovDecisionProcess{\n    P <: IntervalProbabilities,\n    VT <: AbstractVector{Int32},\n    VI <: Union{AllStates, AbstractVector}\n}\n\nA type representing (stationary) Orthogona Interval Markov Decision Processes (OIMDP), which are IMDPs where the transition  probabilities for each state can be represented as the product of the transition probabilities of individual processes.\n\nTODO: Update theory section\n\nFormally, let (S S_0 A barP underbarP) be an interval Markov decision processes, where S is the set of states, S_0 subset S is the set of initial states, A is the set of actions, and barP  A to mathbbR^S times S and underbarP  A to mathbbR^S times S are functions representing the upper and lower bound transition probability matrices prespectively for each action. Then the IntervalMarkovDecisionProcess type is defined as follows: indices 1:num_states are the states in S, transition_prob represents barP and underbarP, actions are  implicitly defined by stateptr (e.g. if stateptr[3] == 4 and stateptr[4] == 7 then the actions available to state 3 are [4, 5, 6]),  and initial_states is the set of initial states S_0. If no initial states are specified, then the initial states are assumed to be all states in S.\n\nFields\n\nTODO: Update fields\n\ntransition_prob::P: interval on transition probabilities where columns represent source/action pairs and rows represent target states.\nstateptr::VT: pointer to the start of each source state in transition_prob (i.e. transition_prob[:, stateptr[j]:stateptr[j + 1] - 1] is the transition   probability matrix for source state j) in the style of colptr for sparse matrices in CSC format.\ninitial_states::VI: initial states.\nnum_states::Int32: number of states.\n\nExamples\n\nTODO: Update examples\n\ntransition_probs = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.1 0.2 0.0\n        0.1 0.3 0.2 0.3 0.0\n        0.2 0.1 0.3 0.4 1.0\n    ],\n    upper = [\n        0.5 0.7 0.6 0.6 0.0\n        0.6 0.5 0.5 0.5 0.0\n        0.7 0.3 0.4 0.4 1.0\n    ],\n)\n\nstateptr = [1, 3, 5, 6]\ninitial_states = [1]\n\nmdp = IntervalMarkovDecisionProcess(transition_probs, stateptr, initial_states)\n\nThere is also a constructor for IntervalMarkovDecisionProcess where the transition probabilities are given as a list of  transition probabilities for each source state.\n\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [prob1, prob2, prob3]\ninitial_states = [1]\n\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_states)\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.stateptr-Tuple{OrthogonalIntervalMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.stateptr","text":"stateptr(mdp::OrthogonalIntervalMarkovDecisionProcess)\n\nReturn the state pointer of the Interval Markov Decision Process. The state pointer is a vector of integers where the i-th element is the index of the first element of the i-th state in the transition probability matrix.  I.e. transition_prob[:, stateptr[j]:stateptr[j + 1] - 1] is the transition probability matrix for source state j.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#Probability-representation","page":"Systems","title":"Probability representation","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalProbabilities\nOrthogonalIntervalProbabilities\nlower\nupper\ngap\nsum_lower\nnum_source\nnum_target\naxes_source","category":"page"},{"location":"reference/systems/#IntervalMDP.IntervalProbabilities","page":"Systems","title":"IntervalMDP.IntervalProbabilities","text":"IntervalProbabilities{R, VR <: AbstractVector{R}, MR <: AbstractMatrix{R}}\n\nA matrix pair to represent the lower and upper bound transition probabilities from all source states or source/action pairs to all target states. The matrices can be Matrix{R} or SparseMatrixCSC{R}, or their CUDA equivalents. For memory efficiency, it is recommended to use sparse matrices.\n\nThe columns represent the source and the rows represent the target, as if the probability matrix was a linear transformation. Mathematically, let P be the probability matrix. Then P_ij represents the probability of transitioning from state j (or with state/action pair j) to state i. Due to the column-major format of Julia, this is also a more efficient representation (in terms of cache locality).\n\nThe lower bound is explicitly stored, while the upper bound is computed from the lower bound and the gap. This choice is  because it simplifies repeated probability assignment using O-maximization [1].\n\nFields\n\nlower::MR: The lower bound transition probabilities from a source state or source/action pair to a target state.\ngap::MR: The gap between upper and lower bound transition probabilities from a source state or source/action pair to a target state.\nsum_lower::VR: The sum of lower bound transition probabilities from a source state or source/action pair to all target states.\n\nExamples\n\ndense_prob = IntervalProbabilities(;\n    lower = [0.0 0.5; 0.1 0.3; 0.2 0.1],\n    upper = [0.5 0.7; 0.6 0.5; 0.7 0.3],\n)\n\nsparse_prob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.OrthogonalIntervalProbabilities","page":"Systems","title":"IntervalMDP.OrthogonalIntervalProbabilities","text":"OrthogonalIntervalProbabilities{N, P <: IntervalProbabilities}\n\nA tuple of IntervalProbabilities transition probabilities from all source states or source/action pairs to the target states along each axis. \n\nFields\n\nprobs::NTuple{N, P}: A tuple of IntervalProbabilities transition probabilities along each axis.\nsource_dims::NTuple{N, Int32}: The dimensions of the orthogonal probabilities for the source axis. This is flattened to a single dimension for indexing.\n\nExamples\n\nTODO: Update example\n\n```jldoctest\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.lower","page":"Systems","title":"IntervalMDP.lower","text":"lower(p::IntervalProbabilities)\n\nReturn the lower bound transition probabilities from a source state or source/action pair to a target state.\n\n\n\n\n\nlower(p::OrthogonalIntervalProbabilities, i)\n\nReturn the lower bound transition probabilities from a source state or source/action pair to a target state.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.upper","page":"Systems","title":"IntervalMDP.upper","text":"upper(p::IntervalProbabilities)\n\nReturn the upper bound transition probabilities from a source state or source/action pair to a target state.\n\nnote: Note\nIt is not recommended to use this function for the hot loop of O-maximization. Because the IntervalProbabilities stores the lower and gap transition probabilities, fetching the upper bound requires allocation and computation.\n\n\n\n\n\nupper(p::OrthogonalIntervalProbabilities, i)\n\nReturn the upper bound transition probabilities from a source state or source/action pair to a target state.\n\nnote: Note\nIt is not recommended to use this function for the hot loop of O-maximization. Because the IntervalProbabilities stores the lower and gap transition probabilities, fetching the upper bound requires allocation and computation.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.gap","page":"Systems","title":"IntervalMDP.gap","text":"gap(p::IntervalProbabilities)\n\nReturn the gap between upper and lower bound transition probabilities from a source state or source/action pair to a target state.\n\n\n\n\n\ngap(p::OrthogonalIntervalProbabilities, i)\n\nReturn the gap between upper and lower bound transition probabilities from a source state or source/action pair to a target state.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.sum_lower","page":"Systems","title":"IntervalMDP.sum_lower","text":"sum_lower(p::IntervalProbabilities)\n\nReturn the sum of lower bound transition probabilities from a source state or source/action pair to all target states. This is useful in efficiently implementing O-maximization, where we start with a lower bound probability assignment and iteratively, according to the ordering, adding the gap until the sum of probabilities is 1.\n\n\n\n\n\nsum_lower(p::OrthogonalIntervalProbabilities, i)\n\nReturn the sum of lower bound transition probabilities from a source state or source/action pair to all target states. This is useful in efficiently implementing O-maximization, where we start with a lower bound probability assignment and iteratively, according to the ordering, adding the gap until the sum of probabilities is 1.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.num_source","page":"Systems","title":"IntervalMDP.num_source","text":"num_source(p::IntervalProbabilities)\n\nReturn the number of source states or source/action pairs.\n\n\n\n\n\nnum_source(p::OrthogonalIntervalProbabilities)\n\nReturn the number of source states or source/action pairs.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.num_target","page":"Systems","title":"IntervalMDP.num_target","text":"num_target(p::IntervalProbabilities)\n\nReturn the number of target states.\n\n\n\n\n\n","category":"function"},{"location":"reference/systems/#IntervalMDP.axes_source","page":"Systems","title":"IntervalMDP.axes_source","text":"axes_source(p::IntervalProbabilities)\n\nReturn the valid range of indices for the source states or source/action pairs.\n\n\n\n\n\naxes_source(p::OrthogonalIntervalProbabilities)\n\nReturn the valid range of indices for the source states or source/action pairs.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"To simplify the dicussion on the algorithmic choices, we will assume that the goal is to compute the maximizing pessimistic probability of reaching a set of states G, that is, ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"max_pi  min_eta  mathbbP_pieta leftomega in Omega  exists k in 0K  omega(k)in G  right","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"See Theory for more details on the theory behind IMDPs including strategies and adversaries; in this case the maximization and minimization operators respectively. The algorithms are easily adapted to other specifications, such as minimizing optimistic probability, which is useful for safety, or maximizing pessimitic discounted reward. Assume furthermore that the transition probabilities are represented as a sparse matrix. This is the most common representation for large models, and the algorithms are easily adapted to dense matrices with the sorting (see Sorting) being shared across states such that parallelizing this has a smaller impact on performance.","category":"page"},{"location":"algorithms/#Solving-reachability-as-value-iteration","page":"Algorithms","title":"Solving reachability as value iteration","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Computing the solution to the above problem can be reframed in terms of value iteration. The value function V_k is the probability of reaching G in k steps or fewer. The value function is initialized to V_0(s) = 1 if s in G and V_0(s) = 0 otherwise. The value function is then iteratively updated according to the Bellman equation","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"beginaligned\n    V_0(s) = mathbf1_G(s) \n    V_k(s) = mathbf1_G(s) + mathbf1_Ssetminus G(s) max_a in A min_p_sain Gamma_sa sum_s in S V_k-1(s) p_sa(s)\nendaligned","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where mathbf1_G(s) = 1  if s in G and 0 otherwise is the indicator function for set G. This Bellman update is repeated until k = K, or if K = infty, the value function converges, i.e. V_k = V_k-1 for some k. The value function is then the solution to the problem. Exact convergence is virtually impossible to achieve in a finite number of iterations due to the finite precision of floating point numbers. Hence, we instead use a residual tolerance epsilon and stop when Bellman residual V_k - V_k-1 is less than the threshold, V_k - V_k-1_infty  epsilon.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"In a more programmatic formulation, the algorithm (for K = infty) can be summarized as follows:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function value_iteration(system, spec)\n    V = initialize_value_function(spec)\n\n    while !converged(V)\n        V = bellman_update(V, system)\n    end\nend","category":"page"},{"location":"algorithms/#Efficient-value-iteration","page":"Algorithms","title":"Efficient value iteration","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Computing the Bellman update for can be done indepently for each state. ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function bellman_update(V, system)\n    # Thread.@threads parallelize across available threads\n    Thread.@threads for s in states(system)\n        # Minimize over probability distributions in `Gamma_{s,a}`, i.e. pessimistic\n        V_state = minimize_feasible_dist(V, system, s)\n\n        # Maximize over actions\n        V[s] = maximum(V_state)\n    end\nend","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"For each state, we need to compute the minimum over all feasible distributions per state-action pairs and the maximum over all actions for each state. The minimum over all feasible distributions can be computed as a solution to a Linear Programming (LP) problem, namely","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"    beginaligned\n        min_p_sa quad  sum_s in S V_k-1(s) cdot p_sa(s) \n        quad  underlineP(sas) leq p_sa(s) leq overlineP(sas) quad forall s in S \n        quad  sum_s in S p_sa(s) = 1 \n    endaligned","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"However, due to the particular structure of the LP problem, we can use a more efficient algorithm: O-maximization, or ordering-maximization [1]. In the case of pessimistic probability, we want to assign the most possible probability mass to the destinations with the smallest value of V_k-1, while obeying that the probability distribution is feasible, i.e. within the probability bounds and that it sums to 1. This is done by sorting the values of V_k-1 and then assigning state with the smallest value its upper bound, then the second smallest, and so on until the remaining mass must be assigned to the lower bound of the remaining states for probability distribution is feasible.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function minimize_feasible_dist(V, system, s)\n    # Sort values of `V` in ascending order\n    order = sortperm(V)\n\n    # Initialize distribution to lower bounds\n    p = lower_bounds(system, s)\n    rem = sum(p)\n\n    # Assign upper bounds to states with smallest values\n    # until remaining mass is zero\n    for idx in order\n        gap = upper_bounds(system, s)[idx] - p[idx]\n        if rem <= gap\n            p[idx] += rem\n            break\n        else\n            p[idx] += gap\n            rem -= gap\n        end\n    end\n\n    return p\nend","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"We abstract this algorithm into the sorting phase and the O-maximization phase: ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function minimize_feasible_dist(V, system, s)\n    # Sort values of `V` in ascending order\n    order = sortstates(V)\n    p = o_maximize(system, s, order)\n    return p\nend","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"When computing computing the above on a GPU, we can and should parallelize both the sorting and the O-maximization phase. In the following two sections, we will discuss how parallelize these phases.","category":"page"},{"location":"algorithms/#Sorting","page":"Algorithms","title":"Sorting","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Sorting in parallel on the GPU is a well-studied problem, and there are many algorithms for doing so. We choose to use bitonic sorting, which is a sorting network that is easily parallelized and implementable on a GPU. The idea is to merge bitonic subsets, i.e. sets with first increasing then decreasing subsets of equal size, of increasingly larger sizes and perform minor rounds of swaps to maintain the bitonic property. The figure below shows 3 major rounds to sort a set of 8 elements (each line represents an element, each arrow is a comparison pointing towards the larger element). The latency[1] of the sorting network is O((lg n)^2), and thus it scales well to larger number of elements. See Wikipedia for more details.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"(Image: )","category":"page"},{"location":"algorithms/#O-maximization","page":"Algorithms","title":"O-maximization","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"In order to parallelize the O-maximization phase, observe that O-maximization implicity implements a cumulative sum according to the ordering over gaps and this is the only dependency between the states. Hence, if we can parallelize this cumulative sum, then we can parallelize the O-maximization phase. Luckily, there is a well-studied algorithm for computing the cumulative sum in parallel: tree reduction for prefix scan. The idea is best explained with figure below.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"(Image: )","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Here, we recursively compute the cumulative sum of larger and larger subsets of the array. The latency is O(lg n), and thus very efficient. See Wikipedia for more details. When implementing the tree reduction on GPU, it is possible to use warp shuffles to very efficiently perform tree reductions of up to 32 elements. For larger sets, shared memory to store the intermediate results, which is much faster than global memory. See CUDA Programming Model for more details on why these choices are important.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Putting it all together, we get the following (pseudo-code) algorithm for O-maximization:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function o_maximize(system, s, order)\n    p = lower_bounds(system, s)\n    rem = 1 - sum(p)\n    gap = upper_bounds(system, s) - p\n\n    # Ordered cumulative sum of gaps\n    cumgap = cumulative_sum(gap[order])\n\n    @parallelize for (i, o) in enumerate(order)\n        rem_state = max(rem - cumgap[i] + gap[o], 0)\n        if gap[o] < rem_state\n            p[o] += gap[o]\n        else\n            p[o] += rem_state\n            break\n        end\n    end\n\n    return p\nend","category":"page"},{"location":"algorithms/#CUDA-Programming-Model","page":"Algorithms","title":"CUDA Programming Model","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"We here give a brief introduction to the CUDA programming model to understand to algorithmic choices. For a more in-depth introduction, see the CUDA C++ Programming Guide. The CUDA framework is Single-Instruction Multiple-Thread (SIMT) parallel execution platform and Application Programming Interface. This is in contrast to Single-Instruction Multiple-Data where all data must be processed homogeneously without control flow. SIMT makes CUDA more flexible for heterogeneous processing and control flow. The smallest execution unit in CUDA is a thread, which is a sequential processing of instructions. A thread is uniquely identified by its thread index, which allows indexing into the global data for parallel processing. A group of 32 threads[2] is called a warp, which will be executed mostly synchronously on a streaming multiprocessor. If control flow makes threads in a wrap diverge, instructions may need to be decoded twice and executed in two separate cycles. Due to this synchronous behavior, data can be shared in registers between threads in a warp for maximum performance. A collection of (up to) 1024 threads is called a block, and this is the largest aggregation that can be synchronized. Furthermore, threads in a block share the appropriately named shared memory. This is memory that is stored locally on the streaming multiprocessor for fast access. Note that shared memory is unintuitively faster than local memory (not to be confused with registers) due to local memory being allocated in device memory. Finally, a collection of (up to) 65536 blocks is called the grid of a kernel, which is the set of instructions to be executed. The grid is singular as only a single ever exists per launched kernel. Hence, if more blocks are necessary to process the amount of data, then a grid-strided loop or multiple kernels are necessary. ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"(Image: )","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"[1]: Note that when assessing parallel algorithms, the asymptotic performance is measured by the latency, which is the delay in the number of parallel operations, before the result is available. This is in contrast to traditional algorithms, which are assessed by the total number of operations.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"[2]: with consecutive thread indices aligned to a multiple of 32.","category":"page"}]
}
